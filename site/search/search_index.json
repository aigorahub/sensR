{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"sensPy","text":"<p>Thurstonian Models for Sensory Discrimination in Python</p> <p>sensPy is a Python port of the R package sensR, providing statistical methods for analyzing sensory discrimination tests.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Discrimination Analysis: Analyze triangle, duo-trio, 2-AFC, 3-AFC, tetrad, and other protocols</li> <li>Power Analysis: Calculate statistical power and required sample sizes</li> <li>Advanced Models: Beta-binomial, Same-Different, Degree-of-Difference (DOD), 2-AC</li> <li>Group Comparisons: Compare d-prime across multiple groups with post-hoc tests</li> <li>ROC Analysis: Compute ROC curves and AUC with confidence intervals</li> <li>Interactive Plots: Beautiful Plotly visualizations</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import senspy as sp\n\n# Analyze a triangle test: 80 correct out of 100\nresult = sp.discrim(correct=80, total=100, method=\"triangle\")\nprint(f\"d-prime: {result.d_prime:.3f}\")\nprint(f\"p-value: {result.p_value:.4f}\")\n\n# Calculate power\npower = sp.discrim_power(d_prime=1.0, n=100, method=\"triangle\")\nprint(f\"Power: {power:.1%}\")\n\n# Interactive ROC curve\nfig = sp.plot_roc(d_prime=1.5, se_d=0.2)\nfig.show()\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install senspy\n</code></pre> <p>Or with uv:</p> <pre><code>uv pip install senspy\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Installation and quick start</li> <li>User Guide - Detailed protocol and analysis guides</li> <li>API Reference - Complete function reference</li> <li>Changelog - Version history</li> </ul>"},{"location":"#supported-protocols","title":"Supported Protocols","text":"Protocol Method Guessing Prob Triangle <code>\"triangle\"</code> 1/3 Duo-trio <code>\"duotrio\"</code> 1/2 2-AFC <code>\"twoAFC\"</code> 1/2 3-AFC <code>\"threeAFC\"</code> 1/3 Tetrad <code>\"tetrad\"</code> 1/3 Hexad <code>\"hexad\"</code> 1/10"},{"location":"#license","title":"License","text":"<p>GPL-2.0-or-later (same as sensR)</p>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Original sensR Package</li> </ul>"},{"location":"GAP_ANALYSIS/","title":"sensR to sensPy: Gap Analysis","text":"<p>Version: 1.0 Date: 2025-12-16 Scope: Complete rewrite (Delete &amp; Rewrite approach)</p>"},{"location":"GAP_ANALYSIS/#executive-summary","title":"Executive Summary","text":"<p>This document provides a comprehensive gap analysis comparing the sensR R package (v1.5-3) against the current sensPy implementation. Following the decision to delete and rewrite the existing Python code, all functions are currently unimplemented.</p> <p>Overall Status: 0 of 127 functions implemented (0%)</p>"},{"location":"GAP_ANALYSIS/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Implementation Status Overview</li> <li>Detailed Function Gap Analysis</li> <li>Feature Parity Checklist</li> <li>Priority Implementation Queue</li> </ol>"},{"location":"GAP_ANALYSIS/#1-implementation-status-overview","title":"1. Implementation Status Overview","text":""},{"location":"GAP_ANALYSIS/#11-category-summary","title":"1.1 Category Summary","text":"Category Total Functions Implemented Missing % Complete Link Functions 13 0 13 0% Double Link Functions 5 0 5 0% Discrimination 4 0 4 0% Statistical Models 12 0 12 0% Power Analysis 9 0 9 0% Sample Size 4 0 4 0% Inference/Testing 8 0 8 0% ROC/AUC 6 0 6 0% Utilities 14 0 14 0% S3 Methods 52 0 52 0% TOTAL 127 0 127 0%"},{"location":"GAP_ANALYSIS/#12-visual-progress","title":"1.2 Visual Progress","text":"<pre><code>Link Functions:        [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nDouble Links:          [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nDiscrimination:        [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nModels:                [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nPower Analysis:        [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nSample Size:           [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nInference:             [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nROC/AUC:               [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nUtilities:             [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\nS3 Methods:            [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nOVERALL:               [\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]   0%\n</code></pre>"},{"location":"GAP_ANALYSIS/#2-detailed-function-gap-analysis","title":"2. Detailed Function Gap Analysis","text":""},{"location":"GAP_ANALYSIS/#21-link-functions-linksr","title":"2.1 Link Functions (<code>links.R</code>)","text":"R Function Python Target Status Priority Notes <code>duotrio()</code> <code>DuoTrioLink</code> \u274c Missing P0 Core - most used <code>triangle()</code> <code>TriangleLink</code> \u274c Missing P0 Core - most used <code>twoAFC()</code> <code>TwoAFCLink</code> \u274c Missing P0 Core - most used <code>threeAFC()</code> <code>ThreeAFCLink</code> \u274c Missing P0 <code>tetrad()</code> <code>TetradLink</code> \u274c Missing P0 <code>hexad()</code> <code>HexadLink</code> \u274c Missing P1 Less common <code>twofive()</code> <code>TwoFiveLink</code> \u274c Missing P1 Less common <code>twofiveF()</code> <code>TwoFiveFLink</code> \u274c Missing P1 Less common <p>Required Methods per Link: - <code>linkinv(eta)</code> - d-prime to probability - <code>linkfun(mu)</code> - probability to d-prime - <code>mu.eta(eta)</code> - derivative for delta method</p>"},{"location":"GAP_ANALYSIS/#22-double-link-functions-doublelinksr","title":"2.2 Double Link Functions (<code>doublelinks.R</code>)","text":"R Function Python Target Status Priority Notes <code>doubleduotrio()</code> <code>DoubleDuoTrioLink</code> \u274c Missing P2 <code>doubletriangle()</code> <code>DoubleTriangleLink</code> \u274c Missing P2 <code>doubletwoAFC()</code> <code>DoubleTwoAFCLink</code> \u274c Missing P2 <code>doublethreeAFC()</code> <code>DoubleThreeAFCLink</code> \u274c Missing P2 <code>doubletetrad()</code> <code>DoubleTetradLink</code> \u274c Missing P2"},{"location":"GAP_ANALYSIS/#23-discrimination-analysis-discrimr","title":"2.3 Discrimination Analysis (<code>discrim.R</code>)","text":"R Function Python Target Status Priority Notes <code>discrim()</code> <code>discrim()</code> \u274c Missing P0 Main entry point <code>AnotA()</code> <code>anota()</code> \u274c Missing P1 Specialized protocol <code>discrimSim()</code> <code>discrim_sim()</code> \u274c Missing P2 Simulation <code>profBinom()</code> <code>_prof_binom()</code> \u274c Missing P1 Internal helper <p>Required Features: - All statistics: exact, likelihood, Wald, score - Confidence intervals: Wald, profile likelihood - Profile computation for plotting</p>"},{"location":"GAP_ANALYSIS/#24-beta-binomial-model-betabinr","title":"2.4 Beta-Binomial Model (<code>betaBin.R</code>)","text":"R Function Python Target Status Priority Notes <code>betabin()</code> <code>BetaBinomial.fit()</code> \u274c Missing P0 Main fitting <code>print.betabin</code> <code>BetaBinomial.__str__()</code> \u274c Missing P1 <code>summary.betabin</code> <code>BetaBinomial.summary()</code> \u274c Missing P1 <code>vcov.betabin</code> <code>BetaBinomial.vcov</code> \u274c Missing P1 <code>logLik.betabin</code> <code>BetaBinomial.loglik</code> \u274c Missing P1"},{"location":"GAP_ANALYSIS/#25-two-alternative-certainty-model-twoacr","title":"2.5 Two-Alternative Certainty Model (<code>twoAC.R</code>)","text":"R Function Python Target Status Priority Notes <code>twoAC()</code> <code>TwoAC.fit()</code> \u274c Missing P1 Main fitting <code>twoACpwr()</code> <code>twoac_power()</code> \u274c Missing P2 <code>estimate.2AC()</code> <code>TwoAC._estimate()</code> \u274c Missing P1 Internal <code>nll.2AC()</code> <code>TwoAC._nll()</code> \u274c Missing P1 Internal <code>LRtest.2AC()</code> <code>TwoAC.lr_test()</code> \u274c Missing P2 <code>profile.twoAC</code> <code>TwoAC.profile()</code> \u274c Missing P2 <code>confint.twoAC</code> <code>TwoAC.confint()</code> \u274c Missing P1 <code>clm2twoAC()</code> <code>clm_to_twoac()</code> \u274c Missing P3 Integration"},{"location":"GAP_ANALYSIS/#26-same-different-model-samediffr","title":"2.6 Same-Different Model (<code>samediff.R</code>)","text":"R Function Python Target Status Priority Notes <code>samediff()</code> <code>SameDiff.fit()</code> \u274c Missing P1 Main fitting <code>samediffSim()</code> <code>samediff_sim()</code> \u274c Missing P2 <code>samediffPwr()</code> <code>samediff_power()</code> \u274c Missing P2 <code>profile.samediff</code> <code>SameDiff.profile()</code> \u274c Missing P2 <code>confint.samediff</code> <code>SameDiff.confint()</code> \u274c Missing P1 <code>summary.samediff</code> <code>SameDiff.summary()</code> \u274c Missing P1"},{"location":"GAP_ANALYSIS/#27-degree-of-difference-model-dodr","title":"2.7 Degree-of-Difference Model (<code>dod.R</code>)","text":"R Function Python Target Status Priority Notes <code>dod()</code> <code>DOD.fit()</code> \u274c Missing P1 Main fitting <code>dod_fit()</code> <code>DOD._fit()</code> \u274c Missing P1 Internal <code>dodControl()</code> <code>DODControl</code> \u274c Missing P1 Config dataclass <code>dodSim()</code> <code>dod_sim()</code> \u274c Missing P2 <code>optimal_tau()</code> <code>optimal_tau()</code> \u274c Missing P2 <code>par2prob_dod()</code> <code>_par_to_prob_dod()</code> \u274c Missing P1 Internal <code>dod_nll()</code> <code>_dod_nll()</code> \u274c Missing P1 Internal <code>confint.dod_fit</code> <code>DOD.confint()</code> \u274c Missing P1"},{"location":"GAP_ANALYSIS/#28-power-analysis-powerr-newpowerr-dod_powerr","title":"2.8 Power Analysis (<code>power.R</code>, <code>newPower.R</code>, <code>dod_power.R</code>)","text":"R Function Python Target Status Priority Notes <code>discrimPwr()</code> <code>discrim_power()</code> \u274c Missing P1 Main power <code>d.primePwr()</code> <code>dprime_power()</code> \u274c Missing P1 <code>d.primePwr2()</code> <code>dprime_power2()</code> \u274c Missing P2 Alternative <code>normalPwr()</code> <code>_normal_power()</code> \u274c Missing P1 Internal <code>pdPwr()</code> <code>_pd_power()</code> \u274c Missing P1 Internal <code>binomPwr()</code> <code>_binom_power()</code> \u274c Missing P2 Internal <code>dodPwr()</code> <code>dod_power()</code> \u274c Missing P2 DOD specific <code>dodPwr_internal()</code> <code>_dod_power_internal()</code> \u274c Missing P2 Internal <code>testPwrArgs()</code> <code>_test_power_args()</code> \u274c Missing P2 Validation"},{"location":"GAP_ANALYSIS/#29-sample-size-samplesizer","title":"2.9 Sample Size (<code>sample.size.R</code>)","text":"R Function Python Target Status Priority Notes <code>discrimSS()</code> <code>discrim_sample_size()</code> \u274c Missing P1 <code>d.primeSS()</code> <code>dprime_sample_size()</code> \u274c Missing P1 <code>normalSS()</code> <code>_normal_sample_size()</code> \u274c Missing P1 Internal <code>pdSS()</code> <code>_pd_sample_size()</code> \u274c Missing P1 Internal"},{"location":"GAP_ANALYSIS/#210-inference-testing-dprimetestr","title":"2.10 Inference &amp; Testing (<code>d.primeTest.R</code>)","text":"R Function Python Target Status Priority Notes <code>dprime_test()</code> <code>dprime_test()</code> \u274c Missing P1 <code>dprime_compare()</code> <code>dprime_compare()</code> \u274c Missing P1 <code>posthoc()</code> <code>posthoc()</code> \u274c Missing P2 <code>dprime_table()</code> <code>_dprime_table()</code> \u274c Missing P1 Internal <code>dprime_estim()</code> <code>_dprime_estim()</code> \u274c Missing P1 Internal <code>dprime_nll()</code> <code>_dprime_nll()</code> \u274c Missing P1 Internal <code>dprime_testStat()</code> <code>_dprime_test_stat()</code> \u274c Missing P1 Internal <code>getPosthoc()</code> <code>_get_posthoc()</code> \u274c Missing P2 Internal"},{"location":"GAP_ANALYSIS/#211-roc-auc-rocr","title":"2.11 ROC &amp; AUC (<code>ROC.R</code>)","text":"R Function Python Target Status Priority Notes <code>SDT()</code> <code>sdt_transform()</code> \u274c Missing P2 <code>ROC()</code> <code>roc_curve()</code> \u274c Missing P2 Generic <code>ROC.default()</code> <code>roc_default()</code> \u274c Missing P2 <code>ROC.anota()</code> <code>roc_anota()</code> \u274c Missing P2 <code>AUC()</code> <code>auc()</code> \u274c Missing P2 Generic <code>AUC.default()</code> <code>auc_default()</code> \u274c Missing P2"},{"location":"GAP_ANALYSIS/#212-utilities-utilsr","title":"2.12 Utilities (<code>utils.R</code>)","text":"R Function Python Target Status Priority Notes <code>getPguess()</code> <code>get_p_guess()</code> \u274c Missing P0 <code>getFamily()</code> <code>get_link()</code> \u274c Missing P0 <code>rescale()</code> <code>rescale()</code> \u274c Missing P0 Core utility <code>pc2pd()</code> <code>pc_to_pd()</code> \u274c Missing P0 <code>pd2pc()</code> <code>pd_to_pc()</code> \u274c Missing P0 <code>psyfun()</code> <code>psy_fun()</code> \u274c Missing P0 <code>psyinv()</code> <code>psy_inv()</code> \u274c Missing P0 <code>psyderiv()</code> <code>psy_deriv()</code> \u274c Missing P0 <code>findcr()</code> <code>find_critical()</code> \u274c Missing P1 <code>test.crit()</code> <code>_test_crit()</code> \u274c Missing P1 Internal <code>delimit()</code> <code>delimit()</code> \u274c Missing P0 <code>normalPvalue()</code> <code>normal_pvalue()</code> \u274c Missing P0 <code>lrp_binom()</code> <code>_lr_pvalue_binom()</code> \u274c Missing P2 Internal <code>Waldp_binom()</code> <code>_wald_pvalue_binom()</code> \u274c Missing P2 Internal"},{"location":"GAP_ANALYSIS/#3-feature-parity-checklist","title":"3. Feature Parity Checklist","text":""},{"location":"GAP_ANALYSIS/#31-protocol-support","title":"3.1 Protocol Support","text":"Protocol Single Double Notes Duo-trio \u274c \u274c Triangle \u274c \u274c Tetrad \u274c \u274c 2-AFC \u274c \u274c 3-AFC \u274c \u274c Hexad \u274c N/A No double version in R 2/5 \u274c N/A No double version in R 2/5F \u274c N/A No double version in R"},{"location":"GAP_ANALYSIS/#32-statistical-methods","title":"3.2 Statistical Methods","text":"Feature discrim betabin twoAC samediff dod ML Estimation \u274c \u274c \u274c \u274c \u274c Wald CI \u274c \u274c \u274c \u274c \u274c Profile CI \u274c \u274c \u274c \u274c \u274c LR Test \u274c \u274c \u274c \u274c \u274c Wald Test \u274c \u274c \u274c \u274c \u274c Score Test \u274c N/A N/A N/A N/A"},{"location":"GAP_ANALYSIS/#33-analysis-features","title":"3.3 Analysis Features","text":"Feature Status Notes Power Analysis \u274c All methods Sample Size \u274c All methods Simulation \u274c discrimSim, dodSim, etc. d-prime Tests \u274c dprime_test, dprime_compare Post-hoc \u274c With letter displays ROC/AUC \u274c SDT transform"},{"location":"GAP_ANALYSIS/#34-visualization","title":"3.4 Visualization","text":"Feature Status Notes Profile plots \u274c plot.discrim, etc. Psychometric curves \u274c ROC curves \u274c Diagnostic plots \u274c"},{"location":"GAP_ANALYSIS/#35-integration-features","title":"3.5 Integration Features","text":"Feature Status Notes Pandas integration \u274c DataFrames for input/output NumPy arrays \u274c Vector operations matplotlib plots \u274c Publication quality Numba acceleration \u274c Performance critical paths"},{"location":"GAP_ANALYSIS/#4-priority-implementation-queue","title":"4. Priority Implementation Queue","text":""},{"location":"GAP_ANALYSIS/#41-phase-0-foundation-priority-p0","title":"4.1 Phase 0 (Foundation) - Priority P0","text":"<p>Must be implemented first as dependencies for everything else:</p> <pre><code>1. senspy/core/types.py          # Type definitions\n2. senspy/core/base.py           # Base classes\n3. senspy/utils/stats.py         # delimit, normalPvalue\n4. senspy/utils/transforms.py    # pc2pd, pd2pc, rescale\n5. senspy/links/psychometric.py  # All 8 single link functions\n6. senspy/__init__.py            # Public API\n</code></pre> <p>Estimated Test Cases: ~150 unit tests, ~100 golden data tests</p>"},{"location":"GAP_ANALYSIS/#42-phase-1-core-models-priority-p1","title":"4.2 Phase 1 (Core Models) - Priority P1","text":"<p>Core discrimination analysis and statistical models:</p> <pre><code>7. senspy/discrimination/discrim.py   # discrim()\n8. senspy/models/betabinomial.py      # BetaBinomial\n9. senspy/power/discrim_power.py      # discrimPwr, d.primePwr\n10. senspy/power/sample_size.py       # discrimSS, d.primeSS\n11. senspy/inference/dprime_tests.py  # dprime_test, dprime_compare\n</code></pre> <p>Estimated Test Cases: ~200 unit tests, ~150 golden data tests</p>"},{"location":"GAP_ANALYSIS/#43-phase-2-advanced-models-priority-p2","title":"4.3 Phase 2 (Advanced Models) - Priority P2","text":"<p>Specialized models and advanced features:</p> <pre><code>12. senspy/links/double.py            # 5 double link functions\n13. senspy/models/twoac.py            # TwoAC\n14. senspy/models/samediff.py         # SameDiff\n15. senspy/models/dod.py              # DOD\n16. senspy/power/dod_power.py         # dodPwr\n17. senspy/discrimination/anota.py    # AnotA\n18. senspy/roc/                       # SDT, ROC, AUC\n</code></pre> <p>Estimated Test Cases: ~300 unit tests, ~200 golden data tests</p>"},{"location":"GAP_ANALYSIS/#44-phase-3-polish-priority-p3","title":"4.4 Phase 3 (Polish) - Priority P3","text":"<p>Visualization and integration:</p> <pre><code>19. senspy/plotting/psychometric.py   # Psychometric curves\n20. senspy/plotting/profile.py        # Profile plots\n21. senspy/plotting/roc.py            # ROC curves\n22. senspy/datasets/                  # Example datasets\n23. Documentation                     # Sphinx docs\n24. Tutorials                         # Jupyter notebooks\n</code></pre> <p>Estimated Test Cases: ~100 integration tests, visual comparison tests</p>"},{"location":"GAP_ANALYSIS/#appendix-a-mapping-reference","title":"Appendix A: Mapping Reference","text":""},{"location":"GAP_ANALYSIS/#a1-r-file-to-python-module-mapping","title":"A.1 R File to Python Module Mapping","text":"R Source File Python Module Functions <code>links.R</code> <code>senspy.links.psychometric</code> 8 <code>doublelinks.R</code> <code>senspy.links.double</code> 5 <code>discrim.R</code> <code>senspy.discrimination.discrim</code> 4 <code>betaBin.R</code> <code>senspy.models.betabinomial</code> 5 <code>twoAC.R</code> <code>senspy.models.twoac</code> 8 <code>samediff.R</code> <code>senspy.models.samediff</code> 6 <code>dod.R</code> <code>senspy.models.dod</code> 8+ <code>dod_power.R</code> <code>senspy.power.dod_power</code> 3 <code>ROC.R</code> <code>senspy.roc</code> 6 <code>power.R</code> <code>senspy.power.discrim_power</code> 4 <code>newPower.R</code> <code>senspy.power.discrim_power</code> 5 <code>sample.size.R</code> <code>senspy.power.sample_size</code> 4 <code>d.primeTest.R</code> <code>senspy.inference.dprime_tests</code> 8 <code>utils.R</code> <code>senspy.utils</code> 14"},{"location":"GAP_ANALYSIS/#a2-r-function-name-to-python-name-conventions","title":"A.2 R Function Name to Python Name Conventions","text":"R Convention Python Convention Example <code>camelCase</code> <code>snake_case</code> <code>getPguess</code> \u2192 <code>get_p_guess</code> <code>dot.separated</code> <code>snake_case</code> <code>d.prime</code> \u2192 <code>d_prime</code> <code>S3.method</code> <code>class.method()</code> <code>print.discrim</code> \u2192 <code>Discrim.__str__()</code> <code>internal_</code> <code>_private</code> <code>dod_nll_internal</code> \u2192 <code>_dod_nll</code>"},{"location":"GAP_ANALYSIS/#appendix-b-test-case-estimates","title":"Appendix B: Test Case Estimates","text":"Category Unit Tests Golden Tests Property Tests Total Links 80 60 30 170 Discrimination 40 50 10 100 Models 120 100 20 240 Power 60 80 10 150 Inference 40 50 10 100 ROC 20 30 5 55 Utilities 60 30 20 110 Integration 30 - - 30 TOTAL 450 400 105 955"},{"location":"GAP_ANALYSIS/#document-history","title":"Document History","text":"Version Date Author Changes 1.0 2025-12-16 Claude Initial gap analysis"},{"location":"PORTING_PLAN/","title":"sensR to sensPy: Master Porting Plan","text":"<p>Version: 1.0 Date: 2025-12-16 Status: Approved Scope: Complete rewrite of senspy from scratch</p>"},{"location":"PORTING_PLAN/#executive-summary","title":"Executive Summary","text":"<p>This document defines the authoritative roadmap for porting the sensR R package (v1.5-3) to sensPy, a modern Python library for Thurstonian models in sensory discrimination. The port will achieve 1:1 numerical parity with R while leveraging Python best practices: type hints, NumPy/SciPy integration, Numba optimization, and comprehensive testing.</p>"},{"location":"PORTING_PLAN/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Project Overview</li> <li>Architecture Design</li> <li>Module Structure</li> <li>Function Inventory</li> <li>Implementation Phases</li> <li>Testing Strategy</li> <li>Quality Standards</li> <li>Dependencies</li> </ol>"},{"location":"PORTING_PLAN/#1-project-overview","title":"1. Project Overview","text":""},{"location":"PORTING_PLAN/#11-source-package-sensr","title":"1.1 Source Package: sensR","text":"<p>The sensR package provides methods for sensory discrimination:</p> <ul> <li>Protocols: duotrio, tetrad, triangle, 2-AFC, 3-AFC, A-not A, same-different, 2-AC, degree-of-difference (DOD)</li> <li>Capabilities: d-prime estimation, standard errors, confidence intervals, power analysis, sample size calculations</li> <li>Statistical Methods: Maximum likelihood, profile likelihood, Wald/likelihood ratio tests</li> </ul>"},{"location":"PORTING_PLAN/#12-target-package-senspy","title":"1.2 Target Package: sensPy","text":"<p>sensPy will be a complete Python port maintaining: - Identical numerical results (within floating-point tolerance) - Pythonic API design following scikit-learn conventions - Modern Python features (type hints, dataclasses, protocols) - Performance optimization via Numba where beneficial</p>"},{"location":"PORTING_PLAN/#13-key-design-decisions","title":"1.3 Key Design Decisions","text":"Decision Choice Rationale Existing code Delete &amp; Rewrite Fresh start ensures clean architecture Testing Hybrid RPy2/Static RPy2 for golden data generation, static fixtures for CI Scope Full (functions + plots + data) Complete feature parity Min Python 3.9+ Modern type hints, dataclasses Optimization Numba @njit for hot paths 10-100x speedup for likelihood loops"},{"location":"PORTING_PLAN/#2-architecture-design","title":"2. Architecture Design","text":""},{"location":"PORTING_PLAN/#21-package-layout","title":"2.1 Package Layout","text":"<pre><code>senspy/\n\u251c\u2500\u2500 __init__.py              # Public API exports\n\u251c\u2500\u2500 _version.py              # Version info\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 types.py             # Type definitions, protocols\n\u2502   \u2514\u2500\u2500 base.py              # Base classes (StatsModel, Protocol)\n\u251c\u2500\u2500 links/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 psychometric.py      # Link functions (duotrio, triangle, etc.)\n\u2502   \u2514\u2500\u2500 double.py            # Double protocol links\n\u251c\u2500\u2500 discrimination/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 discrim.py           # discrim() main function\n\u2502   \u251c\u2500\u2500 anota.py             # A-Not-A protocol\n\u2502   \u2514\u2500\u2500 protocols.py         # Protocol enumeration and helpers\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 betabinomial.py      # BetaBinomial class\n\u2502   \u251c\u2500\u2500 twoac.py             # TwoAC class\n\u2502   \u251c\u2500\u2500 samediff.py          # SameDiff class\n\u2502   \u2514\u2500\u2500 dod.py               # DOD class\n\u251c\u2500\u2500 power/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 discrim_power.py     # discrimPwr, d.primePwr\n\u2502   \u251c\u2500\u2500 sample_size.py       # discrimSS, d.primeSS\n\u2502   \u2514\u2500\u2500 dod_power.py         # dodPwr\n\u251c\u2500\u2500 inference/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 dprime_tests.py      # dprime_test, dprime_compare, posthoc\n\u2502   \u2514\u2500\u2500 confidence.py        # Profile likelihood CI utilities\n\u251c\u2500\u2500 roc/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 sdt.py               # SDT transform\n\u2502   \u2514\u2500\u2500 auc.py               # ROC, AUC functions\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 transforms.py        # pc2pd, pd2pc, rescale\n\u2502   \u251c\u2500\u2500 critical.py          # findcr, test.crit\n\u2502   \u2514\u2500\u2500 stats.py             # normalPvalue, delimit\n\u251c\u2500\u2500 plotting/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 psychometric.py      # Psychometric function plots\n\u2502   \u251c\u2500\u2500 profile.py           # Profile likelihood plots\n\u2502   \u2514\u2500\u2500 roc.py               # ROC curve plots\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 data.py              # Built-in example datasets\n\u2514\u2500\u2500 _numba/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 likelihoods.py       # JIT-compiled likelihood functions\n</code></pre>"},{"location":"PORTING_PLAN/#22-class-hierarchy","title":"2.2 Class Hierarchy","text":"<pre><code>StatsModel (ABC)\n\u251c\u2500\u2500 BetaBinomial\n\u251c\u2500\u2500 TwoAC\n\u251c\u2500\u2500 SameDiff\n\u2514\u2500\u2500 DOD\n\nProtocolLink (Protocol)\n\u251c\u2500\u2500 DuoTrioLink\n\u251c\u2500\u2500 TriangleLink\n\u251c\u2500\u2500 TetradLink\n\u251c\u2500\u2500 TwoAFCLink\n\u251c\u2500\u2500 ThreeAFCLink\n\u251c\u2500\u2500 HexadLink\n\u251c\u2500\u2500 TwoFiveLink\n\u2514\u2500\u2500 TwoFiveFLink\n\nResult (ABC)\n\u251c\u2500\u2500 DiscrimResult\n\u251c\u2500\u2500 BetaBinomialResult\n\u251c\u2500\u2500 TwoACResult\n\u251c\u2500\u2500 SameDiffResult\n\u2514\u2500\u2500 DODResult\n</code></pre>"},{"location":"PORTING_PLAN/#23-core-interfaces","title":"2.3 Core Interfaces","text":"<pre><code>from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Protocol, Optional\nimport numpy as np\nfrom numpy.typing import NDArray\n\nclass StatsModel(ABC):\n    \"\"\"Base class for all statistical models.\"\"\"\n\n    @abstractmethod\n    def fit(self, data: NDArray) -&gt; \"StatsModel\":\n        \"\"\"Fit the model to data.\"\"\"\n        ...\n\n    @abstractmethod\n    def summary(self) -&gt; str:\n        \"\"\"Return model summary.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def coefficients(self) -&gt; NDArray:\n        \"\"\"Model coefficients.\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def vcov(self) -&gt; Optional[NDArray]:\n        \"\"\"Variance-covariance matrix.\"\"\"\n        ...\n\n    @abstractmethod\n    def confint(self, level: float = 0.95) -&gt; NDArray:\n        \"\"\"Confidence intervals.\"\"\"\n        ...\n\nclass ProtocolLink(Protocol):\n    \"\"\"Protocol for psychometric link functions.\"\"\"\n\n    @property\n    def name(self) -&gt; str: ...\n\n    @property\n    def p_guess(self) -&gt; float: ...\n\n    def linkinv(self, eta: NDArray) -&gt; NDArray:\n        \"\"\"d-prime to probability (pc).\"\"\"\n        ...\n\n    def linkfun(self, mu: NDArray) -&gt; NDArray:\n        \"\"\"Probability (pc) to d-prime.\"\"\"\n        ...\n\n    def mu_eta(self, eta: NDArray) -&gt; NDArray:\n        \"\"\"Derivative of linkinv (for delta method).\"\"\"\n        ...\n</code></pre>"},{"location":"PORTING_PLAN/#3-module-structure","title":"3. Module Structure","text":""},{"location":"PORTING_PLAN/#31-r-to-python-module-mapping","title":"3.1 R-to-Python Module Mapping","text":"R File Python Module Description <code>links.R</code> <code>senspy.links.psychometric</code> Single protocol psychometric functions <code>doublelinks.R</code> <code>senspy.links.double</code> Double protocol psychometric functions <code>discrim.R</code> <code>senspy.discrimination.discrim</code> Main discrimination analysis <code>betaBin.R</code> <code>senspy.models.betabinomial</code> Beta-binomial overdispersion model <code>twoAC.R</code> <code>senspy.models.twoac</code> Two-alternative certainty model <code>samediff.R</code> <code>senspy.models.samediff</code> Same-different protocol model <code>dod.R</code> <code>senspy.models.dod</code> Degree-of-difference model <code>dod_power.R</code> <code>senspy.power.dod_power</code> DOD power analysis <code>ROC.R</code> <code>senspy.roc</code> ROC curves and AUC <code>power.R</code> <code>senspy.power.discrim_power</code> Discrimination power analysis <code>newPower.R</code> <code>senspy.power.discrim_power</code> Alternative power implementations <code>sample.size.R</code> <code>senspy.power.sample_size</code> Sample size calculations <code>d.primeTest.R</code> <code>senspy.inference.dprime_tests</code> d-prime comparison tests <code>utils.R</code> <code>senspy.utils</code> Utility functions <code>clls.R</code> <code>senspy.inference</code> (deprecated, skip) <code>discrimR.R</code> - (deprecated, skip) <code>warning_functions.R</code> - (R-specific, skip)"},{"location":"PORTING_PLAN/#4-function-inventory","title":"4. Function Inventory","text":""},{"location":"PORTING_PLAN/#41-core-discrimination-functions","title":"4.1 Core Discrimination Functions","text":"R Function Python Function Module Priority <code>discrim()</code> <code>discrim()</code> <code>discrimination.discrim</code> P0 <code>AnotA()</code> <code>anota()</code> <code>discrimination.anota</code> P1 <code>discrimSim()</code> <code>discrim_sim()</code> <code>discrimination.discrim</code> P2 <code>rescale()</code> <code>rescale()</code> <code>utils.transforms</code> P0"},{"location":"PORTING_PLAN/#42-link-functions-p0-highest-priority","title":"4.2 Link Functions (P0 - Highest Priority)","text":"R Function Python Function Module <code>duotrio()</code> <code>DuoTrioLink</code> <code>links.psychometric</code> <code>triangle()</code> <code>TriangleLink</code> <code>links.psychometric</code> <code>twoAFC()</code> <code>TwoAFCLink</code> <code>links.psychometric</code> <code>threeAFC()</code> <code>ThreeAFCLink</code> <code>links.psychometric</code> <code>tetrad()</code> <code>TetradLink</code> <code>links.psychometric</code> <code>hexad()</code> <code>HexadLink</code> <code>links.psychometric</code> <code>twofive()</code> <code>TwoFiveLink</code> <code>links.psychometric</code> <code>twofiveF()</code> <code>TwoFiveFLink</code> <code>links.psychometric</code> <code>doubleduotrio()</code> <code>DoubleDuoTrioLink</code> <code>links.double</code> <code>doubletriangle()</code> <code>DoubleTriangleLink</code> <code>links.double</code> <code>doubletwoAFC()</code> <code>DoubleTwoAFCLink</code> <code>links.double</code> <code>doublethreeAFC()</code> <code>DoubleThreeAFCLink</code> <code>links.double</code> <code>doubletetrad()</code> <code>DoubleTetradLink</code> <code>links.double</code>"},{"location":"PORTING_PLAN/#43-statistical-models-p0-p1","title":"4.3 Statistical Models (P0-P1)","text":"R Function Python Class Module Priority <code>betabin()</code> <code>BetaBinomial</code> <code>models.betabinomial</code> P0 <code>twoAC()</code> <code>TwoAC</code> <code>models.twoac</code> P1 <code>samediff()</code> <code>SameDiff</code> <code>models.samediff</code> P1 <code>dod()</code> <code>DOD</code> <code>models.dod</code> P1"},{"location":"PORTING_PLAN/#44-power-sample-size-p1","title":"4.4 Power &amp; Sample Size (P1)","text":"R Function Python Function Module <code>discrimPwr()</code> <code>discrim_power()</code> <code>power.discrim_power</code> <code>d.primePwr()</code> <code>dprime_power()</code> <code>power.discrim_power</code> <code>discrimSS()</code> <code>discrim_sample_size()</code> <code>power.sample_size</code> <code>d.primeSS()</code> <code>dprime_sample_size()</code> <code>power.sample_size</code> <code>twoACpwr()</code> <code>twoac_power()</code> <code>power.twoac_power</code> <code>dodPwr()</code> <code>dod_power()</code> <code>power.dod_power</code>"},{"location":"PORTING_PLAN/#45-inference-testing-p1","title":"4.5 Inference &amp; Testing (P1)","text":"R Function Python Function Module <code>dprime_test()</code> <code>dprime_test()</code> <code>inference.dprime_tests</code> <code>dprime_compare()</code> <code>dprime_compare()</code> <code>inference.dprime_tests</code> <code>posthoc()</code> <code>posthoc()</code> <code>inference.dprime_tests</code>"},{"location":"PORTING_PLAN/#46-roc-auc-p2","title":"4.6 ROC &amp; AUC (P2)","text":"R Function Python Function Module <code>SDT()</code> <code>sdt_transform()</code> <code>roc.sdt</code> <code>ROC()</code> <code>roc_curve()</code> <code>roc.roc</code> <code>AUC()</code> <code>auc()</code> <code>roc.auc</code>"},{"location":"PORTING_PLAN/#47-utility-functions-p0","title":"4.7 Utility Functions (P0)","text":"R Function Python Function Module <code>getPguess()</code> <code>get_p_guess()</code> <code>utils.stats</code> <code>getFamily()</code> <code>get_link()</code> <code>links</code> <code>pc2pd()</code> <code>pc_to_pd()</code> <code>utils.transforms</code> <code>pd2pc()</code> <code>pd_to_pc()</code> <code>utils.transforms</code> <code>psyfun()</code> <code>psy_fun()</code> <code>utils.transforms</code> <code>psyinv()</code> <code>psy_inv()</code> <code>utils.transforms</code> <code>psyderiv()</code> <code>psy_deriv()</code> <code>utils.transforms</code> <code>findcr()</code> <code>find_critical()</code> <code>utils.critical</code> <code>delimit()</code> <code>delimit()</code> <code>utils.stats</code> <code>normalPvalue()</code> <code>normal_pvalue()</code> <code>utils.stats</code>"},{"location":"PORTING_PLAN/#5-implementation-phases","title":"5. Implementation Phases","text":""},{"location":"PORTING_PLAN/#phase-0-foundation-week-1-2","title":"Phase 0: Foundation (Week 1-2)","text":"<p>Goal: Core infrastructure and fundamental functions</p> <p>Deliverables: - [ ] Package skeleton with pyproject.toml - [ ] Type definitions (<code>core/types.py</code>) - [ ] Base classes (<code>core/base.py</code>) - [ ] All psychometric link functions (<code>links/</code>) - [ ] Utility transforms (<code>utils/transforms.py</code>) - [ ] <code>rescale()</code> function - [ ] Basic test infrastructure with golden data</p> <p>Acceptance Criteria: - All link functions produce identical results to R (tolerance: 1e-12) - <code>rescale()</code> matches R output exactly</p>"},{"location":"PORTING_PLAN/#phase-1-core-models-week-3-5","title":"Phase 1: Core Models (Week 3-5)","text":"<p>Goal: Main statistical models and discrimination analysis</p> <p>Deliverables: - [ ] <code>discrim()</code> function with all statistics (exact, likelihood, Wald, score) - [ ] <code>BetaBinomial</code> class with fit/summary/confint - [ ] <code>TwoAC</code> class with profile likelihood - [ ] <code>SameDiff</code> class - [ ] Power functions (<code>discrim_power</code>, <code>dprime_power</code>) - [ ] Sample size functions (<code>discrim_sample_size</code>, <code>dprime_sample_size</code>)</p> <p>Acceptance Criteria: - Coefficients match R within 1e-10 - Log-likelihoods match R within 1e-9 - Confidence intervals match R within 1e-6</p>"},{"location":"PORTING_PLAN/#phase-2-advanced-models-week-6-8","title":"Phase 2: Advanced Models (Week 6-8)","text":"<p>Goal: DOD model and d-prime comparison tests</p> <p>Deliverables: - [ ] <code>DOD</code> class with all fitting options - [ ] <code>dod_power()</code> with simulation - [ ] <code>dprime_test()</code> function - [ ] <code>dprime_compare()</code> function - [ ] <code>posthoc()</code> with letter displays - [ ] <code>A-Not-A</code> protocol</p> <p>Acceptance Criteria: - All test statistics match R output - P-values match within 1e-8 - Letter displays identical to R</p>"},{"location":"PORTING_PLAN/#phase-3-visualization-polish-week-9-10","title":"Phase 3: Visualization &amp; Polish (Week 9-10)","text":"<p>Goal: Plotting functions and documentation</p> <p>Deliverables: - [ ] Profile likelihood plots - [ ] Psychometric function plots - [ ] ROC curves - [ ] Complete API documentation - [ ] Tutorial notebooks - [ ] Performance benchmarks</p> <p>Acceptance Criteria: - Plots visually match R output - Documentation 100% complete - All benchmarks pass</p>"},{"location":"PORTING_PLAN/#6-testing-strategy","title":"6. Testing Strategy","text":"<p>See TESTING_STRATEGY.md for complete details.</p>"},{"location":"PORTING_PLAN/#61-test-types","title":"6.1 Test Types","text":"Type Purpose Tool Unit Individual function correctness pytest Golden Data 1:1 R parity validation pytest + fixtures Property Mathematical invariants hypothesis Integration End-to-end workflows pytest Benchmark Performance regression pytest-benchmark"},{"location":"PORTING_PLAN/#62-r-validation-approach-hybrid","title":"6.2 R Validation Approach (Hybrid)","text":"<p>Development Phase: - Use RPy2 to call sensR directly and compare outputs - Generate golden data fixtures from RPy2 calls - Store fixtures as JSON for CI portability</p> <p>CI Phase: - Load static JSON fixtures - No R dependency required - Fast execution (~30s total)</p>"},{"location":"PORTING_PLAN/#63-tolerances","title":"6.3 Tolerances","text":"Metric Tolerance Rationale Coefficients 1e-10 Numerical precision of optimization Log-likelihood 1e-9 Accumulated floating-point error P-values 1e-8 Distribution function precision Confidence intervals 1e-6 Profile likelihood interpolation Standard errors 1e-8 Hessian inversion precision"},{"location":"PORTING_PLAN/#7-quality-standards","title":"7. Quality Standards","text":""},{"location":"PORTING_PLAN/#71-code-standards","title":"7.1 Code Standards","text":"<ul> <li>Type hints: Required on all public functions</li> <li>Docstrings: NumPy style, required on all public API</li> <li>Line length: 88 characters (Black default)</li> <li>Imports: isort with black profile</li> <li>Linting: ruff with strict settings</li> </ul>"},{"location":"PORTING_PLAN/#72-documentation-standards","title":"7.2 Documentation Standards","text":"<ul> <li>Every public function has:</li> <li>Parameter descriptions</li> <li>Return type documentation</li> <li>At least one example</li> <li>Reference to corresponding R function</li> <li>Module-level docstrings explain purpose and usage</li> </ul>"},{"location":"PORTING_PLAN/#73-test-coverage-requirements","title":"7.3 Test Coverage Requirements","text":"<ul> <li>Minimum 90% line coverage</li> <li>100% coverage for core numerical functions</li> <li>Every R function parity test</li> </ul>"},{"location":"PORTING_PLAN/#8-dependencies","title":"8. Dependencies","text":""},{"location":"PORTING_PLAN/#81-runtime-dependencies","title":"8.1 Runtime Dependencies","text":"<pre><code>[project]\ndependencies = [\n    \"numpy&gt;=1.20\",\n    \"scipy&gt;=1.7\",\n    \"pandas&gt;=1.3\",\n    \"numba&gt;=0.55\",\n    \"matplotlib&gt;=3.4\",\n]\n</code></pre>"},{"location":"PORTING_PLAN/#82-development-dependencies","title":"8.2 Development Dependencies","text":"<pre><code>[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=7.0\",\n    \"pytest-cov&gt;=4.0\",\n    \"hypothesis&gt;=6.0\",\n    \"rpy2&gt;=3.5\",  # For golden data generation\n    \"black&gt;=23.0\",\n    \"ruff&gt;=0.1\",\n    \"mypy&gt;=1.0\",\n    \"sphinx&gt;=6.0\",\n    \"sphinx-rtd-theme&gt;=1.0\",\n]\n</code></pre>"},{"location":"PORTING_PLAN/#appendix-a-sensr-function-reference","title":"Appendix A: sensR Function Reference","text":""},{"location":"PORTING_PLAN/#a1-exported-functions-namespace","title":"A.1 Exported Functions (NAMESPACE)","text":"<pre><code># Core discrimination\ndiscrim, AnotA, discrimSim\n\n# Link functions\nduotrio, triangle, tetrad, twoAFC, threeAFC, hexad, twofive, twofiveF\ndoubleduotrio, doubletriangle, doubletetrad, doubletwoAFC, doublethreeAFC\n\n# Models\nbetabin, twoAC, samediff, dod, dod_fit\n\n# Power &amp; sample size\ndiscrimPwr, d.primePwr, discrimSS, d.primeSS\ntwoACpwr, dodPwr, samediffPwr\n\n# Inference\ndprime_test, dprime_compare, posthoc\n\n# ROC\nSDT, ROC, AUC\n\n# Utilities\nrescale, getPguess, getFamily\npc2pd, pd2pc, psyfun, psyinv, psyderiv\nfindcr, delimit, normalPvalue\n\n# S3 methods\nprint.*, summary.*, plot.*, confint.*, profile.*, vcov.*, logLik.*\n</code></pre>"},{"location":"PORTING_PLAN/#a2-internal-functions-not-exported","title":"A.2 Internal Functions (Not Exported)","text":"<p>These may be implemented as private functions or skipped:</p> <pre><code># Likelihood functions\nnll.2AC, llSameDiff, dod_nll_internal, dprime_nll\n\n# Helpers\nprofBinom, bbEnvir, estimate.2AC, LRtest.2AC\ngetPosthoc, dprime_table, dprime_estim\n\n# Deprecated\ndiscrimOld, discrimR\n</code></pre>"},{"location":"PORTING_PLAN/#appendix-b-r-test-cases","title":"Appendix B: R Test Cases","text":"<p>Priority test cases for validation:</p> <pre><code># Link functions\nlibrary(sensR)\nduotrio()$linkinv(1.5)  # Expected: 0.7659283\ntriangle()$linkfun(0.6)  # Expected: 1.225537\n\n# discrim\ndiscrim(correct=80, total=100, method=\"triangle\")\n# d.prime: 2.165, pc: 0.8, pd: 0.7\n\n# betabin\ndata &lt;- matrix(c(3,6,5,8,9,5,4,7,8,10,6,6,5,5,6,7), ncol=2)\nbetabin(data, method=\"triangle\")\n\n# twoAC\ntwoAC(c(25, 35, 40))\n\n# power\ndiscrimPwr(pdA=0.3, sample.size=100, pGuess=1/3)\nd.primeSS(d.primeA=1.0, method=\"triangle\", target.power=0.8)\n</code></pre>"},{"location":"PORTING_PLAN/#document-history","title":"Document History","text":"Version Date Author Changes 1.0 2025-12-16 Claude Initial comprehensive plan"},{"location":"TESTING_STRATEGY/","title":"sensR to sensPy: Testing Strategy","text":"<p>Version: 1.0 Date: 2025-12-16 Approach: Hybrid (RPy2 development + Static fixtures for CI)</p>"},{"location":"TESTING_STRATEGY/#overview","title":"Overview","text":"<p>This document defines the testing strategy for ensuring 1:1 numerical parity between sensPy (Python) and sensR (R). The hybrid approach uses RPy2 for development-time validation and golden data generation, while CI tests run against pre-generated static fixtures for portability.</p>"},{"location":"TESTING_STRATEGY/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Testing Philosophy</li> <li>Test Types</li> <li>Golden Data System</li> <li>Tolerance Standards</li> <li>Directory Structure</li> <li>RPy2 Usage Guide</li> <li>CI Configuration</li> <li>Test Examples</li> </ol>"},{"location":"TESTING_STRATEGY/#1-testing-philosophy","title":"1. Testing Philosophy","text":""},{"location":"TESTING_STRATEGY/#11-core-principles","title":"1.1 Core Principles","text":"<ol> <li>R is the reference: All numerical results must match sensR output</li> <li>Reproducibility: Tests must produce identical results across runs</li> <li>Isolation: Tests should not depend on external state</li> <li>Speed: CI tests must complete in under 60 seconds</li> <li>Portability: CI should not require R installation</li> </ol>"},{"location":"TESTING_STRATEGY/#12-testing-pyramid","title":"1.2 Testing Pyramid","text":"<pre><code>         /\\\n        /  \\      Integration Tests (10%)\n       /----\\     End-to-end workflows\n      /      \\\n     /--------\\   Golden Data Tests (30%)\n    /          \\  R parity validation\n   /------------\\\n  /              \\  Unit Tests (60%)\n /________________\\ Individual functions\n</code></pre>"},{"location":"TESTING_STRATEGY/#2-test-types","title":"2. Test Types","text":""},{"location":"TESTING_STRATEGY/#21-unit-tests","title":"2.1 Unit Tests","text":"<p>Purpose: Test individual functions in isolation</p> <p>Characteristics: - Fast execution (&lt;10ms each) - No external dependencies - Test edge cases and error handling - Located in <code>tests/unit/</code></p> <p>Example domains: - Link function calculations at specific d-prime values - Probability transforms (pc2pd, pd2pc) - Utility functions (delimit, findcr)</p>"},{"location":"TESTING_STRATEGY/#22-golden-data-tests","title":"2.2 Golden Data Tests","text":"<p>Purpose: Validate numerical parity with R</p> <p>Characteristics: - Compare Python output against pre-computed R results - Use static JSON fixtures in CI - Can regenerate fixtures with RPy2 locally - Located in <code>tests/golden/</code></p> <p>Coverage: - All exported R functions - Multiple parameter combinations per function - Edge cases (boundary conditions, extreme values)</p>"},{"location":"TESTING_STRATEGY/#23-property-based-tests","title":"2.3 Property-Based Tests","text":"<p>Purpose: Test mathematical invariants</p> <p>Characteristics: - Use Hypothesis for input generation - Test properties that must always hold - Located in <code>tests/properties/</code></p> <p>Example properties: - <code>linkfun(linkinv(x)) == x</code> for valid d-prime - <code>linkinv(x)</code> always in [p_guess, 1] - <code>confint</code> always contains point estimate</p>"},{"location":"TESTING_STRATEGY/#24-integration-tests","title":"2.4 Integration Tests","text":"<p>Purpose: Test end-to-end workflows</p> <p>Characteristics: - Test realistic analysis scenarios - May combine multiple functions - Located in <code>tests/integration/</code></p> <p>Example workflows: - Complete discrimination analysis pipeline - Power analysis followed by sample size calculation - Model fitting with confidence intervals and plots</p>"},{"location":"TESTING_STRATEGY/#3-golden-data-system","title":"3. Golden Data System","text":""},{"location":"TESTING_STRATEGY/#31-architecture","title":"3.1 Architecture","text":"<pre><code>tests/\n\u251c\u2500\u2500 golden/\n\u2502   \u251c\u2500\u2500 fixtures/              # Static JSON fixtures (committed)\n\u2502   \u2502   \u251c\u2500\u2500 links/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 duotrio.json\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 triangle.json\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u251c\u2500\u2500 discrim/\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 discrim.json\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 betabinomial.json\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 twoac.json\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 power/\n\u2502   \u2502       \u2514\u2500\u2500 discrim_power.json\n\u2502   \u251c\u2500\u2500 generate/              # RPy2 scripts to generate fixtures\n\u2502   \u2502   \u251c\u2500\u2500 generate_links.py\n\u2502   \u2502   \u251c\u2500\u2500 generate_discrim.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 test_*.py              # Golden data test modules\n\u2514\u2500\u2500 conftest.py                # Pytest fixtures for loading golden data\n</code></pre>"},{"location":"TESTING_STRATEGY/#32-fixture-format","title":"3.2 Fixture Format","text":"<p>Each fixture file is a JSON array of test cases:</p> <pre><code>{\n  \"metadata\": {\n    \"r_version\": \"4.3.1\",\n    \"sensr_version\": \"1.5-3\",\n    \"generated\": \"2025-12-16T10:00:00Z\",\n    \"generator\": \"generate_links.py\"\n  },\n  \"test_cases\": [\n    {\n      \"id\": \"duotrio_linkinv_basic\",\n      \"description\": \"Basic duotrio linkinv at d.prime=1.5\",\n      \"function\": \"duotrio()$linkinv\",\n      \"inputs\": {\n        \"eta\": 1.5\n      },\n      \"expected\": {\n        \"value\": 0.7659283383632449,\n        \"type\": \"numeric\"\n      }\n    },\n    {\n      \"id\": \"duotrio_linkinv_vector\",\n      \"description\": \"Duotrio linkinv with vector input\",\n      \"function\": \"duotrio()$linkinv\",\n      \"inputs\": {\n        \"eta\": [0.0, 0.5, 1.0, 1.5, 2.0, 3.0]\n      },\n      \"expected\": {\n        \"value\": [0.5, 0.5871962, 0.6729767, 0.7533881, 0.8246459, 0.9293309],\n        \"type\": \"numeric_vector\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"TESTING_STRATEGY/#33-generating-fixtures","title":"3.3 Generating Fixtures","text":"<p>Fixtures are generated using RPy2 scripts that call sensR:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Generate golden data fixtures for link functions.\"\"\"\n\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\n\n# Load sensR\nsensR = importr(\"sensR\")\n\ndef generate_link_fixtures():\n    \"\"\"Generate fixtures for all link functions.\"\"\"\n    fixtures = {\n        \"metadata\": {\n            \"r_version\": str(ro.r(\"R.version.string\")[0]),\n            \"sensr_version\": str(ro.r('packageVersion(\"sensR\")')[0]),\n            \"generated\": datetime.utcnow().isoformat() + \"Z\",\n            \"generator\": \"generate_links.py\"\n        },\n        \"test_cases\": []\n    }\n\n    # Test cases for duotrio\n    test_values = [0.0, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0]\n\n    # linkinv\n    link = sensR.duotrio()\n    linkinv = link.rx2(\"linkinv\")\n    for val in test_values:\n        result = linkinv(val)[0]\n        fixtures[\"test_cases\"].append({\n            \"id\": f\"duotrio_linkinv_{val}\",\n            \"function\": \"duotrio()$linkinv\",\n            \"inputs\": {\"eta\": val},\n            \"expected\": {\"value\": result, \"type\": \"numeric\"}\n        })\n\n    # linkfun\n    linkfun = link.rx2(\"linkfun\")\n    pc_values = [0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n    for val in pc_values:\n        result = linkfun(val)[0]\n        fixtures[\"test_cases\"].append({\n            \"id\": f\"duotrio_linkfun_{val}\",\n            \"function\": \"duotrio()$linkfun\",\n            \"inputs\": {\"mu\": val},\n            \"expected\": {\"value\": result, \"type\": \"numeric\"}\n        })\n\n    # ... repeat for other link functions\n\n    return fixtures\n\n\nif __name__ == \"__main__\":\n    output_path = Path(__file__).parent.parent / \"fixtures\" / \"links\" / \"duotrio.json\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    fixtures = generate_link_fixtures()\n    with open(output_path, \"w\") as f:\n        json.dump(fixtures, f, indent=2)\n\n    print(f\"Generated {len(fixtures['test_cases'])} test cases\")\n</code></pre>"},{"location":"TESTING_STRATEGY/#34-running-golden-data-generation","title":"3.4 Running Golden Data Generation","text":"<pre><code># Generate all fixtures (requires R + sensR installed)\nmake generate-golden\n\n# Generate specific fixture\npython tests/golden/generate/generate_links.py\n\n# Verify fixtures are up to date\nmake verify-golden\n</code></pre>"},{"location":"TESTING_STRATEGY/#4-tolerance-standards","title":"4. Tolerance Standards","text":""},{"location":"TESTING_STRATEGY/#41-numerical-tolerances","title":"4.1 Numerical Tolerances","text":"Metric Type Absolute Tolerance Relative Tolerance Rationale Coefficients (d-prime, pc, pd) 1e-10 1e-10 Optimization convergence Log-likelihood 1e-9 1e-9 Accumulated FP error Standard errors 1e-8 1e-8 Hessian inversion P-values 1e-8 1e-6 Distribution tail precision Confidence intervals 1e-6 1e-6 Profile interpolation Power calculations 1e-6 1e-6 Binomial summation Sample sizes 0 (exact match) - Integer result"},{"location":"TESTING_STRATEGY/#42-comparison-functions","title":"4.2 Comparison Functions","text":"<pre><code>import numpy as np\nfrom numpy.testing import assert_allclose\n\ndef assert_coefficient_match(actual, expected, name=\"coefficient\"):\n    \"\"\"Assert coefficient matches R output.\"\"\"\n    assert_allclose(\n        actual, expected,\n        atol=1e-10, rtol=1e-10,\n        err_msg=f\"{name} mismatch\"\n    )\n\ndef assert_loglik_match(actual, expected):\n    \"\"\"Assert log-likelihood matches R output.\"\"\"\n    assert_allclose(\n        actual, expected,\n        atol=1e-9, rtol=1e-9,\n        err_msg=\"Log-likelihood mismatch\"\n    )\n\ndef assert_pvalue_match(actual, expected):\n    \"\"\"Assert p-value matches R output.\"\"\"\n    assert_allclose(\n        actual, expected,\n        atol=1e-8, rtol=1e-6,\n        err_msg=\"P-value mismatch\"\n    )\n\ndef assert_confint_match(actual, expected):\n    \"\"\"Assert confidence interval matches R output.\"\"\"\n    assert_allclose(\n        actual, expected,\n        atol=1e-6, rtol=1e-6,\n        err_msg=\"Confidence interval mismatch\"\n    )\n</code></pre>"},{"location":"TESTING_STRATEGY/#43-handling-edge-cases","title":"4.3 Handling Edge Cases","text":"<p>Some edge cases require special handling:</p> <pre><code>def compare_with_edge_handling(actual, expected, tolerance):\n    \"\"\"Compare values with edge case handling.\"\"\"\n    # Handle infinities\n    if np.isinf(expected):\n        assert np.isinf(actual) and np.sign(actual) == np.sign(expected)\n        return\n\n    # Handle NaN\n    if np.isnan(expected):\n        assert np.isnan(actual)\n        return\n\n    # Handle very small values (near zero)\n    if abs(expected) &lt; 1e-15:\n        assert abs(actual) &lt; 1e-10\n        return\n\n    # Standard comparison\n    assert_allclose(actual, expected, atol=tolerance, rtol=tolerance)\n</code></pre>"},{"location":"TESTING_STRATEGY/#5-directory-structure","title":"5. Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Shared fixtures and configuration\n\u251c\u2500\u2500 unit/                    # Unit tests\n\u2502   \u251c\u2500\u2500 test_links.py\n\u2502   \u251c\u2500\u2500 test_transforms.py\n\u2502   \u251c\u2500\u2500 test_utils.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 golden/                  # Golden data tests\n\u2502   \u251c\u2500\u2500 fixtures/            # JSON fixture files\n\u2502   \u2502   \u251c\u2500\u2500 links/\n\u2502   \u2502   \u251c\u2500\u2500 discrim/\n\u2502   \u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u2514\u2500\u2500 power/\n\u2502   \u251c\u2500\u2500 generate/            # Fixture generation scripts\n\u2502   \u2502   \u251c\u2500\u2500 generate_links.py\n\u2502   \u2502   \u251c\u2500\u2500 generate_discrim.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 test_links_golden.py\n\u2502   \u251c\u2500\u2500 test_discrim_golden.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 properties/              # Property-based tests\n\u2502   \u251c\u2500\u2500 test_link_properties.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 integration/             # Integration tests\n\u2502   \u251c\u2500\u2500 test_workflows.py\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 benchmarks/              # Performance benchmarks\n    \u251c\u2500\u2500 bench_links.py\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"TESTING_STRATEGY/#6-rpy2-usage-guide","title":"6. RPy2 Usage Guide","text":""},{"location":"TESTING_STRATEGY/#61-setup","title":"6.1 Setup","text":"<pre><code># Install R (Ubuntu/Debian)\nsudo apt-get install r-base r-base-dev\n\n# Install sensR in R\nR -e 'install.packages(\"sensR\", repos=\"https://cloud.r-project.org\")'\n\n# Install rpy2\npip install rpy2\n</code></pre>"},{"location":"TESTING_STRATEGY/#62-basic-usage","title":"6.2 Basic Usage","text":"<pre><code>import rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects import numpy2ri\n\n# Enable numpy conversion\nnumpy2ri.activate()\n\n# Import sensR\nsensR = importr(\"sensR\")\n\n# Call discrim\nresult = sensR.discrim(correct=80, total=100, method=\"triangle\")\n\n# Extract values\nd_prime = result.rx2(\"coefficients\")[2, 0]  # d.prime estimate\nstd_err = result.rx2(\"coefficients\")[2, 1]  # std.err\n</code></pre>"},{"location":"TESTING_STRATEGY/#63-helper-module","title":"6.3 Helper Module","text":"<p>Create a helper module for consistent R interaction:</p> <pre><code># tests/rpy2_helper.py\n\"\"\"Helper functions for RPy2 interaction with sensR.\"\"\"\n\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, Optional\n\nimport numpy as np\n\ntry:\n    import rpy2.robjects as ro\n    from rpy2.robjects.packages import importr\n    from rpy2.robjects import numpy2ri\n\n    numpy2ri.activate()\n    HAS_RPY2 = True\nexcept ImportError:\n    HAS_RPY2 = False\n\n\ndef requires_rpy2(func):\n    \"\"\"Decorator to skip tests if RPy2 is not available.\"\"\"\n    import pytest\n\n    return pytest.mark.skipif(\n        not HAS_RPY2,\n        reason=\"RPy2 not available\"\n    )(func)\n\n\n@contextmanager\ndef r_context():\n    \"\"\"Context manager for R operations.\"\"\"\n    if not HAS_RPY2:\n        raise RuntimeError(\"RPy2 not installed\")\n\n    sensR = importr(\"sensR\")\n    yield sensR\n\n\ndef call_discrim(correct: int, total: int, method: str, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"Call sensR::discrim and return results as dict.\"\"\"\n    with r_context() as sensR:\n        result = sensR.discrim(\n            correct=correct,\n            total=total,\n            method=method,\n            **kwargs\n        )\n\n        coef = np.array(result.rx2(\"coefficients\"))\n        return {\n            \"pc\": {\"estimate\": coef[0, 0], \"std_err\": coef[0, 1]},\n            \"pd\": {\"estimate\": coef[1, 0], \"std_err\": coef[1, 1]},\n            \"d_prime\": {\"estimate\": coef[2, 0], \"std_err\": coef[2, 1]},\n            \"loglik\": float(result.rx2(\"logLik\")[0]),\n            \"p_value\": float(result.rx2(\"p.value\")[0]),\n        }\n</code></pre>"},{"location":"TESTING_STRATEGY/#64-generating-test-data-interactively","title":"6.4 Generating Test Data Interactively","text":"<pre><code># Interactive fixture development\nfrom tests.rpy2_helper import call_discrim\n\n# Test various scenarios\nscenarios = [\n    {\"correct\": 80, \"total\": 100, \"method\": \"triangle\"},\n    {\"correct\": 50, \"total\": 100, \"method\": \"twoAFC\"},\n    {\"correct\": 90, \"total\": 100, \"method\": \"tetrad\"},\n    {\"correct\": 34, \"total\": 100, \"method\": \"threeAFC\"},  # Edge: near guessing\n]\n\nfor scenario in scenarios:\n    result = call_discrim(**scenario)\n    print(f\"Scenario: {scenario}\")\n    print(f\"  d_prime: {result['d_prime']['estimate']:.10f}\")\n    print(f\"  std_err: {result['d_prime']['std_err']:.10f}\")\n    print()\n</code></pre>"},{"location":"TESTING_STRATEGY/#7-ci-configuration","title":"7. CI Configuration","text":""},{"location":"TESTING_STRATEGY/#71-github-actions-workflow","title":"7.1 GitHub Actions Workflow","text":"<pre><code># .github/workflows/test.yml\nname: Tests\n\non:\n  push:\n    branches: [main, master]\n  pull_request:\n    branches: [main, master]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: ['3.9', '3.10', '3.11', '3.12']\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -e \".[dev]\"\n\n    - name: Run tests (excluding RPy2)\n      run: |\n        pytest tests/ -v --cov=senspy --cov-report=xml \\\n          --ignore=tests/golden/generate/ \\\n          -m \"not rpy2\"\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        files: ./coverage.xml\n\n  golden-data-validation:\n    # Only run on main branch, with R installed\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up R\n      uses: r-lib/actions/setup-r@v2\n\n    - name: Install sensR\n      run: |\n        Rscript -e 'install.packages(\"sensR\", repos=\"https://cloud.r-project.org\")'\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.11'\n\n    - name: Install dependencies with RPy2\n      run: |\n        pip install -e \".[dev]\"\n\n    - name: Validate golden data freshness\n      run: |\n        python -c \"from tests.golden.validate import validate_all; validate_all()\"\n</code></pre>"},{"location":"TESTING_STRATEGY/#72-pytest-configuration","title":"7.2 Pytest Configuration","text":"<pre><code># pyproject.toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\naddopts = \"-v --strict-markers\"\nmarkers = [\n    \"rpy2: tests that require RPy2 and R\",\n    \"slow: tests that take &gt;1 second\",\n    \"benchmark: performance benchmark tests\",\n]\nfilterwarnings = [\n    \"ignore::DeprecationWarning:rpy2.*\",\n]\n</code></pre>"},{"location":"TESTING_STRATEGY/#73-marker-usage","title":"7.3 Marker Usage","text":"<pre><code>import pytest\n\n@pytest.mark.rpy2\ndef test_discrim_against_r():\n    \"\"\"Test discrim matches R output (requires RPy2).\"\"\"\n    ...\n\n@pytest.mark.slow\ndef test_dod_power_simulation():\n    \"\"\"Test DOD power with many simulations.\"\"\"\n    ...\n</code></pre>"},{"location":"TESTING_STRATEGY/#8-test-examples","title":"8. Test Examples","text":""},{"location":"TESTING_STRATEGY/#81-unit-test-example","title":"8.1 Unit Test Example","text":"<pre><code># tests/unit/test_links.py\n\"\"\"Unit tests for psychometric link functions.\"\"\"\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose\n\nfrom senspy.links import DuoTrioLink, TriangleLink\n\n\nclass TestDuoTrioLink:\n    \"\"\"Tests for the duo-trio link function.\"\"\"\n\n    @pytest.fixture\n    def link(self):\n        return DuoTrioLink()\n\n    def test_linkinv_at_zero(self, link):\n        \"\"\"linkinv(0) should equal p_guess = 0.5.\"\"\"\n        assert link.linkinv(0.0) == 0.5\n\n    def test_linkinv_at_infinity(self, link):\n        \"\"\"linkinv(inf) should approach 1.\"\"\"\n        assert_allclose(link.linkinv(20.0), 1.0, atol=1e-6)\n\n    def test_linkinv_monotonic(self, link):\n        \"\"\"linkinv should be monotonically increasing.\"\"\"\n        x = np.linspace(0, 10, 100)\n        y = link.linkinv(x)\n        assert np.all(np.diff(y) &gt;= 0)\n\n    def test_linkfun_inverse_of_linkinv(self, link):\n        \"\"\"linkfun should be the inverse of linkinv.\"\"\"\n        d_primes = np.array([0.5, 1.0, 1.5, 2.0, 3.0])\n        pc = link.linkinv(d_primes)\n        d_prime_recovered = link.linkfun(pc)\n        assert_allclose(d_prime_recovered, d_primes, atol=1e-10)\n\n    def test_mu_eta_positive(self, link):\n        \"\"\"Derivative should be positive for d.prime &gt; 0.\"\"\"\n        x = np.linspace(0.1, 10, 100)\n        deriv = link.mu_eta(x)\n        assert np.all(deriv &gt; 0)\n\n    @pytest.mark.parametrize(\"d_prime,expected_pc\", [\n        (0.0, 0.5),\n        (1.0, 0.6729767),\n        (2.0, 0.8246459),\n    ])\n    def test_linkinv_known_values(self, link, d_prime, expected_pc):\n        \"\"\"Test linkinv against known reference values.\"\"\"\n        assert_allclose(link.linkinv(d_prime), expected_pc, atol=1e-6)\n</code></pre>"},{"location":"TESTING_STRATEGY/#82-golden-data-test-example","title":"8.2 Golden Data Test Example","text":"<pre><code># tests/golden/test_links_golden.py\n\"\"\"Golden data tests for link functions.\"\"\"\n\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose\n\nfrom senspy.links import get_link\n\nFIXTURES_DIR = Path(__file__).parent / \"fixtures\" / \"links\"\n\n\ndef load_fixture(name: str) -&gt; dict:\n    \"\"\"Load a fixture file.\"\"\"\n    with open(FIXTURES_DIR / f\"{name}.json\") as f:\n        return json.load(f)\n\n\nclass TestLinksGolden:\n    \"\"\"Golden data tests for link functions.\"\"\"\n\n    @pytest.fixture(scope=\"class\")\n    def duotrio_fixture(self):\n        return load_fixture(\"duotrio\")\n\n    def test_duotrio_linkinv_golden(self, duotrio_fixture):\n        \"\"\"Test duotrio linkinv against R golden data.\"\"\"\n        link = get_link(\"duotrio\")\n\n        for case in duotrio_fixture[\"test_cases\"]:\n            if \"linkinv\" not in case[\"function\"]:\n                continue\n\n            eta = case[\"inputs\"][\"eta\"]\n            expected = case[\"expected\"][\"value\"]\n\n            if isinstance(eta, list):\n                actual = link.linkinv(np.array(eta))\n                assert_allclose(\n                    actual, expected,\n                    atol=1e-10, rtol=1e-10,\n                    err_msg=f\"Failed for case {case['id']}\"\n                )\n            else:\n                actual = link.linkinv(eta)\n                assert_allclose(\n                    actual, expected,\n                    atol=1e-10, rtol=1e-10,\n                    err_msg=f\"Failed for case {case['id']}\"\n                )\n\n\n@pytest.mark.parametrize(\"method\", [\n    \"duotrio\", \"triangle\", \"tetrad\", \"twoAFC\", \"threeAFC\",\n    \"hexad\", \"twofive\", \"twofiveF\"\n])\nclass TestAllLinksGolden:\n    \"\"\"Parameterized golden data tests for all link functions.\"\"\"\n\n    def test_linkinv_golden(self, method):\n        \"\"\"Test linkinv for each method.\"\"\"\n        fixture = load_fixture(method)\n        link = get_link(method)\n\n        linkinv_cases = [\n            c for c in fixture[\"test_cases\"]\n            if \"linkinv\" in c[\"function\"]\n        ]\n\n        for case in linkinv_cases:\n            eta = case[\"inputs\"][\"eta\"]\n            expected = case[\"expected\"][\"value\"]\n            actual = link.linkinv(np.atleast_1d(eta))\n\n            assert_allclose(\n                actual, np.atleast_1d(expected),\n                atol=1e-10, rtol=1e-10,\n                err_msg=f\"Method {method}, case {case['id']}\"\n            )\n</code></pre>"},{"location":"TESTING_STRATEGY/#83-property-based-test-example","title":"8.3 Property-Based Test Example","text":"<pre><code># tests/properties/test_link_properties.py\n\"\"\"Property-based tests for link functions.\"\"\"\n\nimport numpy as np\nfrom hypothesis import given, assume, settings\nfrom hypothesis import strategies as st\nfrom hypothesis.extra.numpy import arrays\n\nfrom senspy.links import get_link\n\n\n@given(st.sampled_from([\n    \"duotrio\", \"triangle\", \"tetrad\", \"twoAFC\", \"threeAFC\"\n]))\n@settings(max_examples=100)\ndef test_linkinv_output_range(method):\n    \"\"\"linkinv output should always be in [p_guess, 1].\"\"\"\n    link = get_link(method)\n    d_primes = np.linspace(0, 10, 50)\n\n    pc = link.linkinv(d_primes)\n\n    assert np.all(pc &gt;= link.p_guess - 1e-10)\n    assert np.all(pc &lt;= 1.0 + 1e-10)\n\n\n@given(\n    st.sampled_from([\"duotrio\", \"triangle\", \"tetrad\", \"twoAFC\", \"threeAFC\"]),\n    st.floats(min_value=0.01, max_value=10.0, allow_nan=False)\n)\ndef test_linkfun_linkinv_roundtrip(method, d_prime):\n    \"\"\"linkfun(linkinv(x)) should equal x.\"\"\"\n    link = get_link(method)\n\n    pc = link.linkinv(d_prime)\n    d_prime_recovered = link.linkfun(pc)\n\n    np.testing.assert_allclose(d_prime_recovered, d_prime, rtol=1e-8)\n\n\n@given(\n    st.sampled_from([\"duotrio\", \"triangle\", \"tetrad\", \"twoAFC\", \"threeAFC\"]),\n    arrays(\n        dtype=np.float64,\n        shape=st.integers(min_value=2, max_value=100),\n        elements=st.floats(min_value=0.0, max_value=10.0, allow_nan=False)\n    )\n)\ndef test_linkinv_monotonicity(method, d_primes):\n    \"\"\"linkinv should be monotonically increasing.\"\"\"\n    assume(len(d_primes) &gt;= 2)\n\n    link = get_link(method)\n    d_primes_sorted = np.sort(d_primes)\n\n    pc = link.linkinv(d_primes_sorted)\n\n    # Allow small numerical tolerance\n    assert np.all(np.diff(pc) &gt;= -1e-10)\n</code></pre>"},{"location":"TESTING_STRATEGY/#84-integration-test-example","title":"8.4 Integration Test Example","text":"<pre><code># tests/integration/test_workflows.py\n\"\"\"Integration tests for complete analysis workflows.\"\"\"\n\nimport numpy as np\nimport pytest\n\nfrom senspy import discrim, discrim_power, discrim_sample_size, rescale\n\n\nclass TestDiscriminationWorkflow:\n    \"\"\"Test complete discrimination analysis workflow.\"\"\"\n\n    def test_triangle_analysis_workflow(self):\n        \"\"\"Test complete triangle test analysis.\"\"\"\n        # 1. Analyze experimental data\n        result = discrim(correct=80, total=100, method=\"triangle\")\n\n        # 2. Verify estimates are reasonable\n        assert result.d_prime &gt; 0\n        assert 0 &lt; result.pc &lt; 1\n        assert 0 &lt; result.pd &lt; 1\n\n        # 3. Check confidence interval contains estimate\n        ci = result.confint(level=0.95)\n        assert ci[0] &lt;= result.d_prime &lt;= ci[1]\n\n        # 4. Compute power for detected effect\n        power = discrim_power(\n            pd_a=result.pd,\n            sample_size=100,\n            p_guess=1/3\n        )\n        assert power &gt; 0.5  # Should have decent power at observed effect\n\n        # 5. Compute sample size for 90% power\n        n_required = discrim_sample_size(\n            pd_a=result.pd,\n            target_power=0.9,\n            p_guess=1/3\n        )\n        assert n_required &gt; 0\n\n    def test_rescale_consistency(self):\n        \"\"\"Test that rescale produces consistent conversions.\"\"\"\n        # Start with d-prime\n        d_prime = 1.5\n        method = \"triangle\"\n\n        # Convert d-prime -&gt; pc -&gt; pd -&gt; d-prime (roundtrip)\n        result1 = rescale(d_prime=d_prime, method=method)\n        result2 = rescale(pc=result1.pc, method=method)\n        result3 = rescale(pd=result2.pd, method=method)\n\n        # Should recover original d-prime\n        np.testing.assert_allclose(\n            result3.d_prime, d_prime,\n            atol=1e-10,\n            err_msg=\"Rescale roundtrip failed\"\n        )\n</code></pre>"},{"location":"TESTING_STRATEGY/#appendix-makefile-targets","title":"Appendix: Makefile Targets","text":"<pre><code># Makefile\n.PHONY: test test-unit test-golden test-all generate-golden verify-golden\n\ntest:\n    pytest tests/ -v --ignore=tests/golden/generate/ -m \"not rpy2\"\n\ntest-unit:\n    pytest tests/unit/ -v\n\ntest-golden:\n    pytest tests/golden/ -v --ignore=tests/golden/generate/\n\ntest-all:\n    pytest tests/ -v\n\ntest-rpy2:\n    pytest tests/ -v -m \"rpy2\"\n\ngenerate-golden:\n    python tests/golden/generate/generate_all.py\n\nverify-golden:\n    python tests/golden/generate/verify_fixtures.py\n\ncoverage:\n    pytest tests/ --cov=senspy --cov-report=html --cov-report=term-missing\n</code></pre>"},{"location":"TESTING_STRATEGY/#document-history","title":"Document History","text":"Version Date Author Changes 1.0 2025-12-16 Claude Initial testing strategy"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to sensPy will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial release of sensPy</li> <li>Complete port of sensR (R package v1.5-3) to Python</li> <li>Psychometric link functions for all major protocols:</li> <li>Triangle, Duo-trio, 2-AFC, 3-AFC, Tetrad, Hexad, Twofive, TwofiveF</li> <li>Core discrimination analysis (<code>discrim()</code>)</li> <li>Beta-binomial model for replicated data (<code>betabin()</code>)</li> <li>Two-Alternative Certainty model (<code>twoac()</code>)</li> <li>Same-Different protocol (<code>samediff()</code>)</li> <li>Degree-of-Difference model (<code>dod()</code>, <code>dod_fit()</code>, <code>dod_sim()</code>)</li> <li>A-Not-A protocol (<code>anota()</code>)</li> <li>Power analysis functions:</li> <li><code>discrim_power()</code>, <code>dprime_power()</code></li> <li><code>dod_power()</code> with Monte Carlo simulation</li> <li>Sample size calculation:</li> <li><code>discrim_sample_size()</code>, <code>dprime_sample_size()</code></li> <li>D-prime hypothesis testing:</li> <li><code>dprime_test()</code> - single group tests</li> <li><code>dprime_compare()</code> - multiple group comparison</li> <li><code>posthoc()</code> - pairwise comparisons with CLD</li> <li>ROC and signal detection:</li> <li><code>sdt()</code> - SDT transform for rating data</li> <li><code>roc()</code> - ROC curve computation</li> <li><code>auc()</code> - Area under curve with CI</li> <li>Interactive Plotly visualizations:</li> <li><code>plot_roc()</code> - ROC curves with confidence bands</li> <li><code>plot_psychometric()</code> - psychometric functions</li> <li><code>plot_psychometric_comparison()</code> - protocol comparison</li> <li><code>plot_sdt_distributions()</code> - signal/noise distributions</li> <li><code>plot_profile_likelihood()</code> - profile likelihood plots</li> <li><code>plot_power_curve()</code> - power analysis curves</li> <li><code>plot_sample_size_curve()</code> - sample size planning</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"changelog/#deprecated","title":"Deprecated","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>N/A (initial release)</li> </ul>"},{"location":"changelog/#comparison-with-sensr","title":"Comparison with sensR","text":"<p>sensPy aims for numerical parity with sensR v1.5-3. Key differences:</p> Feature sensR (R) sensPy (Python) Plotting Base R graphics Plotly (interactive) Type hints No Yes (full typing) Result objects S3 classes Dataclasses CI methods Profile likelihood Profile likelihood Optimization <code>optim()</code> <code>scipy.optimize</code> <p>Results should match within floating-point tolerance (typically 1e-10 for coefficients).</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for all sensPy functions and classes.</p>"},{"location":"api/#module-overview","title":"Module Overview","text":"Module Description Core Functions Link functions, utilities, transforms Discrimination <code>discrim()</code>, <code>anota()</code> Models <code>betabin()</code>, <code>twoac()</code>, <code>samediff()</code>, <code>dod()</code> Power &amp; Sample Size Power analysis and sample size calculation ROC &amp; AUC Signal detection theory functions Plotting Interactive Plotly visualizations"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#basic-analysis","title":"Basic Analysis","text":"<pre><code>from senspy import discrim\n\nresult = discrim(correct=80, total=100, method=\"triangle\")\n</code></pre>"},{"location":"api/#power-analysis","title":"Power Analysis","text":"<pre><code>from senspy import discrim_power, discrim_sample_size\n\npower = discrim_power(d_prime=1.0, n=100, method=\"triangle\")\nn = discrim_sample_size(d_prime=1.0, method=\"triangle\", power=0.8)\n</code></pre>"},{"location":"api/#plotting","title":"Plotting","text":"<pre><code>from senspy import plot_roc, plot_psychometric\n\nfig = plot_roc(d_prime=1.5)\nfig = plot_psychometric(method=\"triangle\")\n</code></pre>"},{"location":"api/#conventions","title":"Conventions","text":"<ul> <li>All d-prime values are on the standard normal scale</li> <li>Proportions are in [0, 1], not percentages</li> <li>Angles (where applicable) are in radians</li> <li>P-values are two-tailed unless otherwise specified</li> </ul>"},{"location":"api/core/","title":"Core Functions","text":""},{"location":"api/core/#link-functions","title":"Link Functions","text":""},{"location":"api/core/#psy_fun","title":"psy_fun","text":""},{"location":"api/core/#senspy.psy_fun","title":"<code>psy_fun(d_prime: ArrayLike, method: str | Protocol = 'triangle') -&gt; NDArray[np.floating]</code>","text":"<p>Convert d-prime to proportion correct.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime</code> <code>array_like</code> <p>Sensitivity (d-prime). Must be non-negative.</p> required <code>method</code> <code>str or Protocol</code> <p>Discrimination protocol.</p> <code>\"triangle\"</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Proportion correct corresponding to the d-prime value(s).</p> Notes <p>Corresponds to <code>psyfun()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; psy_fun(1.5, method=\"triangle\")\narray([0.628...])\n&gt;&gt;&gt; psy_fun([0, 1, 2], method=\"twoafc\")\narray([0.5  , 0.76..., 0.92...])\n</code></pre> Source code in <code>senspy/links/psychometric.py</code> <pre><code>def psy_fun(\n    d_prime: ArrayLike,\n    method: str | Protocol = \"triangle\",\n) -&gt; NDArray[np.floating]:\n    \"\"\"Convert d-prime to proportion correct.\n\n    Parameters\n    ----------\n    d_prime : array_like\n        Sensitivity (d-prime). Must be non-negative.\n    method : str or Protocol, default \"triangle\"\n        Discrimination protocol.\n\n    Returns\n    -------\n    ndarray\n        Proportion correct corresponding to the d-prime value(s).\n\n    Notes\n    -----\n    Corresponds to `psyfun()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; psy_fun(1.5, method=\"triangle\")\n    array([0.628...])\n    &gt;&gt;&gt; psy_fun([0, 1, 2], method=\"twoafc\")\n    array([0.5  , 0.76..., 0.92...])\n    \"\"\"\n    d_prime = np.atleast_1d(np.asarray(d_prime, dtype=np.float64))\n\n    if np.any(d_prime &lt; 0):\n        raise ValueError(\"d_prime must be non-negative\")\n\n    link = get_link(method)\n    return link.linkinv(d_prime)\n</code></pre>"},{"location":"api/core/#psy_inv","title":"psy_inv","text":""},{"location":"api/core/#senspy.psy_inv","title":"<code>psy_inv(pc: ArrayLike, method: str | Protocol = 'triangle') -&gt; NDArray[np.floating]</code>","text":"<p>Convert proportion correct to d-prime.</p> <p>Parameters:</p> Name Type Description Default <code>pc</code> <code>array_like</code> <p>Proportion correct. Must be in [p_guess, 1].</p> required <code>method</code> <code>str or Protocol</code> <p>Discrimination protocol.</p> <code>\"triangle\"</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>D-prime corresponding to the proportion correct value(s).</p> Notes <p>Corresponds to <code>psyinv()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; psy_inv(0.628, method=\"triangle\")\narray([1.5...])\n&gt;&gt;&gt; psy_inv(0.8, method=\"twoafc\")\narray([1.19...])\n</code></pre> Source code in <code>senspy/links/psychometric.py</code> <pre><code>def psy_inv(\n    pc: ArrayLike,\n    method: str | Protocol = \"triangle\",\n) -&gt; NDArray[np.floating]:\n    \"\"\"Convert proportion correct to d-prime.\n\n    Parameters\n    ----------\n    pc : array_like\n        Proportion correct. Must be in [p_guess, 1].\n    method : str or Protocol, default \"triangle\"\n        Discrimination protocol.\n\n    Returns\n    -------\n    ndarray\n        D-prime corresponding to the proportion correct value(s).\n\n    Notes\n    -----\n    Corresponds to `psyinv()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; psy_inv(0.628, method=\"triangle\")\n    array([1.5...])\n    &gt;&gt;&gt; psy_inv(0.8, method=\"twoafc\")\n    array([1.19...])\n    \"\"\"\n    pc = np.atleast_1d(np.asarray(pc, dtype=np.float64))\n\n    if np.any(pc &lt; 0) or np.any(pc &gt; 1):\n        raise ValueError(\"pc must be in [0, 1]\")\n\n    link = get_link(method)\n    return link.linkfun(pc)\n</code></pre>"},{"location":"api/core/#psy_deriv","title":"psy_deriv","text":""},{"location":"api/core/#senspy.psy_deriv","title":"<code>psy_deriv(d_prime: ArrayLike, method: str | Protocol = 'triangle') -&gt; NDArray[np.floating]</code>","text":"<p>Compute the derivative of the psychometric function.</p> <p>This is d(pc)/d(d'), used in the delta method for standard error calculations.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime</code> <code>array_like</code> <p>Sensitivity (d-prime). Must be non-negative.</p> required <code>method</code> <code>str or Protocol</code> <p>Discrimination protocol.</p> <code>\"triangle\"</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Derivative of pc with respect to d-prime.</p> Notes <p>Corresponds to <code>psyderiv()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; psy_deriv(1.5, method=\"triangle\")\narray([0.29...])\n</code></pre> Source code in <code>senspy/links/psychometric.py</code> <pre><code>def psy_deriv(\n    d_prime: ArrayLike,\n    method: str | Protocol = \"triangle\",\n) -&gt; NDArray[np.floating]:\n    \"\"\"Compute the derivative of the psychometric function.\n\n    This is d(pc)/d(d'), used in the delta method for standard error\n    calculations.\n\n    Parameters\n    ----------\n    d_prime : array_like\n        Sensitivity (d-prime). Must be non-negative.\n    method : str or Protocol, default \"triangle\"\n        Discrimination protocol.\n\n    Returns\n    -------\n    ndarray\n        Derivative of pc with respect to d-prime.\n\n    Notes\n    -----\n    Corresponds to `psyderiv()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; psy_deriv(1.5, method=\"triangle\")\n    array([0.29...])\n    \"\"\"\n    d_prime = np.atleast_1d(np.asarray(d_prime, dtype=np.float64))\n\n    if np.any(d_prime &lt; 0):\n        raise ValueError(\"d_prime must be non-negative\")\n\n    link = get_link(method)\n    return link.mu_eta(d_prime)\n</code></pre>"},{"location":"api/core/#get_link","title":"get_link","text":""},{"location":"api/core/#senspy.get_link","title":"<code>get_link(method: str | Protocol) -&gt; Link</code>","text":"<p>Get the link function object for a protocol.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str or Protocol</code> <p>The discrimination protocol.</p> required <p>Returns:</p> Type Description <code>Link</code> <p>The link function object for the protocol.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; link = get_link(\"triangle\")\n&gt;&gt;&gt; link.p_guess\n0.3333333333333333\n&gt;&gt;&gt; link.linkinv(np.array([1.5]))\narray([0.628...])\n</code></pre> Source code in <code>senspy/links/psychometric.py</code> <pre><code>def get_link(method: str | Protocol) -&gt; Link:\n    \"\"\"Get the link function object for a protocol.\n\n    Parameters\n    ----------\n    method : str or Protocol\n        The discrimination protocol.\n\n    Returns\n    -------\n    Link\n        The link function object for the protocol.\n\n    Examples\n    --------\n    &gt;&gt;&gt; link = get_link(\"triangle\")\n    &gt;&gt;&gt; link.p_guess\n    0.3333333333333333\n    &gt;&gt;&gt; link.linkinv(np.array([1.5]))\n    array([0.628...])\n    \"\"\"\n    protocol = parse_protocol(method)\n    return _LINKS[protocol]\n</code></pre>"},{"location":"api/core/#transforms","title":"Transforms","text":""},{"location":"api/core/#pc_to_pd","title":"pc_to_pd","text":""},{"location":"api/core/#senspy.pc_to_pd","title":"<code>pc_to_pd(pc: ArrayLike, p_guess: float) -&gt; NDArray[np.floating]</code>","text":"<p>Convert proportion correct to proportion of discriminators.</p> <p>Parameters:</p> Name Type Description Default <code>pc</code> <code>array_like</code> <p>Proportion correct. Values should be in [p_guess, 1].</p> required <code>p_guess</code> <code>float</code> <p>Guessing probability (chance level).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Proportion of discriminators. Values in [0, 1].</p> Notes <p>The relationship is: pd = (pc - p_guess) / (1 - p_guess)</p> <p>Values of pc below p_guess are mapped to pd = 0.</p> <p>Corresponds to <code>pc2pd()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pc_to_pd(0.8, p_guess=0.5)\narray([0.6])\n&gt;&gt;&gt; pc_to_pd(0.6, p_guess=1/3)\narray([0.4])\n</code></pre> Source code in <code>senspy/utils/transforms.py</code> <pre><code>def pc_to_pd(\n    pc: ArrayLike,\n    p_guess: float,\n) -&gt; NDArray[np.floating]:\n    \"\"\"Convert proportion correct to proportion of discriminators.\n\n    Parameters\n    ----------\n    pc : array_like\n        Proportion correct. Values should be in [p_guess, 1].\n    p_guess : float\n        Guessing probability (chance level).\n\n    Returns\n    -------\n    ndarray\n        Proportion of discriminators. Values in [0, 1].\n\n    Notes\n    -----\n    The relationship is: pd = (pc - p_guess) / (1 - p_guess)\n\n    Values of pc below p_guess are mapped to pd = 0.\n\n    Corresponds to `pc2pd()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pc_to_pd(0.8, p_guess=0.5)\n    array([0.6])\n    &gt;&gt;&gt; pc_to_pd(0.6, p_guess=1/3)\n    array([0.4])\n    \"\"\"\n    pc = np.atleast_1d(np.asarray(pc, dtype=np.float64))\n\n    if not 0 &lt;= p_guess &lt;= 1:\n        raise ValueError(f\"p_guess must be in [0, 1], got {p_guess}\")\n\n    pd = (pc - p_guess) / (1 - p_guess)\n    pd = np.maximum(pd, 0.0)  # pd cannot be negative\n\n    return pd\n</code></pre>"},{"location":"api/core/#pd_to_pc","title":"pd_to_pc","text":""},{"location":"api/core/#senspy.pd_to_pc","title":"<code>pd_to_pc(pd: ArrayLike, p_guess: float) -&gt; NDArray[np.floating]</code>","text":"<p>Convert proportion of discriminators to proportion correct.</p> <p>Parameters:</p> Name Type Description Default <code>pd</code> <code>array_like</code> <p>Proportion of discriminators. Values should be in [0, 1].</p> required <code>p_guess</code> <code>float</code> <p>Guessing probability (chance level).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Proportion correct. Values in [p_guess, 1].</p> Notes <p>The relationship is: pc = p_guess + pd * (1 - p_guess)</p> <p>Corresponds to <code>pd2pc()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; pd_to_pc(0.6, p_guess=0.5)\narray([0.8])\n&gt;&gt;&gt; pd_to_pc(0.4, p_guess=1/3)\narray([0.6])\n</code></pre> Source code in <code>senspy/utils/transforms.py</code> <pre><code>def pd_to_pc(\n    pd: ArrayLike,\n    p_guess: float,\n) -&gt; NDArray[np.floating]:\n    \"\"\"Convert proportion of discriminators to proportion correct.\n\n    Parameters\n    ----------\n    pd : array_like\n        Proportion of discriminators. Values should be in [0, 1].\n    p_guess : float\n        Guessing probability (chance level).\n\n    Returns\n    -------\n    ndarray\n        Proportion correct. Values in [p_guess, 1].\n\n    Notes\n    -----\n    The relationship is: pc = p_guess + pd * (1 - p_guess)\n\n    Corresponds to `pd2pc()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; pd_to_pc(0.6, p_guess=0.5)\n    array([0.8])\n    &gt;&gt;&gt; pd_to_pc(0.4, p_guess=1/3)\n    array([0.6])\n    \"\"\"\n    pd = np.atleast_1d(np.asarray(pd, dtype=np.float64))\n\n    if not 0 &lt;= p_guess &lt;= 1:\n        raise ValueError(f\"p_guess must be in [0, 1], got {p_guess}\")\n\n    pc = p_guess + pd * (1 - p_guess)\n\n    return pc\n</code></pre>"},{"location":"api/core/#rescale","title":"rescale","text":""},{"location":"api/core/#senspy.rescale","title":"<code>rescale(pc: ArrayLike | None = None, pd: ArrayLike | None = None, d_prime: ArrayLike | None = None, se: ArrayLike | None = None, method: str | Protocol = 'triangle') -&gt; RescaleResult</code>","text":"<p>Convert between pc, pd, and d-prime scales.</p> <p>Provide exactly one of <code>pc</code>, <code>pd</code>, or <code>d_prime</code>, and optionally a standard error. The function will compute the other two scales.</p> <p>Parameters:</p> Name Type Description Default <code>pc</code> <code>array_like</code> <p>Proportion correct.</p> <code>None</code> <code>pd</code> <code>array_like</code> <p>Proportion of discriminators.</p> <code>None</code> <code>d_prime</code> <code>array_like</code> <p>Sensitivity (d').</p> <code>None</code> <code>se</code> <code>array_like</code> <p>Standard error of the provided parameter. If given, standard errors for the other parameters will be computed via the delta method.</p> <code>None</code> <code>method</code> <code>str or Protocol</code> <p>Discrimination protocol. Determines the psychometric function used for conversion.</p> <code>\"triangle\"</code> <p>Returns:</p> Type Description <code>RescaleResult</code> <p>Result object with pc, pd, d_prime, and optionally standard errors.</p> Notes <p>Corresponds to <code>rescale()</code> in sensR's utils.R.</p> <p>The conversion uses the psychometric function for the given protocol: - pc = psyfun(d_prime, method) - d_prime = psyinv(pc, method) - pd = (pc - p_guess) / (1 - p_guess)</p> <p>Standard errors are propagated using the delta method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = rescale(d_prime=1.5, method=\"triangle\")\n&gt;&gt;&gt; result.pc\n0.628...\n&gt;&gt;&gt; result.pd\n0.442...\n</code></pre> <pre><code>&gt;&gt;&gt; result = rescale(pc=0.8, method=\"twoafc\")\n&gt;&gt;&gt; result.d_prime\n1.683...\n</code></pre> <pre><code>&gt;&gt;&gt; result = rescale(d_prime=1.5, se=0.2, method=\"triangle\")\n&gt;&gt;&gt; result.se_pc\n0.058...\n</code></pre> Source code in <code>senspy/utils/transforms.py</code> <pre><code>def rescale(\n    pc: ArrayLike | None = None,\n    pd: ArrayLike | None = None,\n    d_prime: ArrayLike | None = None,\n    se: ArrayLike | None = None,\n    method: str | Protocol = \"triangle\",\n) -&gt; RescaleResult:\n    \"\"\"Convert between pc, pd, and d-prime scales.\n\n    Provide exactly one of `pc`, `pd`, or `d_prime`, and optionally\n    a standard error. The function will compute the other two scales.\n\n    Parameters\n    ----------\n    pc : array_like, optional\n        Proportion correct.\n    pd : array_like, optional\n        Proportion of discriminators.\n    d_prime : array_like, optional\n        Sensitivity (d').\n    se : array_like, optional\n        Standard error of the provided parameter. If given, standard\n        errors for the other parameters will be computed via the\n        delta method.\n    method : str or Protocol, default \"triangle\"\n        Discrimination protocol. Determines the psychometric function\n        used for conversion.\n\n    Returns\n    -------\n    RescaleResult\n        Result object with pc, pd, d_prime, and optionally standard errors.\n\n    Notes\n    -----\n    Corresponds to `rescale()` in sensR's utils.R.\n\n    The conversion uses the psychometric function for the given protocol:\n    - pc = psyfun(d_prime, method)\n    - d_prime = psyinv(pc, method)\n    - pd = (pc - p_guess) / (1 - p_guess)\n\n    Standard errors are propagated using the delta method.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = rescale(d_prime=1.5, method=\"triangle\")\n    &gt;&gt;&gt; result.pc\n    0.628...\n    &gt;&gt;&gt; result.pd\n    0.442...\n\n    &gt;&gt;&gt; result = rescale(pc=0.8, method=\"twoafc\")\n    &gt;&gt;&gt; result.d_prime\n    1.683...\n\n    &gt;&gt;&gt; result = rescale(d_prime=1.5, se=0.2, method=\"triangle\")\n    &gt;&gt;&gt; result.se_pc\n    0.058...\n    \"\"\"\n    # Import here to avoid circular imports\n    from senspy.links import psy_fun, psy_inv, psy_deriv\n\n    # Validate inputs: exactly one of pc, pd, d_prime must be provided\n    provided = sum(x is not None for x in [pc, pd, d_prime])\n    if provided != 1:\n        raise ValueError(\n            \"Exactly one of pc, pd, or d_prime must be provided. \"\n            f\"Got {provided} arguments.\"\n        )\n\n    protocol = parse_protocol(method)\n    p_guess = protocol.p_guess\n\n    # Convert to arrays\n    if pc is not None:\n        pc = np.atleast_1d(np.asarray(pc, dtype=np.float64))\n        if se is not None:\n            se = np.atleast_1d(np.asarray(se, dtype=np.float64))\n            se_pc = se\n    elif pd is not None:\n        pd = np.atleast_1d(np.asarray(pd, dtype=np.float64))\n        if se is not None:\n            se = np.atleast_1d(np.asarray(se, dtype=np.float64))\n            se_pd = se\n    else:  # d_prime is not None\n        d_prime = np.atleast_1d(np.asarray(d_prime, dtype=np.float64))\n        if se is not None:\n            se = np.atleast_1d(np.asarray(se, dtype=np.float64))\n            se_d_prime = se\n\n    # Initialize standard errors as None\n    se_pc_out = None\n    se_pd_out = None\n    se_d_prime_out = None\n\n    # Compute conversions\n    if pc is not None:\n        # pc -&gt; pd, d_prime\n        # Restrict pc to be &gt;= p_guess\n        pc_adj = np.maximum(pc, p_guess)\n        pd_out = pc_to_pd(pc_adj, p_guess)\n        d_prime_out = psy_inv(pc_adj, method=protocol)\n        pc_out = pc_adj\n\n        if se is not None:\n            se_pc_out = se.copy()\n            # Where pc was below p_guess, SE is undefined\n            se_pc_out[pc &lt; p_guess] = np.nan\n            se_pd_out = se_pc_out / (1 - p_guess)\n            # SE of d_prime via delta method: se_d = se_pc / |d(pc)/d(d')|\n            deriv = psy_deriv(d_prime_out, method=protocol)\n            se_d_prime_out = se_pc_out / deriv\n\n    elif pd is not None:\n        # pd -&gt; pc, d_prime\n        pc_out = pd_to_pc(pd, p_guess)\n        d_prime_out = psy_inv(pc_out, method=protocol)\n        pd_out = pd\n\n        if se is not None:\n            se_pd_out = se.copy()\n            se_pc_out = se_pd_out * (1 - p_guess)\n            deriv = psy_deriv(d_prime_out, method=protocol)\n            se_d_prime_out = se_pc_out / deriv\n\n    else:  # d_prime is not None\n        # d_prime -&gt; pc, pd\n        d_prime = np.maximum(d_prime, 0.0)  # d_prime cannot be negative\n        pc_out = psy_fun(d_prime, method=protocol)\n        pd_out = pc_to_pd(pc_out, p_guess)\n        d_prime_out = d_prime\n\n        if se is not None:\n            se_d_prime_out = se.copy()\n            deriv = psy_deriv(d_prime_out, method=protocol)\n            se_pc_out = se_d_prime_out * deriv\n            se_pd_out = se_pc_out / (1 - p_guess)\n\n    # Return scalar if input was scalar\n    if pc_out.size == 1:\n        pc_out = float(pc_out[0])\n        pd_out = float(pd_out[0])\n        d_prime_out = float(d_prime_out[0])\n        if se_pc_out is not None:\n            se_pc_out = float(se_pc_out[0])\n            se_pd_out = float(se_pd_out[0])\n            se_d_prime_out = float(se_d_prime_out[0])\n\n    return RescaleResult(\n        pc=pc_out,\n        pd=pd_out,\n        d_prime=d_prime_out,\n        method=protocol,\n        se_pc=se_pc_out,\n        se_pd=se_pd_out,\n        se_d_prime=se_d_prime_out,\n    )\n</code></pre>"},{"location":"api/core/#utilities","title":"Utilities","text":""},{"location":"api/core/#find_critical","title":"find_critical","text":""},{"location":"api/core/#senspy.find_critical","title":"<code>find_critical(sample_size: int, alpha: float = 0.05, p0: float = 0.5, pd0: float = 0.0, test: Literal['difference', 'similarity'] = 'difference') -&gt; int</code>","text":"<p>Find the critical value for a one-tailed binomial test.</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>int</code> <p>Number of trials.</p> required <code>alpha</code> <code>float</code> <p>Significance level.</p> <code>0.05</code> <code>p0</code> <code>float</code> <p>Guessing probability (chance level).</p> <code>0.5</code> <code>pd0</code> <code>float</code> <p>Proportion of discriminators under H0.</p> <code>0.0</code> <code>test</code> <code>(difference, similarity)</code> <p>Type of test: - \"difference\": H1 is that the true proportion is greater than H0 - \"similarity\": H1 is that the true proportion is less than H0</p> <code>\"difference\"</code> <p>Returns:</p> Type Description <code>int</code> <p>Critical value. For a \"difference\" test, reject H0 if observed &gt;= critical. For a \"similarity\" test, reject H0 if observed &lt;= critical.</p> Notes <p>Corresponds to <code>findcr()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_critical(sample_size=100, alpha=0.05, p0=0.5)\n59\n&gt;&gt;&gt; find_critical(sample_size=100, alpha=0.05, p0=1/3)\n43\n</code></pre> Source code in <code>senspy/utils/stats.py</code> <pre><code>def find_critical(\n    sample_size: int,\n    alpha: float = 0.05,\n    p0: float = 0.5,\n    pd0: float = 0.0,\n    test: Literal[\"difference\", \"similarity\"] = \"difference\",\n) -&gt; int:\n    \"\"\"Find the critical value for a one-tailed binomial test.\n\n    Parameters\n    ----------\n    sample_size : int\n        Number of trials.\n    alpha : float, default 0.05\n        Significance level.\n    p0 : float, default 0.5\n        Guessing probability (chance level).\n    pd0 : float, default 0.0\n        Proportion of discriminators under H0.\n    test : {\"difference\", \"similarity\"}, default \"difference\"\n        Type of test:\n        - \"difference\": H1 is that the true proportion is greater than H0\n        - \"similarity\": H1 is that the true proportion is less than H0\n\n    Returns\n    -------\n    int\n        Critical value. For a \"difference\" test, reject H0 if\n        observed &gt;= critical. For a \"similarity\" test, reject H0 if\n        observed &lt;= critical.\n\n    Notes\n    -----\n    Corresponds to `findcr()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; find_critical(sample_size=100, alpha=0.05, p0=0.5)\n    59\n    &gt;&gt;&gt; find_critical(sample_size=100, alpha=0.05, p0=1/3)\n    43\n    \"\"\"\n    if sample_size != int(sample_size) or sample_size &lt;= 0:\n        raise ValueError(\"sample_size must be a positive integer\")\n    if not 0 &lt; alpha &lt; 1:\n        raise ValueError(\"alpha must be between 0 and 1 (exclusive)\")\n    if not 0 &lt; p0 &lt; 1:\n        raise ValueError(\"p0 must be between 0 and 1 (exclusive)\")\n    if not 0 &lt;= pd0 &lt;= 1:\n        raise ValueError(\"pd0 must be between 0 and 1 (inclusive)\")\n\n    sample_size = int(sample_size)\n\n    # Compute pc (proportion correct) under H0\n    pc = pd0 + p0 * (1 - pd0)\n\n    if test == \"difference\":\n        # Find smallest x such that P(X &gt;= x) &lt;= alpha\n        # This is equivalent to finding x where P(X &lt;= x-1) &gt;= 1-alpha\n        for x in range(sample_size + 2):\n            p_upper = 1 - stats.binom.cdf(x - 1, sample_size, pc)\n            if p_upper &lt;= alpha:\n                return x\n        return sample_size + 1  # No rejection possible\n\n    elif test == \"similarity\":\n        # Find largest x such that P(X &lt;= x) &lt;= alpha\n        for x in range(sample_size, -2, -1):\n            p_lower = stats.binom.cdf(x, sample_size, pc)\n            if p_lower &lt;= alpha:\n                return x\n        return -1  # No rejection possible\n\n    else:\n        raise ValueError(\n            f\"Unknown test: {test!r}. Must be 'difference' or 'similarity'.\"\n        )\n</code></pre>"},{"location":"api/core/#delimit","title":"delimit","text":""},{"location":"api/core/#senspy.delimit","title":"<code>delimit(x: ArrayLike, lower: float | None = None, upper: float | None = None) -&gt; NDArray[np.floating]</code>","text":"<p>Constrain values to be within specified bounds.</p> <p>Sets values below <code>lower</code> to <code>lower</code> and values above <code>upper</code> to <code>upper</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>Input values to constrain.</p> required <code>lower</code> <code>float</code> <p>Lower bound. Values below this are set to <code>lower</code>.</p> <code>None</code> <code>upper</code> <code>float</code> <p>Upper bound. Values above this are set to <code>upper</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with values constrained to [lower, upper].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both bounds are specified and lower &gt;= upper.</p> Notes <p>Corresponds to <code>delimit()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; delimit([0.1, 0.5, 0.9], lower=0.2, upper=0.8)\narray([0.2, 0.5, 0.8])\n&gt;&gt;&gt; delimit(-0.5, lower=0)\narray([0.])\n</code></pre> Source code in <code>senspy/utils/stats.py</code> <pre><code>def delimit(\n    x: ArrayLike,\n    lower: float | None = None,\n    upper: float | None = None,\n) -&gt; NDArray[np.floating]:\n    \"\"\"Constrain values to be within specified bounds.\n\n    Sets values below `lower` to `lower` and values above `upper` to `upper`.\n\n    Parameters\n    ----------\n    x : array_like\n        Input values to constrain.\n    lower : float, optional\n        Lower bound. Values below this are set to `lower`.\n    upper : float, optional\n        Upper bound. Values above this are set to `upper`.\n\n    Returns\n    -------\n    ndarray\n        Array with values constrained to [lower, upper].\n\n    Raises\n    ------\n    ValueError\n        If both bounds are specified and lower &gt;= upper.\n\n    Notes\n    -----\n    Corresponds to `delimit()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; delimit([0.1, 0.5, 0.9], lower=0.2, upper=0.8)\n    array([0.2, 0.5, 0.8])\n    &gt;&gt;&gt; delimit(-0.5, lower=0)\n    array([0.])\n    \"\"\"\n    x = np.atleast_1d(np.asarray(x, dtype=np.float64))\n\n    if lower is not None and upper is not None:\n        if lower &gt;= upper:\n            raise ValueError(f\"lower ({lower}) must be less than upper ({upper})\")\n\n    result = x.copy()\n\n    if lower is not None:\n        result = np.maximum(result, lower)\n    if upper is not None:\n        result = np.minimum(result, upper)\n\n    return result\n</code></pre>"},{"location":"api/core/#normal_pvalue","title":"normal_pvalue","text":""},{"location":"api/core/#senspy.normal_pvalue","title":"<code>normal_pvalue(statistic: ArrayLike, alternative: Literal['two.sided', 'greater', 'less'] = 'two.sided') -&gt; NDArray[np.floating]</code>","text":"<p>Compute p-value for a standard normal test statistic.</p> <p>Parameters:</p> Name Type Description Default <code>statistic</code> <code>array_like</code> <p>Test statistic(s) following a standard normal distribution under H0.</p> required <code>alternative</code> <code>(sided, greater, less)</code> <p>Type of alternative hypothesis: - \"two.sided\": H1: parameter != 0 - \"greater\": H1: parameter &gt; 0 - \"less\": H1: parameter &lt; 0</p> <code>\"two.sided\"</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>P-value(s) corresponding to the test statistic(s).</p> Notes <p>Corresponds to <code>normalPvalue()</code> in sensR's utils.R.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; normal_pvalue(1.96, alternative=\"two.sided\")\narray([0.05...])\n&gt;&gt;&gt; normal_pvalue(1.645, alternative=\"greater\")\narray([0.05...])\n</code></pre> Source code in <code>senspy/utils/stats.py</code> <pre><code>def normal_pvalue(\n    statistic: ArrayLike,\n    alternative: Literal[\"two.sided\", \"greater\", \"less\"] = \"two.sided\",\n) -&gt; NDArray[np.floating]:\n    \"\"\"Compute p-value for a standard normal test statistic.\n\n    Parameters\n    ----------\n    statistic : array_like\n        Test statistic(s) following a standard normal distribution under H0.\n    alternative : {\"two.sided\", \"greater\", \"less\"}, default \"two.sided\"\n        Type of alternative hypothesis:\n        - \"two.sided\": H1: parameter != 0\n        - \"greater\": H1: parameter &gt; 0\n        - \"less\": H1: parameter &lt; 0\n\n    Returns\n    -------\n    ndarray\n        P-value(s) corresponding to the test statistic(s).\n\n    Notes\n    -----\n    Corresponds to `normalPvalue()` in sensR's utils.R.\n\n    Examples\n    --------\n    &gt;&gt;&gt; normal_pvalue(1.96, alternative=\"two.sided\")\n    array([0.05...])\n    &gt;&gt;&gt; normal_pvalue(1.645, alternative=\"greater\")\n    array([0.05...])\n    \"\"\"\n    statistic = np.atleast_1d(np.asarray(statistic, dtype=np.float64))\n\n    if alternative == \"greater\":\n        p_value = stats.norm.sf(statistic)  # 1 - CDF (upper tail)\n    elif alternative == \"less\":\n        p_value = stats.norm.cdf(statistic)  # lower tail\n    elif alternative == \"two.sided\":\n        p_value = 2 * stats.norm.sf(np.abs(statistic))\n    else:\n        raise ValueError(\n            f\"Unknown alternative: {alternative!r}. \"\n            \"Must be 'two.sided', 'greater', or 'less'.\"\n        )\n\n    return p_value\n</code></pre>"},{"location":"api/discrimination/","title":"Discrimination Analysis","text":""},{"location":"api/discrimination/#discrim","title":"discrim","text":"<p>The main function for analyzing discrimination test data.</p>"},{"location":"api/discrimination/#senspy.discrim","title":"<code>discrim</code>","text":"<p>Discrimination analysis for sensory testing.</p> <p>This module implements the core <code>discrim()</code> function for analyzing discrimination test data. It estimates d-prime (sensitivity) from the number of correct responses in forced-choice trials.</p> <p>Corresponds to sensR's discrim() function.</p>"},{"location":"api/discrimination/#senspy.discrim.discrim","title":"<code>discrim(correct: int, total: int, method: str | Protocol = 'triangle', *, d_prime0: float | None = None, pd0: float | None = None, conf_level: float = 0.95, statistic: str | Statistic = 'exact', test: str = 'difference') -&gt; DiscrimResult</code>","text":"<p>Analyze discrimination test data.</p> <p>Estimates d-prime (sensitivity) from the number of correct responses in a forced-choice discrimination test.</p> <p>Parameters:</p> Name Type Description Default <code>correct</code> <code>int</code> <p>Number of correct responses.</p> required <code>total</code> <code>int</code> <p>Total number of trials.</p> required <code>method</code> <code>str or Protocol</code> <p>Discrimination protocol: \"duotrio\", \"triangle\", \"twoafc\", \"threeafc\", \"tetrad\", \"hexad\", \"twofive\", \"twofivef\".</p> <code>\"triangle\"</code> <code>d_prime0</code> <code>float</code> <p>Null hypothesis value for d-prime (for similarity tests).</p> <code>None</code> <code>pd0</code> <code>float</code> <p>Null hypothesis value for proportion discriminators. Only one of d_prime0 or pd0 can be specified.</p> <code>None</code> <code>conf_level</code> <code>float</code> <p>Confidence level for intervals.</p> <code>0.95</code> <code>statistic</code> <code>str or Statistic</code> <p>Test statistic: \"exact\", \"likelihood\", \"wald\", \"score\".</p> <code>\"exact\"</code> <code>test</code> <code>str</code> <p>Type of test: \"difference\" (H1: d' &gt; d'0) or \"similarity\" (H1: d' &lt; d'0).</p> <code>\"difference\"</code> <p>Returns:</p> Type Description <code>DiscrimResult</code> <p>Result object with estimates, standard errors, confidence intervals, and test results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = discrim(correct=80, total=100, method=\"triangle\")\n&gt;&gt;&gt; result.d_prime\n2.164...\n&gt;&gt;&gt; result.confint()\narray([1.65..., 2.75...])\n</code></pre> <pre><code>&gt;&gt;&gt; # Similarity test\n&gt;&gt;&gt; result = discrim(correct=40, total=100, method=\"triangle\",\n...                  d_prime0=1.0, test=\"similarity\")\n&gt;&gt;&gt; result.p_value\n0.02...\n</code></pre> Notes <p>Corresponds to <code>discrim()</code> in sensR.</p> <p>The function supports four test statistics:</p> <ul> <li>exact: Exact binomial test (default). P-value from binomial   distribution, CI from Clopper-Pearson method.</li> <li>likelihood: Likelihood ratio test. P-value from signed likelihood   root statistic, CI from profile likelihood.</li> <li>wald: Wald test. P-value and CI based on normal approximation.</li> <li>score: Score test. P-value from Pearson chi-square, CI from   Wilson score interval.</li> </ul> Source code in <code>senspy/discrim.py</code> <pre><code>def discrim(\n    correct: int,\n    total: int,\n    method: str | Protocol = \"triangle\",\n    *,\n    d_prime0: float | None = None,\n    pd0: float | None = None,\n    conf_level: float = 0.95,\n    statistic: str | Statistic = \"exact\",\n    test: str = \"difference\",\n) -&gt; DiscrimResult:\n    \"\"\"Analyze discrimination test data.\n\n    Estimates d-prime (sensitivity) from the number of correct responses\n    in a forced-choice discrimination test.\n\n    Parameters\n    ----------\n    correct : int\n        Number of correct responses.\n    total : int\n        Total number of trials.\n    method : str or Protocol, default \"triangle\"\n        Discrimination protocol: \"duotrio\", \"triangle\", \"twoafc\", \"threeafc\",\n        \"tetrad\", \"hexad\", \"twofive\", \"twofivef\".\n    d_prime0 : float, optional\n        Null hypothesis value for d-prime (for similarity tests).\n    pd0 : float, optional\n        Null hypothesis value for proportion discriminators.\n        Only one of d_prime0 or pd0 can be specified.\n    conf_level : float, default 0.95\n        Confidence level for intervals.\n    statistic : str or Statistic, default \"exact\"\n        Test statistic: \"exact\", \"likelihood\", \"wald\", \"score\".\n    test : str, default \"difference\"\n        Type of test: \"difference\" (H1: d' &gt; d'0) or \"similarity\" (H1: d' &lt; d'0).\n\n    Returns\n    -------\n    DiscrimResult\n        Result object with estimates, standard errors, confidence intervals,\n        and test results.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = discrim(correct=80, total=100, method=\"triangle\")\n    &gt;&gt;&gt; result.d_prime\n    2.164...\n    &gt;&gt;&gt; result.confint()\n    array([1.65..., 2.75...])\n\n    &gt;&gt;&gt; # Similarity test\n    &gt;&gt;&gt; result = discrim(correct=40, total=100, method=\"triangle\",\n    ...                  d_prime0=1.0, test=\"similarity\")\n    &gt;&gt;&gt; result.p_value\n    0.02...\n\n    Notes\n    -----\n    Corresponds to `discrim()` in sensR.\n\n    The function supports four test statistics:\n\n    - **exact**: Exact binomial test (default). P-value from binomial\n      distribution, CI from Clopper-Pearson method.\n    - **likelihood**: Likelihood ratio test. P-value from signed likelihood\n      root statistic, CI from profile likelihood.\n    - **wald**: Wald test. P-value and CI based on normal approximation.\n    - **score**: Score test. P-value from Pearson chi-square, CI from\n      Wilson score interval.\n    \"\"\"\n    # Validate inputs\n    if not isinstance(correct, (int, np.integer)):\n        if correct != int(correct):\n            raise ValueError(\"'correct' must be a non-negative integer\")\n        correct = int(correct)\n    if correct &lt; 0:\n        raise ValueError(\"'correct' must be a non-negative integer\")\n\n    if not isinstance(total, (int, np.integer)):\n        if total != int(total):\n            raise ValueError(\"'total' must be a positive integer\")\n        total = int(total)\n    if total &lt;= 0:\n        raise ValueError(\"'total' must be a positive integer\")\n\n    if correct &gt; total:\n        raise ValueError(\"'correct' cannot be larger than 'total'\")\n\n    if not 0 &lt; conf_level &lt; 1:\n        raise ValueError(\"'conf_level' must be between 0 and 1\")\n\n    # Parse method\n    protocol = parse_protocol(method)\n    p_guess = protocol.p_guess\n\n    # Parse statistic\n    if isinstance(statistic, str):\n        stat_lower = statistic.lower()\n        stat_type = {\n            \"exact\": Statistic.EXACT,\n            \"likelihood\": Statistic.LIKELIHOOD,\n            \"wald\": Statistic.WALD,\n            \"score\": Statistic.SCORE,\n        }.get(stat_lower)\n        if stat_type is None:\n            raise ValueError(\n                f\"Unknown statistic: {statistic!r}. \"\n                \"Valid options: 'exact', 'likelihood', 'wald', 'score'\"\n            )\n    else:\n        stat_type = statistic\n\n    # Parse test type\n    test_lower = test.lower()\n    if test_lower not in (\"difference\", \"similarity\"):\n        raise ValueError(\n            f\"Unknown test: {test!r}. Valid options: 'difference', 'similarity'\"\n        )\n\n    # Handle null hypothesis specification\n    if d_prime0 is not None and pd0 is not None:\n        raise ValueError(\"Only specify one of 'd_prime0' and 'pd0'\")\n\n    if test_lower == \"similarity\" and d_prime0 is None and pd0 is None:\n        raise ValueError(\n            \"Either 'd_prime0' or 'pd0' must be specified for a similarity test\"\n        )\n\n    # Set null hypothesis values\n    if pd0 is not None:\n        if not 0 &lt;= pd0 &lt;= 1:\n            raise ValueError(\"'pd0' must be between 0 and 1\")\n        pc0 = pd_to_pc(pd0, p_guess)[0]\n    elif d_prime0 is not None:\n        if d_prime0 &lt; 0:\n            raise ValueError(\"'d_prime0' must be non-negative\")\n        pc0 = psy_fun(d_prime0, method=protocol)[0]\n    else:\n        pd0 = 0.0\n        pc0 = p_guess\n\n    # Compute estimates\n    pc_hat = correct / total\n    se_pc = np.sqrt(pc_hat * (1 - pc_hat) / total) if 0 &lt; pc_hat &lt; 1 else 0.0\n\n    # Convert to pd and d-prime\n    if pc_hat &lt;= p_guess:\n        pd_hat = 0.0\n        d_prime_hat = 0.0\n        se_pd = np.nan\n        se_d_prime = np.nan\n    else:\n        pd_hat = pc_to_pd(pc_hat, p_guess)[0]\n        d_prime_hat = psy_inv(pc_hat, method=protocol)[0]\n\n        # Standard errors via delta method\n        if pc_hat &lt; 1:\n            # SE for pd: pd = (pc - p_guess) / (1 - p_guess)\n            se_pd = se_pc / (1 - p_guess)\n\n            # SE for d-prime: use derivative of inverse link\n            deriv = psy_deriv(d_prime_hat, method=protocol)[0]\n            if deriv &gt; 0:\n                se_d_prime = se_pc / deriv\n            else:\n                se_d_prime = np.nan\n        else:\n            se_pd = np.nan\n            se_d_prime = np.nan\n\n    # Compute p-value and confidence interval based on statistic\n    if stat_type == Statistic.EXACT:\n        # Exact binomial test\n        if test_lower == \"difference\":\n            p_value = 1 - stats.binom.cdf(correct - 1, total, pc0)\n        else:\n            p_value = stats.binom.cdf(correct, total, pc0)\n\n        # Clopper-Pearson CI\n        ci_result = stats.binomtest(correct, total)\n        ci_pc = ci_result.proportion_ci(confidence_level=conf_level)\n        ci_lower_pc, ci_upper_pc = ci_pc.low, ci_pc.high\n        stat_value = float(correct)\n\n    elif stat_type == Statistic.LIKELIHOOD:\n        # Likelihood ratio test\n        p_seq, l_root, log_lik_max, _ = _profile_binom(correct, total)\n\n        log_lik_null = stats.binom.logpmf(correct, total, pc0)\n        stat_value = np.sign(pc_hat - pc0) * np.sqrt(\n            2 * max(log_lik_max - log_lik_null, 0)\n        )\n\n        if test_lower == \"difference\":\n            p_value = stats.norm.sf(stat_value)\n        else:\n            p_value = stats.norm.cdf(stat_value)\n\n        # Profile likelihood CI\n        ci_lower_pc, ci_upper_pc = _confint_profile(p_seq, l_root, conf_level)\n\n    elif stat_type == Statistic.WALD:\n        # Wald test\n        if se_pc &gt; 0:\n            stat_value = (pc_hat - pc0) / np.sqrt(pc_hat * (1 - pc_hat) / total)\n        else:\n            # SE is 0 when pc_hat is 0 or 1 (boundary cases)\n            # If pc_hat differs from pc0, we have extreme evidence\n            if pc_hat &gt; pc0:\n                stat_value = np.inf\n            elif pc_hat &lt; pc0:\n                stat_value = -np.inf\n            else:\n                stat_value = 0.0\n\n        if test_lower == \"difference\":\n            p_value = stats.norm.sf(stat_value)\n        else:\n            p_value = stats.norm.cdf(stat_value)\n\n        # Wald CI\n        alpha = 1 - conf_level\n        z = stats.norm.ppf(1 - alpha / 2)\n        ci_lower_pc = pc_hat - z * se_pc\n        ci_upper_pc = pc_hat + z * se_pc\n\n    elif stat_type == Statistic.SCORE:\n        # Score test (Wilson)\n        pc0_bounded = delimit(pc0, lower=1e-8, upper=1 - 1e-8)[0]\n        stat_value = (pc_hat - pc0_bounded) / np.sqrt(\n            pc0_bounded * (1 - pc0_bounded) / total\n        )\n\n        if test_lower == \"difference\":\n            p_value = stats.norm.sf(stat_value)\n        else:\n            p_value = stats.norm.cdf(stat_value)\n\n        # Wilson score CI\n        alpha = 1 - conf_level\n        z = stats.norm.ppf(1 - alpha / 2)\n        denom = 1 + z**2 / total\n        center = (pc_hat + z**2 / (2 * total)) / denom\n        margin = z * np.sqrt(pc_hat * (1 - pc_hat) / total + z**2 / (4 * total**2)) / denom\n        ci_lower_pc = center - margin\n        ci_upper_pc = center + margin\n\n    else:\n        # Should not reach here if input validation is correct\n        raise ValueError(f\"Unhandled statistic type: {stat_type}\")\n\n    # Bound CI to valid range\n    ci_lower_pc = max(0, ci_lower_pc)\n    ci_upper_pc = min(1, ci_upper_pc)\n\n    # Convert CI to d-prime scale\n    if ci_lower_pc &lt;= p_guess:\n        ci_lower_d = 0.0\n    else:\n        ci_lower_d = psy_inv(ci_lower_pc, method=protocol)[0]\n\n    if ci_upper_pc &lt;= p_guess:\n        ci_upper_d = 0.0\n    elif ci_upper_pc &gt;= 1:\n        ci_upper_d = np.inf\n    else:\n        ci_upper_d = psy_inv(ci_upper_pc, method=protocol)[0]\n\n    # Build result\n    result = DiscrimResult(\n        d_prime=d_prime_hat,\n        pc=pc_hat,\n        pd=pd_hat,\n        se_d_prime=se_d_prime,  # NaN when undefined (at boundaries)\n        se_pc=se_pc,\n        se_pd=se_pd,  # NaN when undefined (at boundaries)\n        p_value=float(p_value),\n        statistic=float(stat_value),\n        stat_type=stat_type,\n        method=protocol,\n        correct=correct,\n        total=total,\n        conf_level=conf_level,\n        _ci_d_prime=np.array([ci_lower_d, ci_upper_d]),\n        _ci_pc=np.array([ci_lower_pc, ci_upper_pc]),\n        _ci_pd=np.array([\n            pc_to_pd(ci_lower_pc, p_guess)[0] if ci_lower_pc &gt; p_guess else 0.0,\n            pc_to_pd(ci_upper_pc, p_guess)[0] if ci_upper_pc &gt; p_guess else 0.0,\n        ]),\n    )\n\n    return result\n</code></pre>"},{"location":"api/discrimination/#anota","title":"anota","text":"<p>A-Not-A protocol analysis using signal detection theory.</p>"},{"location":"api/discrimination/#senspy.anota","title":"<code>anota</code>","text":"<p>A-Not-A (AnotA) protocol for sensory discrimination.</p> <p>This module implements the A-Not-A discrimination protocol, which is a signal detection method where assessors are presented with samples and must identify whether each is \"A\" or \"Not-A\".</p> <p>The protocol involves: - A samples: n1 presentations, x1 correctly identified as \"A\" - Not-A samples: n2 presentations, x2 correctly identified as \"Not-A\"</p> <p>The d-prime is estimated using probit regression.</p> References <p>Ennis, J.M. (1993). The power of sensory discrimination methods. Journal of Sensory Studies, 8(4), 353-370.</p>"},{"location":"api/discrimination/#senspy.anota.ANotAResult","title":"<code>ANotAResult(d_prime: float, se_d_prime: float, p_value: float, hit_rate: float, false_alarm_rate: float, data: dict)</code>  <code>dataclass</code>","text":"<p>Result from A-Not-A analysis.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated d-prime (discriminability).</p> <code>se_d_prime</code> <code>float</code> <p>Standard error of d-prime.</p> <code>p_value</code> <code>float</code> <p>P-value from Fisher's exact test.</p> <code>hit_rate</code> <code>float</code> <p>Proportion of A samples correctly identified (x1/n1).</p> <code>false_alarm_rate</code> <code>float</code> <p>Proportion of Not-A samples incorrectly identified as A (1 - x2/n2).</p> <code>data</code> <code>dict</code> <p>Input data: x1, n1, x2, n2.</p>"},{"location":"api/discrimination/#senspy.anota.anota","title":"<code>anota(x1: int, n1: int, x2: int, n2: int) -&gt; ANotAResult</code>","text":"<p>Fit A-Not-A discrimination model.</p> <p>The A-Not-A protocol presents assessors with samples that are either \"A\" or \"Not-A\", and they must identify which type each sample is.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>int</code> <p>Number of A samples correctly identified as \"A\" (hits).</p> required <code>n1</code> <code>int</code> <p>Total number of A samples presented.</p> required <code>x2</code> <code>int</code> <p>Number of Not-A samples correctly identified as \"Not-A\".</p> required <code>n2</code> <code>int</code> <p>Total number of Not-A samples presented.</p> required <p>Returns:</p> Type Description <code>ANotAResult</code> <p>Analysis results including d-prime estimate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = anota(x1=80, n1=100, x2=70, n2=100)\n&gt;&gt;&gt; print(f\"d-prime: {result.d_prime:.3f}\")\n&gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n</code></pre> Notes <p>d-prime is computed using probit regression: - Hit rate (H) = x1/n1 - False alarm rate (F) = (n2-x2)/n2 = 1 - x2/n2 - d' = z(H) - z(F)</p> <p>The p-value is from Fisher's exact test for testing whether there is a difference in discrimination ability.</p> Source code in <code>senspy/anota.py</code> <pre><code>def anota(\n    x1: int,\n    n1: int,\n    x2: int,\n    n2: int,\n) -&gt; ANotAResult:\n    \"\"\"Fit A-Not-A discrimination model.\n\n    The A-Not-A protocol presents assessors with samples that are either\n    \"A\" or \"Not-A\", and they must identify which type each sample is.\n\n    Parameters\n    ----------\n    x1 : int\n        Number of A samples correctly identified as \"A\" (hits).\n    n1 : int\n        Total number of A samples presented.\n    x2 : int\n        Number of Not-A samples correctly identified as \"Not-A\".\n    n2 : int\n        Total number of Not-A samples presented.\n\n    Returns\n    -------\n    ANotAResult\n        Analysis results including d-prime estimate.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = anota(x1=80, n1=100, x2=70, n2=100)\n    &gt;&gt;&gt; print(f\"d-prime: {result.d_prime:.3f}\")\n    &gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n\n    Notes\n    -----\n    d-prime is computed using probit regression:\n    - Hit rate (H) = x1/n1\n    - False alarm rate (F) = (n2-x2)/n2 = 1 - x2/n2\n    - d' = z(H) - z(F)\n\n    The p-value is from Fisher's exact test for testing whether there\n    is a difference in discrimination ability.\n    \"\"\"\n    # Validate inputs\n    for name, val in [(\"x1\", x1), (\"n1\", n1), (\"x2\", x2), (\"n2\", n2)]:\n        if not isinstance(val, (int, np.integer)):\n            if isinstance(val, float) and val == int(val):\n                val = int(val)\n            else:\n                raise ValueError(f\"{name} must be a positive integer\")\n        if val &lt;= 0:\n            raise ValueError(f\"{name} must be a positive integer\")\n\n    x1, n1, x2, n2 = int(x1), int(n1), int(x2), int(n2)\n\n    if x1 &gt; n1:\n        raise ValueError(\"x1 cannot exceed n1\")\n    if x2 &gt; n2:\n        raise ValueError(\"x2 cannot exceed n2\")\n\n    # Compute hit rate and false alarm rate\n    hit_rate = x1 / n1\n    false_alarm_rate = (n2 - x2) / n2  # FA = proportion of Not-A called \"A\"\n\n    # Compute d-prime using probit (z-scores)\n    # d' = z(H) - z(FA)\n    # Need to handle edge cases where rates are 0 or 1\n    def safe_probit(p, n):\n        \"\"\"Compute probit with adjustment for extreme values.\"\"\"\n        # Apply 1/(2n) correction (Macmillan &amp; Kaplan, 1985)\n        # This avoids infinite z-scores at p=0 or p=1\n        if p &lt;= 0:\n            p = 0.5 / n\n        elif p &gt;= 1:\n            p = (n - 0.5) / n\n        return stats.norm.ppf(p)\n\n    z_hit = safe_probit(hit_rate, n1)\n    z_fa = safe_probit(false_alarm_rate, n2)\n\n    d_prime = z_hit - z_fa\n\n    # Compute standard error using delta method\n    # Var(d') \u2248 Var(z_H) + Var(z_FA)\n    # Var(z(p)) \u2248 p(1-p) / [n * \u03c6(z(p))^2] where \u03c6 is std normal pdf\n    def var_probit(p, n):\n        \"\"\"Variance of probit-transformed proportion.\"\"\"\n        # Use same 1/(2n) correction as safe_probit for consistency\n        if p &lt;= 0:\n            p = 0.5 / n\n        elif p &gt;= 1:\n            p = (n - 0.5) / n\n        z = stats.norm.ppf(p)\n        phi = stats.norm.pdf(z)\n        if phi &lt; 1e-10:\n            return np.inf\n        return p * (1 - p) / (n * phi**2)\n\n    var_z_hit = var_probit(hit_rate, n1)\n    var_z_fa = var_probit(false_alarm_rate, n2)\n\n    se_d_prime = np.sqrt(var_z_hit + var_z_fa)\n\n    # Fisher's exact test\n    # Contingency table:\n    #           A    Not-A\n    # \"A\"       x1   n2-x2\n    # \"Not-A\"   n1-x1  x2\n    table = np.array([[x1, n2 - x2], [n1 - x1, x2]])\n    _, p_value = fisher_exact(table, alternative=\"greater\")\n\n    return ANotAResult(\n        d_prime=d_prime,\n        se_d_prime=se_d_prime,\n        p_value=p_value,\n        hit_rate=hit_rate,\n        false_alarm_rate=false_alarm_rate,\n        data={\"x1\": x1, \"n1\": n1, \"x2\": x2, \"n2\": n2},\n    )\n</code></pre>"},{"location":"api/discrimination/#result-classes","title":"Result Classes","text":""},{"location":"api/discrimination/#discrimresult","title":"DiscrimResult","text":""},{"location":"api/discrimination/#senspy.DiscrimResult","title":"<code>DiscrimResult(d_prime: float, pc: float, pd: float, se_d_prime: float, se_pc: float, se_pd: float, p_value: float, statistic: float, stat_type: Statistic, method: Protocol, correct: int, total: int, conf_level: float = 0.95, loglik: float | None = None, _ci_d_prime: NDArray[np.floating] | None = None, _ci_pc: NDArray[np.floating] | None = None, _ci_pd: NDArray[np.floating] | None = None)</code>  <code>dataclass</code>","text":"<p>Result from a discrimination analysis.</p> <p>This class holds the results from the <code>discrim()</code> function, including point estimates, standard errors, confidence intervals, and test results.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated d-prime (sensitivity measure).</p> <code>pc</code> <code>float</code> <p>Proportion correct (estimated).</p> <code>pd</code> <code>float</code> <p>Proportion discriminators.</p> <code>se_d_prime</code> <code>float</code> <p>Standard error of d-prime estimate.</p> <code>se_pc</code> <code>float</code> <p>Standard error of proportion correct.</p> <code>se_pd</code> <code>float</code> <p>Standard error of proportion discriminators.</p> <code>p_value</code> <code>float</code> <p>P-value from the significance test.</p> <code>statistic</code> <code>float</code> <p>Value of the test statistic.</p> <code>stat_type</code> <code>Statistic</code> <p>Type of test statistic used.</p> <code>method</code> <code>Protocol</code> <p>Discrimination protocol used.</p> <code>correct</code> <code>int</code> <p>Number of correct responses.</p> <code>total</code> <code>int</code> <p>Total number of trials.</p> <code>conf_level</code> <code>float</code> <p>Confidence level for intervals (default 0.95).</p> <code>loglik</code> <code>float | None</code> <p>Log-likelihood at the MLE (if computed).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy import discrim\n&gt;&gt;&gt; result = discrim(correct=80, total=100, method=\"triangle\")\n&gt;&gt;&gt; result.d_prime\n2.164...\n&gt;&gt;&gt; result.confint()\narray([1.65..., 2.75...])\n</code></pre>"},{"location":"api/discrimination/#senspy.DiscrimResult.coefficients","title":"<code>coefficients: dict[str, dict[str, float]]</code>  <code>property</code>","text":"<p>Return coefficients as a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with 'pc', 'pd', 'd_prime' keys, each containing 'estimate' and 'se' values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result.coefficients['d_prime']['estimate']\n2.164...\n</code></pre>"},{"location":"api/discrimination/#senspy.DiscrimResult.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Return a detailed representation.</p> Source code in <code>senspy/core/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a detailed representation.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"api/discrimination/#senspy.DiscrimResult.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Return a brief string representation.</p> Source code in <code>senspy/core/base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a brief string representation.\"\"\"\n    return (\n        f\"DiscrimResult(method={self.method.value!r}, \"\n        f\"d_prime={self.d_prime:.4f}, p_value={self.p_value:.4g})\"\n    )\n</code></pre>"},{"location":"api/discrimination/#senspy.DiscrimResult.confint","title":"<code>confint(level: float | None = None, parameter: str = 'd_prime') -&gt; NDArray[np.floating]</code>","text":"<p>Compute confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>float</code> <p>Confidence level (0 &lt; level &lt; 1). If None, uses the level from the original analysis and returns the pre-computed CI.</p> <code>None</code> <code>parameter</code> <code>str</code> <p>Which parameter to compute CI for: \"d_prime\", \"pc\", or \"pd\".</p> <code>\"d_prime\"</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (2,) with [lower, upper] bounds.</p> Notes <p>When <code>level</code> matches the original analysis level (default), the pre-computed CI is returned (using the method specified during analysis: Clopper-Pearson for exact, profile likelihood for likelihood, Wilson for score, or Wald for wald statistic).</p> <p>When <code>level</code> differs from the original, a Wald approximation is computed instead. This is faster but may be less accurate, especially for small samples or extreme proportions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result.confint()\narray([1.65..., 2.75...])\n&gt;&gt;&gt; result.confint(level=0.99)\narray([1.45..., 2.95...])\n</code></pre> Source code in <code>senspy/core/base.py</code> <pre><code>def confint(\n    self, level: float | None = None, parameter: str = \"d_prime\"\n) -&gt; NDArray[np.floating]:\n    \"\"\"Compute confidence intervals.\n\n    Parameters\n    ----------\n    level : float, optional\n        Confidence level (0 &lt; level &lt; 1). If None, uses the level\n        from the original analysis and returns the pre-computed CI.\n    parameter : str, default \"d_prime\"\n        Which parameter to compute CI for: \"d_prime\", \"pc\", or \"pd\".\n\n    Returns\n    -------\n    ndarray\n        Array of shape (2,) with [lower, upper] bounds.\n\n    Notes\n    -----\n    When ``level`` matches the original analysis level (default), the\n    pre-computed CI is returned (using the method specified during\n    analysis: Clopper-Pearson for exact, profile likelihood for\n    likelihood, Wilson for score, or Wald for wald statistic).\n\n    When ``level`` differs from the original, a Wald approximation\n    is computed instead. This is faster but may be less accurate,\n    especially for small samples or extreme proportions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result.confint()\n    array([1.65..., 2.75...])\n    &gt;&gt;&gt; result.confint(level=0.99)\n    array([1.45..., 2.95...])\n    \"\"\"\n    from scipy import stats\n\n    if level is None:\n        level = self.conf_level\n\n    # Use pre-computed CI if level matches and CI is available\n    use_precomputed = level == self.conf_level\n\n    if parameter == \"d_prime\":\n        if use_precomputed and self._ci_d_prime is not None:\n            return self._ci_d_prime.copy()\n        estimate, se = self.d_prime, self.se_d_prime\n        lower_bound = 0.0  # d-prime cannot be negative\n    elif parameter == \"pc\":\n        if use_precomputed and self._ci_pc is not None:\n            return self._ci_pc.copy()\n        estimate, se = self.pc, self.se_pc\n        lower_bound = self.method.p_guess\n    elif parameter == \"pd\":\n        if use_precomputed and self._ci_pd is not None:\n            return self._ci_pd.copy()\n        estimate, se = self.pd, self.se_pd\n        lower_bound = 0.0\n    else:\n        raise ValueError(f\"Unknown parameter: {parameter}\")\n\n    # Fall back to Wald CI if no pre-computed CI or different level\n    alpha = 1 - level\n    z = stats.norm.ppf(1 - alpha / 2)\n    lower = max(lower_bound, estimate - z * se)\n    upper = estimate + z * se\n\n    return np.array([lower, upper])\n</code></pre>"},{"location":"api/discrimination/#senspy.DiscrimResult.summary","title":"<code>summary() -&gt; str</code>","text":"<p>Return a formatted summary of the results.</p> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string with the analysis results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(result.summary())\nDiscrimination Analysis (triangle)\n...\n</code></pre> Source code in <code>senspy/core/base.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Return a formatted summary of the results.\n\n    Returns\n    -------\n    str\n        A formatted string with the analysis results.\n\n    Examples\n    --------\n    &gt;&gt;&gt; print(result.summary())\n    Discrimination Analysis (triangle)\n    ...\n    \"\"\"\n    ci = self.confint()\n    lines = [\n        f\"Discrimination Analysis ({self.method.value})\",\n        \"=\" * 45,\n        f\"Data: {self.correct} correct out of {self.total} trials\",\n        \"\",\n        \"Estimates:\",\n        f\"  {'Parameter':&lt;12} {'Estimate':&gt;10} {'Std.Err':&gt;10}\",\n        f\"  {'-' * 12} {'-' * 10} {'-' * 10}\",\n        f\"  {'pc':&lt;12} {self.pc:&gt;10.4f} {self.se_pc:&gt;10.4f}\",\n        f\"  {'pd':&lt;12} {self.pd:&gt;10.4f} {self.se_pd:&gt;10.4f}\",\n        f\"  {'d.prime':&lt;12} {self.d_prime:&gt;10.4f} {self.se_d_prime:&gt;10.4f}\",\n        \"\",\n        f\"{self.conf_level:.0%} Confidence Interval for d-prime:\",\n        f\"  [{ci[0]:.4f}, {ci[1]:.4f}]\",\n        \"\",\n        f\"Test of H0: d-prime = 0 ({self.stat_type.value} test)\",\n        f\"  Statistic = {self.statistic:.4f}, p-value = {self.p_value:.4g}\",\n    ]\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/discrimination/#anotaresult","title":"ANotAResult","text":""},{"location":"api/discrimination/#senspy.ANotAResult","title":"<code>ANotAResult(d_prime: float, se_d_prime: float, p_value: float, hit_rate: float, false_alarm_rate: float, data: dict)</code>  <code>dataclass</code>","text":"<p>Result from A-Not-A analysis.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated d-prime (discriminability).</p> <code>se_d_prime</code> <code>float</code> <p>Standard error of d-prime.</p> <code>p_value</code> <code>float</code> <p>P-value from Fisher's exact test.</p> <code>hit_rate</code> <code>float</code> <p>Proportion of A samples correctly identified (x1/n1).</p> <code>false_alarm_rate</code> <code>float</code> <p>Proportion of Not-A samples incorrectly identified as A (1 - x2/n2).</p> <code>data</code> <code>dict</code> <p>Input data: x1, n1, x2, n2.</p>"},{"location":"api/models/","title":"Statistical Models","text":""},{"location":"api/models/#beta-binomial-model","title":"Beta-Binomial Model","text":""},{"location":"api/models/#betabin","title":"betabin","text":""},{"location":"api/models/#senspy.betabin","title":"<code>betabin</code>","text":"<p>Beta-binomial models for over-dispersed discrimination data.</p> <p>This module implements the beta-binomial model and chance-corrected beta-binomial model for replicated discrimination tests with overdispersion.</p>"},{"location":"api/models/#senspy.betabin.BetaBinomialResult","title":"<code>BetaBinomialResult(coefficients: dict[str, float], vcov: np.ndarray | None, log_likelihood: float, log_lik_null: float, log_lik_mu: float, data: np.ndarray, method: str, corrected: bool, convergence: int, message: str, n_iter: int, _p_guess: float)</code>  <code>dataclass</code>","text":"<p>Result from beta-binomial model fitting.</p> <p>Attributes:</p> Name Type Description <code>coefficients</code> <code>dict</code> <p>Dictionary with 'mu' and 'gamma' parameter estimates.</p> <code>vcov</code> <code>ndarray | None</code> <p>2x2 variance-covariance matrix of (mu, gamma), or None if not computed.</p> <code>log_likelihood</code> <code>float</code> <p>Log-likelihood at the MLE.</p> <code>log_lik_null</code> <code>float</code> <p>Log-likelihood under null model (p = p_guess).</p> <code>log_lik_mu</code> <code>float</code> <p>Log-likelihood under binomial model (p = observed proportion).</p> <code>data</code> <code>ndarray</code> <p>The input data matrix (successes, trials).</p> <code>method</code> <code>str</code> <p>The discrimination protocol used.</p> <code>corrected</code> <code>bool</code> <p>Whether the chance-corrected model was fit.</p> <code>convergence</code> <code>int</code> <p>Optimizer convergence status (0 = converged).</p> <code>message</code> <code>str</code> <p>Optimizer message.</p> <code>n_iter</code> <code>int</code> <p>Number of optimizer iterations.</p>"},{"location":"api/models/#senspy.betabin.BetaBinomialResult.gamma","title":"<code>gamma: float</code>  <code>property</code>","text":"<p>Overdispersion parameter estimate.</p>"},{"location":"api/models/#senspy.betabin.BetaBinomialResult.mu","title":"<code>mu: float</code>  <code>property</code>","text":"<p>Mean parameter estimate.</p>"},{"location":"api/models/#senspy.betabin.BetaBinomialResult.lr_association","title":"<code>lr_association() -&gt; tuple[float, float]</code>","text":"<p>Likelihood ratio test for association (difference from null).</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>(test statistic G^2, p-value) with 2 df.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def lr_association(self) -&gt; tuple[float, float]:\n    \"\"\"Likelihood ratio test for association (difference from null).\n\n    Returns\n    -------\n    tuple[float, float]\n        (test statistic G^2, p-value) with 2 df.\n    \"\"\"\n    g2 = 2 * (self.log_likelihood - self.log_lik_null)\n    p_value = stats.chi2.sf(g2, df=2)\n    return g2, p_value\n</code></pre>"},{"location":"api/models/#senspy.betabin.BetaBinomialResult.lr_overdispersion","title":"<code>lr_overdispersion() -&gt; tuple[float, float]</code>","text":"<p>Likelihood ratio test for overdispersion.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>(test statistic G^2, p-value) with 1 df. Note: The true df may be between 0.5 and 1 since gamma is tested at the boundary.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def lr_overdispersion(self) -&gt; tuple[float, float]:\n    \"\"\"Likelihood ratio test for overdispersion.\n\n    Returns\n    -------\n    tuple[float, float]\n        (test statistic G^2, p-value) with 1 df.\n        Note: The true df may be between 0.5 and 1 since gamma is tested\n        at the boundary.\n    \"\"\"\n    g2 = 2 * (self.log_likelihood - self.log_lik_mu)\n    p_value = stats.chi2.sf(g2, df=1)\n    return g2, p_value\n</code></pre>"},{"location":"api/models/#senspy.betabin.BetaBinomialResult.se","title":"<code>se() -&gt; dict[str, float] | None</code>","text":"<p>Standard errors of mu and gamma.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def se(self) -&gt; dict[str, float] | None:\n    \"\"\"Standard errors of mu and gamma.\"\"\"\n    if self.vcov is None:\n        return None\n    se_vals = np.sqrt(np.diag(self.vcov))\n    return {\"mu\": se_vals[0], \"gamma\": se_vals[1]}\n</code></pre>"},{"location":"api/models/#senspy.betabin.BetaBinomialResult.summary","title":"<code>summary(level: float = 0.95) -&gt; BetaBinomialSummary</code>","text":"<p>Generate summary with confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>float</code> <p>Confidence level (default 0.95).</p> <code>0.95</code> <p>Returns:</p> Type Description <code>BetaBinomialSummary</code> <p>Summary object with estimates, SEs, and CIs on multiple scales.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def summary(self, level: float = 0.95) -&gt; \"BetaBinomialSummary\":\n    \"\"\"Generate summary with confidence intervals.\n\n    Parameters\n    ----------\n    level : float\n        Confidence level (default 0.95).\n\n    Returns\n    -------\n    BetaBinomialSummary\n        Summary object with estimates, SEs, and CIs on multiple scales.\n    \"\"\"\n    return BetaBinomialSummary.from_result(self, level=level)\n</code></pre>"},{"location":"api/models/#senspy.betabin.BetaBinomialSummary","title":"<code>BetaBinomialSummary(estimates: dict[str, float], std_errors: dict[str, float | None], ci_lower: dict[str, float | None], ci_upper: dict[str, float | None], level: float, log_likelihood: float, lr_overdispersion: tuple[float, float], lr_association: tuple[float, float], method: str, corrected: bool)</code>  <code>dataclass</code>","text":"<p>Summary of beta-binomial model fit.</p>"},{"location":"api/models/#senspy.betabin.BetaBinomialSummary.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Format summary as string.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format summary as string.\"\"\"\n    lines = []\n    model_type = \"Chance-corrected beta-binomial\" if self.corrected else \"Beta-binomial\"\n    lines.append(f\"{model_type} model for the {self.method} protocol\")\n    lines.append(f\"with {self.level * 100:.1f}% confidence intervals\")\n    lines.append(\"\")\n\n    # Header\n    lines.append(f\"{'Parameter':&lt;10} {'Estimate':&gt;10} {'Std.Err':&gt;10} {'Lower':&gt;10} {'Upper':&gt;10}\")\n    lines.append(\"-\" * 55)\n\n    # Parameters\n    for param in [\"mu\", \"gamma\", \"pc\", \"pd\", \"d_prime\"]:\n        est = self.estimates[param]\n        se = self.std_errors[param]\n        lo = self.ci_lower[param]\n        hi = self.ci_upper[param]\n\n        est_str = f\"{est:10.4f}\" if not np.isnan(est) else f\"{'NA':&gt;10}\"\n        se_str = f\"{se:10.4f}\" if se is not None else f\"{'NA':&gt;10}\"\n        lo_str = f\"{lo:10.4f}\" if lo is not None else f\"{'NA':&gt;10}\"\n        hi_str = f\"{hi:10.4f}\" if hi is not None else f\"{'NA':&gt;10}\"\n\n        lines.append(f\"{param:&lt;10} {est_str} {se_str} {lo_str} {hi_str}\")\n\n    lines.append(\"\")\n    lines.append(f\"log-likelihood: {self.log_likelihood:.4f}\")\n\n    g2_od, p_od = self.lr_overdispersion\n    lines.append(f\"LR-test of over-dispersion: G^2={g2_od:.4f}, df=1, p-value={p_od:.4g}\")\n\n    g2_assoc, p_assoc = self.lr_association\n    lines.append(f\"LR-test of association: G^2={g2_assoc:.4f}, df=2, p-value={p_assoc:.4g}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/models/#senspy.betabin.BetaBinomialSummary.from_result","title":"<code>from_result(result: BetaBinomialResult, level: float = 0.95) -&gt; BetaBinomialSummary</code>  <code>classmethod</code>","text":"<p>Create summary from BetaBinomialResult.</p> Source code in <code>senspy/betabin.py</code> <pre><code>@classmethod\ndef from_result(\n    cls, result: BetaBinomialResult, level: float = 0.95\n) -&gt; \"BetaBinomialSummary\":\n    \"\"\"Create summary from BetaBinomialResult.\"\"\"\n    mu = result.mu\n    gamma = result.gamma\n    se_dict = result.se()\n\n    # Initialize estimates\n    estimates = {\"mu\": mu, \"gamma\": gamma, \"pc\": np.nan, \"pd\": np.nan, \"d_prime\": np.nan}\n    std_errors: dict[str, float | None] = {k: None for k in estimates}\n    ci_lower: dict[str, float | None] = {k: None for k in estimates}\n    ci_upper: dict[str, float | None] = {k: None for k in estimates}\n\n    # Fill in mu and gamma SE/CI\n    if se_dict is not None:\n        std_errors[\"mu\"] = se_dict[\"mu\"]\n        std_errors[\"gamma\"] = se_dict[\"gamma\"]\n\n        alpha = (1 - level) / 2\n        z = stats.norm.ppf(1 - alpha)\n\n        # Wald CI for mu\n        mu_lower = max(0.0, mu - z * se_dict[\"mu\"])\n        mu_upper = min(1.0, mu + z * se_dict[\"mu\"])\n        ci_lower[\"mu\"] = mu_lower\n        ci_upper[\"mu\"] = mu_upper\n\n        # Wald CI for gamma\n        ci_lower[\"gamma\"] = max(0.0, gamma - z * se_dict[\"gamma\"])\n        ci_upper[\"gamma\"] = min(1.0, gamma + z * se_dict[\"gamma\"])\n\n    # Transform mu to pc, pd, d_prime scales\n    if not np.isnan(mu):\n        if result.corrected:\n            # mu is on pd scale\n            obj = rescale(pd=mu, method=result.method)\n        else:\n            # mu is on pc scale\n            obj = rescale(pc=mu, method=result.method)\n\n        estimates[\"pc\"] = obj.pc\n        estimates[\"pd\"] = obj.pd\n        estimates[\"d_prime\"] = obj.d_prime\n\n        # Transform SE using delta method (via rescale)\n        if se_dict is not None and se_dict[\"mu\"] is not None:\n            if result.corrected:\n                obj_se = rescale(pd=mu, se=se_dict[\"mu\"], method=result.method)\n            else:\n                obj_se = rescale(pc=mu, se=se_dict[\"mu\"], method=result.method)\n\n            std_errors[\"pc\"] = obj_se.se_pc\n            std_errors[\"pd\"] = obj_se.se_pd\n            std_errors[\"d_prime\"] = obj_se.se_d_prime\n\n        # Transform CI limits\n        if ci_lower[\"mu\"] is not None and ci_upper[\"mu\"] is not None:\n            if result.corrected:\n                obj_lower = rescale(pd=mu_lower, method=result.method)\n                obj_upper = rescale(pd=mu_upper, method=result.method)\n            else:\n                obj_lower = rescale(pc=mu_lower, method=result.method)\n                obj_upper = rescale(pc=mu_upper, method=result.method)\n\n            ci_lower[\"pc\"] = obj_lower.pc\n            ci_lower[\"pd\"] = obj_lower.pd\n            ci_lower[\"d_prime\"] = obj_lower.d_prime\n\n            ci_upper[\"pc\"] = obj_upper.pc\n            ci_upper[\"pd\"] = obj_upper.pd\n            ci_upper[\"d_prime\"] = obj_upper.d_prime\n\n    return cls(\n        estimates=estimates,\n        std_errors=std_errors,\n        ci_lower=ci_lower,\n        ci_upper=ci_upper,\n        level=level,\n        log_likelihood=result.log_likelihood,\n        lr_overdispersion=result.lr_overdispersion(),\n        lr_association=result.lr_association(),\n        method=result.method,\n        corrected=result.corrected,\n    )\n</code></pre>"},{"location":"api/models/#senspy.betabin.betabin","title":"<code>betabin(data: ArrayLike, method: str | Protocol = 'duotrio', corrected: bool = True, start: tuple[float, float] = (0.5, 0.5), vcov: bool = True, grad_tol: float = 0.0001) -&gt; BetaBinomialResult</code>","text":"<p>Fit beta-binomial model to over-dispersed discrimination data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Matrix or array with shape (N, 2) where first column is number of successes (correct responses) and second column is number of trials. Each row represents an independent observation (e.g., a panelist).</p> required <code>method</code> <code>str or Protocol</code> <p>The sensory discrimination protocol. One of 'triangle', 'duotrio', 'twoafc', 'threeafc', 'tetrad', 'hexad', 'twofive', 'twofivef'.</p> <code>'duotrio'</code> <code>corrected</code> <code>bool</code> <p>If True, fit the chance-corrected model where mu represents Pd (probability of discrimination). If False, fit the standard model where mu represents Pc (probability correct).</p> <code>True</code> <code>start</code> <code>tuple[float, float]</code> <p>Starting values for (mu, gamma) optimization. Must be in (0, 1).</p> <code>(0.5, 0.5)</code> <code>vcov</code> <code>bool</code> <p>Whether to compute the variance-covariance matrix.</p> <code>True</code> <code>grad_tol</code> <code>float</code> <p>Gradient tolerance for convergence warning.</p> <code>1e-4</code> <p>Returns:</p> Type Description <code>BetaBinomialResult</code> <p>Fitted model result with coefficients, vcov, log-likelihood, etc.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data has wrong shape or start values are out of bounds.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from senspy import betabin\n&gt;&gt;&gt; # Data: successes and total trials per panelist\n&gt;&gt;&gt; data = np.array([[3, 10], [5, 10], [7, 10], [4, 10], [6, 10]])\n&gt;&gt;&gt; result = betabin(data, method=\"triangle\", corrected=True)\n&gt;&gt;&gt; print(f\"mu={result.mu:.3f}, gamma={result.gamma:.3f}\")\n</code></pre> Source code in <code>senspy/betabin.py</code> <pre><code>def betabin(\n    data: ArrayLike,\n    method: str | Protocol = \"duotrio\",\n    corrected: bool = True,\n    start: tuple[float, float] = (0.5, 0.5),\n    vcov: bool = True,\n    grad_tol: float = 1e-4,\n) -&gt; BetaBinomialResult:\n    \"\"\"Fit beta-binomial model to over-dispersed discrimination data.\n\n    Parameters\n    ----------\n    data : array-like\n        Matrix or array with shape (N, 2) where first column is number of\n        successes (correct responses) and second column is number of trials.\n        Each row represents an independent observation (e.g., a panelist).\n    method : str or Protocol\n        The sensory discrimination protocol. One of 'triangle', 'duotrio',\n        'twoafc', 'threeafc', 'tetrad', 'hexad', 'twofive', 'twofivef'.\n    corrected : bool, default True\n        If True, fit the chance-corrected model where mu represents Pd\n        (probability of discrimination). If False, fit the standard model\n        where mu represents Pc (probability correct).\n    start : tuple[float, float], default (0.5, 0.5)\n        Starting values for (mu, gamma) optimization. Must be in (0, 1).\n    vcov : bool, default True\n        Whether to compute the variance-covariance matrix.\n    grad_tol : float, default 1e-4\n        Gradient tolerance for convergence warning.\n\n    Returns\n    -------\n    BetaBinomialResult\n        Fitted model result with coefficients, vcov, log-likelihood, etc.\n\n    Raises\n    ------\n    ValueError\n        If data has wrong shape or start values are out of bounds.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from senspy import betabin\n    &gt;&gt;&gt; # Data: successes and total trials per panelist\n    &gt;&gt;&gt; data = np.array([[3, 10], [5, 10], [7, 10], [4, 10], [6, 10]])\n    &gt;&gt;&gt; result = betabin(data, method=\"triangle\", corrected=True)\n    &gt;&gt;&gt; print(f\"mu={result.mu:.3f}, gamma={result.gamma:.3f}\")\n    \"\"\"\n    # Validate and convert data\n    data = np.asarray(data)\n    if data.ndim != 2 or data.shape[1] != 2:\n        raise ValueError(\"'data' must have 2 columns (successes, trials)\")\n    if data.shape[0] &lt; 3:\n        raise ValueError(\"'data' must have at least 3 rows\")\n\n    x = data[:, 0].astype(float)\n    n = data[:, 1].astype(float)\n\n    if np.any(x &lt; 0) or np.any(x &gt; n):\n        raise ValueError(\"Successes must be between 0 and trials\")\n\n    # Validate start values\n    if any(s &lt;= 1e-3 or s &gt;= 1 - 1e-3 for s in start):\n        raise ValueError(\"start values must be in the open interval (0, 1)\")\n\n    # Get protocol info\n    protocol = parse_protocol(method)\n    method_str = protocol.value\n    link = get_link(method_str)\n    p_guess = link.p_guess\n\n    # Set up bounds (log-space arithmetic allows small gamma values)\n    lower = np.array([1e-6, 1e-6])\n    upper = np.array([1 - 1e-6, 1 - 1e-6])\n\n    # Define objective function\n    if corrected:\n        def objective(params):\n            return _log_likelihood_corrected(params, x, n, p_guess)\n    else:\n        def objective(params):\n            return _log_likelihood_standard(params, x, n)\n\n    # Optimize\n    result = optimize.minimize(\n        objective,\n        x0=np.array(start),\n        method=\"L-BFGS-B\",\n        bounds=list(zip(lower, upper)),\n        options={\"ftol\": 1e-10, \"gtol\": 1e-8},\n    )\n\n    coef = result.x\n    mu, gamma = coef\n\n    # Check gradient at optimum\n    if all(coef &gt; lower) and all(coef &lt; upper):\n        # Compute gradient numerically\n        eps = 1e-7\n        grad = np.zeros(2)\n        for i in range(2):\n            params_plus = coef.copy()\n            params_plus[i] += eps\n            params_minus = coef.copy()\n            params_minus[i] -= eps\n            grad[i] = (objective(params_plus) - objective(params_minus)) / (2 * eps)\n\n        if np.max(np.abs(grad)) &gt; grad_tol:\n            warnings.warn(\n                f\"Optimizer terminated with max|gradient|: {np.max(np.abs(grad)):.2e}\",\n                stacklevel=2,\n            )\n    else:\n        warnings.warn(\"Parameters at boundary occurred\", stacklevel=2)\n\n    # Compute variance-covariance matrix\n    vcov_matrix = None\n    if vcov and all(coef &gt; lower) and all(coef &lt; upper):\n        # Compute Hessian numerically\n        hess = _compute_hessian(objective, coef)\n        try:\n            vcov_matrix = np.linalg.inv(hess)\n        except np.linalg.LinAlgError:\n            vcov_matrix = None\n\n    # Compute log-likelihoods for tests\n    # Factor (binomial coefficients) - constant term\n    factor = np.sum(special.gammaln(n + 1) - special.gammaln(x + 1) - special.gammaln(n - x + 1))\n    if corrected:\n        factor += np.sum((n - x) * np.log(1 - p_guess)) + np.sum(x * np.log(p_guess))\n\n    log_lik = -result.fun + factor\n\n    # Null model: binomial with p = p_guess\n    log_lik_null = np.sum(stats.binom.logpmf(x.astype(int), n.astype(int), p_guess))\n\n    # Mu-only model: binomial with p = observed proportion\n    p_obs = np.sum(x) / np.sum(n)\n    log_lik_mu = np.sum(stats.binom.logpmf(x.astype(int), n.astype(int), p_obs))\n\n    return BetaBinomialResult(\n        coefficients={\"mu\": mu, \"gamma\": gamma},\n        vcov=vcov_matrix,\n        log_likelihood=log_lik,\n        log_lik_null=log_lik_null,\n        log_lik_mu=log_lik_mu,\n        data=data,\n        method=method_str,\n        corrected=corrected,\n        convergence=0 if result.success else 1,\n        message=result.message,\n        n_iter=result.nit,\n        _p_guess=p_guess,\n    )\n</code></pre>"},{"location":"api/models/#betabinomialresult","title":"BetaBinomialResult","text":""},{"location":"api/models/#senspy.BetaBinomialResult","title":"<code>BetaBinomialResult(coefficients: dict[str, float], vcov: np.ndarray | None, log_likelihood: float, log_lik_null: float, log_lik_mu: float, data: np.ndarray, method: str, corrected: bool, convergence: int, message: str, n_iter: int, _p_guess: float)</code>  <code>dataclass</code>","text":"<p>Result from beta-binomial model fitting.</p> <p>Attributes:</p> Name Type Description <code>coefficients</code> <code>dict</code> <p>Dictionary with 'mu' and 'gamma' parameter estimates.</p> <code>vcov</code> <code>ndarray | None</code> <p>2x2 variance-covariance matrix of (mu, gamma), or None if not computed.</p> <code>log_likelihood</code> <code>float</code> <p>Log-likelihood at the MLE.</p> <code>log_lik_null</code> <code>float</code> <p>Log-likelihood under null model (p = p_guess).</p> <code>log_lik_mu</code> <code>float</code> <p>Log-likelihood under binomial model (p = observed proportion).</p> <code>data</code> <code>ndarray</code> <p>The input data matrix (successes, trials).</p> <code>method</code> <code>str</code> <p>The discrimination protocol used.</p> <code>corrected</code> <code>bool</code> <p>Whether the chance-corrected model was fit.</p> <code>convergence</code> <code>int</code> <p>Optimizer convergence status (0 = converged).</p> <code>message</code> <code>str</code> <p>Optimizer message.</p> <code>n_iter</code> <code>int</code> <p>Number of optimizer iterations.</p>"},{"location":"api/models/#senspy.BetaBinomialResult.gamma","title":"<code>gamma: float</code>  <code>property</code>","text":"<p>Overdispersion parameter estimate.</p>"},{"location":"api/models/#senspy.BetaBinomialResult.mu","title":"<code>mu: float</code>  <code>property</code>","text":"<p>Mean parameter estimate.</p>"},{"location":"api/models/#senspy.BetaBinomialResult.lr_association","title":"<code>lr_association() -&gt; tuple[float, float]</code>","text":"<p>Likelihood ratio test for association (difference from null).</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>(test statistic G^2, p-value) with 2 df.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def lr_association(self) -&gt; tuple[float, float]:\n    \"\"\"Likelihood ratio test for association (difference from null).\n\n    Returns\n    -------\n    tuple[float, float]\n        (test statistic G^2, p-value) with 2 df.\n    \"\"\"\n    g2 = 2 * (self.log_likelihood - self.log_lik_null)\n    p_value = stats.chi2.sf(g2, df=2)\n    return g2, p_value\n</code></pre>"},{"location":"api/models/#senspy.BetaBinomialResult.lr_overdispersion","title":"<code>lr_overdispersion() -&gt; tuple[float, float]</code>","text":"<p>Likelihood ratio test for overdispersion.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>(test statistic G^2, p-value) with 1 df. Note: The true df may be between 0.5 and 1 since gamma is tested at the boundary.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def lr_overdispersion(self) -&gt; tuple[float, float]:\n    \"\"\"Likelihood ratio test for overdispersion.\n\n    Returns\n    -------\n    tuple[float, float]\n        (test statistic G^2, p-value) with 1 df.\n        Note: The true df may be between 0.5 and 1 since gamma is tested\n        at the boundary.\n    \"\"\"\n    g2 = 2 * (self.log_likelihood - self.log_lik_mu)\n    p_value = stats.chi2.sf(g2, df=1)\n    return g2, p_value\n</code></pre>"},{"location":"api/models/#senspy.BetaBinomialResult.se","title":"<code>se() -&gt; dict[str, float] | None</code>","text":"<p>Standard errors of mu and gamma.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def se(self) -&gt; dict[str, float] | None:\n    \"\"\"Standard errors of mu and gamma.\"\"\"\n    if self.vcov is None:\n        return None\n    se_vals = np.sqrt(np.diag(self.vcov))\n    return {\"mu\": se_vals[0], \"gamma\": se_vals[1]}\n</code></pre>"},{"location":"api/models/#senspy.BetaBinomialResult.summary","title":"<code>summary(level: float = 0.95) -&gt; BetaBinomialSummary</code>","text":"<p>Generate summary with confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>float</code> <p>Confidence level (default 0.95).</p> <code>0.95</code> <p>Returns:</p> Type Description <code>BetaBinomialSummary</code> <p>Summary object with estimates, SEs, and CIs on multiple scales.</p> Source code in <code>senspy/betabin.py</code> <pre><code>def summary(self, level: float = 0.95) -&gt; \"BetaBinomialSummary\":\n    \"\"\"Generate summary with confidence intervals.\n\n    Parameters\n    ----------\n    level : float\n        Confidence level (default 0.95).\n\n    Returns\n    -------\n    BetaBinomialSummary\n        Summary object with estimates, SEs, and CIs on multiple scales.\n    \"\"\"\n    return BetaBinomialSummary.from_result(self, level=level)\n</code></pre>"},{"location":"api/models/#two-alternative-certainty-2-ac","title":"Two-Alternative Certainty (2-AC)","text":""},{"location":"api/models/#twoac","title":"twoac","text":""},{"location":"api/models/#senspy.twoac","title":"<code>twoac</code>","text":"<p>2-AC (2-Alternative Constant) protocol for discrimination and preference testing.</p> <p>The 2-AC protocol is equivalent to a 2-AFC protocol with a \"no-difference\" option, or a paired preference test with a \"no-preference\" option.</p> <p>Data format: (count_A, count_no_preference, count_B)</p>"},{"location":"api/models/#senspy.twoac.TwoACResult","title":"<code>TwoACResult(coefficients: np.ndarray, vcov: np.ndarray | None, log_likelihood: float, data: np.ndarray, d_prime_0: float, alternative: str, statistic: str, conf_level: float, stat_value: float | None = None, p_value: float | None = None, confint: np.ndarray | None = None)</code>  <code>dataclass</code>","text":"<p>Result from 2-AC protocol analysis.</p> <p>Attributes:</p> Name Type Description <code>coefficients</code> <code>ndarray</code> <p>2x2 matrix with estimates and standard errors for (tau, d_prime).</p> <code>vcov</code> <code>ndarray | None</code> <p>2x2 variance-covariance matrix, or None if not computable.</p> <code>log_likelihood</code> <code>float</code> <p>Log-likelihood at the MLE.</p> <code>data</code> <code>ndarray</code> <p>The input data (count_A, count_no_pref, count_B).</p> <code>d_prime_0</code> <code>float</code> <p>Null hypothesis value of d-prime.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis type.</p> <code>statistic</code> <code>str</code> <p>Test statistic type ('likelihood' or 'wald').</p> <code>conf_level</code> <code>float</code> <p>Confidence level for intervals.</p> <code>stat_value</code> <code>float | None</code> <p>Test statistic value.</p> <code>p_value</code> <code>float | None</code> <p>P-value from significance test.</p> <code>confint</code> <code>ndarray | None</code> <p>Confidence interval for d-prime.</p>"},{"location":"api/models/#senspy.twoac.TwoACResult.d_prime","title":"<code>d_prime: float</code>  <code>property</code>","text":"<p>D-prime estimate.</p>"},{"location":"api/models/#senspy.twoac.TwoACResult.se_d_prime","title":"<code>se_d_prime: float | None</code>  <code>property</code>","text":"<p>Standard error of d-prime.</p>"},{"location":"api/models/#senspy.twoac.TwoACResult.se_tau","title":"<code>se_tau: float | None</code>  <code>property</code>","text":"<p>Standard error of tau.</p>"},{"location":"api/models/#senspy.twoac.TwoACResult.tau","title":"<code>tau: float</code>  <code>property</code>","text":"<p>Threshold parameter estimate.</p>"},{"location":"api/models/#senspy.twoac.TwoACResult.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Format result as string.</p> Source code in <code>senspy/twoac.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format result as string.\"\"\"\n    lines = []\n    lines.append(f\"Results for the 2-AC protocol with data {self.data}:\")\n    lines.append(\"\")\n\n    # Coefficients table\n    lines.append(f\"{'':12} {'Estimate':&gt;12} {'Std. Error':&gt;12}\")\n    tau_se = f\"{self.se_tau:.4f}\" if self.se_tau is not None else \"NA\"\n    dp_se = f\"{self.se_d_prime:.4f}\" if self.se_d_prime is not None else \"NA\"\n    lines.append(f\"{'tau':&lt;12} {self.tau:&gt;12.4f} {tau_se:&gt;12}\")\n    lines.append(f\"{'d_prime':&lt;12} {self.d_prime:&gt;12.4f} {dp_se:&gt;12}\")\n\n    # Confidence interval\n    if self.confint is not None:\n        lines.append(\"\")\n        stat_name = \"likelihood root\" if self.statistic == \"likelihood\" else \"Wald\"\n        lines.append(\n            f\"Two-sided {self.conf_level * 100:.1f}% confidence interval for d-prime \"\n            f\"based on the {stat_name} statistic:\"\n        )\n        lines.append(f\"  Lower: {self.confint[0]:.4f}  Upper: {self.confint[1]:.4f}\")\n\n    # Significance test\n    if self.stat_value is not None and self.p_value is not None:\n        lines.append(\"\")\n        lines.append(\"Significance test:\")\n        stat_name = \"Likelihood root\" if self.statistic == \"likelihood\" else \"Wald\"\n        lines.append(f\"  {stat_name} statistic = {self.stat_value:.4f}, p-value = {self.p_value:.4g}\")\n        alt_text = {\n            \"two.sided\": \"different from\",\n            \"less\": \"less than\",\n            \"greater\": \"greater than\",\n        }[self.alternative]\n        lines.append(f\"  Alternative hypothesis: d-prime is {alt_text} {self.d_prime_0}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/models/#senspy.twoac.twoac","title":"<code>twoac(data: ArrayLike, d_prime_0: float = 0.0, conf_level: float = 0.95, statistic: str = 'likelihood', alternative: str = 'two.sided') -&gt; TwoACResult</code>","text":"<p>Analyze data from the 2-AC (2-Alternative Constant) protocol.</p> <p>The 2-AC protocol is equivalent to a 2-AFC protocol with a \"no-difference\" option, or a paired preference test with a \"no-preference\" option.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>A non-negative integer vector of length 3 with counts in the form (prefer_A, no_preference, prefer_B). If prefer_B &gt; prefer_A, the estimate of d-prime is positive.</p> required <code>d_prime_0</code> <code>float</code> <p>Value of d-prime under the null hypothesis.</p> <code>0.0</code> <code>conf_level</code> <code>float</code> <p>Confidence level for confidence intervals.</p> <code>0.95</code> <code>statistic</code> <code>str</code> <p>Test statistic type: \"likelihood\" or \"wald\".</p> <code>\"likelihood\"</code> <code>alternative</code> <code>str</code> <p>Alternative hypothesis: \"two.sided\", \"less\", or \"greater\".</p> <code>\"two.sided\"</code> <p>Returns:</p> Type Description <code>TwoACResult</code> <p>Result object with estimates, confidence intervals, and test results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy import twoac\n&gt;&gt;&gt; # Simple discrimination test\n&gt;&gt;&gt; result = twoac([2, 2, 6])\n&gt;&gt;&gt; print(f\"d-prime = {result.d_prime:.3f}\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Discrimination-difference test\n&gt;&gt;&gt; result = twoac([2, 5, 8], d_prime_0=0, alternative=\"greater\")\n&gt;&gt;&gt; print(f\"p-value = {result.p_value:.4f}\")\n</code></pre> Source code in <code>senspy/twoac.py</code> <pre><code>def twoac(\n    data: ArrayLike,\n    d_prime_0: float = 0.0,\n    conf_level: float = 0.95,\n    statistic: str = \"likelihood\",\n    alternative: str = \"two.sided\",\n) -&gt; TwoACResult:\n    \"\"\"Analyze data from the 2-AC (2-Alternative Constant) protocol.\n\n    The 2-AC protocol is equivalent to a 2-AFC protocol with a \"no-difference\"\n    option, or a paired preference test with a \"no-preference\" option.\n\n    Parameters\n    ----------\n    data : array-like\n        A non-negative integer vector of length 3 with counts in the form\n        (prefer_A, no_preference, prefer_B). If prefer_B &gt; prefer_A, the\n        estimate of d-prime is positive.\n    d_prime_0 : float, default 0.0\n        Value of d-prime under the null hypothesis.\n    conf_level : float, default 0.95\n        Confidence level for confidence intervals.\n    statistic : str, default \"likelihood\"\n        Test statistic type: \"likelihood\" or \"wald\".\n    alternative : str, default \"two.sided\"\n        Alternative hypothesis: \"two.sided\", \"less\", or \"greater\".\n\n    Returns\n    -------\n    TwoACResult\n        Result object with estimates, confidence intervals, and test results.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from senspy import twoac\n    &gt;&gt;&gt; # Simple discrimination test\n    &gt;&gt;&gt; result = twoac([2, 2, 6])\n    &gt;&gt;&gt; print(f\"d-prime = {result.d_prime:.3f}\")\n\n    &gt;&gt;&gt; # Discrimination-difference test\n    &gt;&gt;&gt; result = twoac([2, 5, 8], d_prime_0=0, alternative=\"greater\")\n    &gt;&gt;&gt; print(f\"p-value = {result.p_value:.4f}\")\n    \"\"\"\n    # Validate inputs\n    data = np.asarray(data, dtype=float)\n    if len(data) != 3:\n        raise ValueError(\"'data' must be a vector of length 3\")\n    if not np.allclose(np.round(data), data):\n        raise ValueError(\"'data' must contain integer values\")\n    if np.any(data &lt; 0):\n        raise ValueError(\"'data' must contain non-negative values\")\n\n    data = np.round(data).astype(int)\n\n    statistic = statistic.lower()\n    if statistic not in (\"likelihood\", \"wald\"):\n        raise ValueError(\"'statistic' must be 'likelihood' or 'wald'\")\n\n    alternative = alternative.lower().replace(\"-\", \".\").replace(\"_\", \".\")\n    if alternative not in (\"two.sided\", \"less\", \"greater\"):\n        raise ValueError(\"'alternative' must be 'two.sided', 'less', or 'greater'\")\n\n    if not 0 &lt; conf_level &lt; 1:\n        raise ValueError(\"'conf_level' must be between 0 and 1\")\n\n    # Get ML estimates\n    est = _estimate_2ac(data, compute_vcov=True)\n    d_prime = est[\"coefficients\"][1, 0]\n    se_d_prime = est[\"coefficients\"][1, 1]\n\n    # Initialize result\n    result = TwoACResult(\n        coefficients=est[\"coefficients\"],\n        vcov=est[\"vcov\"],\n        log_likelihood=est[\"log_likelihood\"],\n        data=data,\n        d_prime_0=d_prime_0,\n        alternative=alternative,\n        statistic=statistic,\n        conf_level=conf_level,\n    )\n\n    # Compute test statistic\n    if statistic == \"likelihood\":\n        p_value, lroot = _lr_test_2ac(data, d_prime_0, alternative)\n        result.stat_value = lroot\n        result.p_value = p_value\n    elif statistic == \"wald\" and est[\"vcov\"] is not None:\n        wald_stat = (d_prime - d_prime_0) / se_d_prime\n        result.stat_value = wald_stat\n        p_val = normal_pvalue(wald_stat, alternative)\n        result.p_value = float(p_val) if np.ndim(p_val) == 0 else float(p_val[0])\n\n    # Compute confidence interval\n    if est[\"vcov\"] is not None:\n        if statistic == \"wald\":\n            result.confint = _wald_confint_2ac(d_prime, se_d_prime, conf_level)\n        else:  # likelihood\n            result.confint = _profile_confint_2ac(\n                data, est[\"log_likelihood\"], d_prime, se_d_prime, conf_level\n            )\n\n    return result\n</code></pre>"},{"location":"api/models/#twoacresult","title":"TwoACResult","text":""},{"location":"api/models/#senspy.TwoACResult","title":"<code>TwoACResult(coefficients: np.ndarray, vcov: np.ndarray | None, log_likelihood: float, data: np.ndarray, d_prime_0: float, alternative: str, statistic: str, conf_level: float, stat_value: float | None = None, p_value: float | None = None, confint: np.ndarray | None = None)</code>  <code>dataclass</code>","text":"<p>Result from 2-AC protocol analysis.</p> <p>Attributes:</p> Name Type Description <code>coefficients</code> <code>ndarray</code> <p>2x2 matrix with estimates and standard errors for (tau, d_prime).</p> <code>vcov</code> <code>ndarray | None</code> <p>2x2 variance-covariance matrix, or None if not computable.</p> <code>log_likelihood</code> <code>float</code> <p>Log-likelihood at the MLE.</p> <code>data</code> <code>ndarray</code> <p>The input data (count_A, count_no_pref, count_B).</p> <code>d_prime_0</code> <code>float</code> <p>Null hypothesis value of d-prime.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis type.</p> <code>statistic</code> <code>str</code> <p>Test statistic type ('likelihood' or 'wald').</p> <code>conf_level</code> <code>float</code> <p>Confidence level for intervals.</p> <code>stat_value</code> <code>float | None</code> <p>Test statistic value.</p> <code>p_value</code> <code>float | None</code> <p>P-value from significance test.</p> <code>confint</code> <code>ndarray | None</code> <p>Confidence interval for d-prime.</p>"},{"location":"api/models/#senspy.TwoACResult.d_prime","title":"<code>d_prime: float</code>  <code>property</code>","text":"<p>D-prime estimate.</p>"},{"location":"api/models/#senspy.TwoACResult.se_d_prime","title":"<code>se_d_prime: float | None</code>  <code>property</code>","text":"<p>Standard error of d-prime.</p>"},{"location":"api/models/#senspy.TwoACResult.se_tau","title":"<code>se_tau: float | None</code>  <code>property</code>","text":"<p>Standard error of tau.</p>"},{"location":"api/models/#senspy.TwoACResult.tau","title":"<code>tau: float</code>  <code>property</code>","text":"<p>Threshold parameter estimate.</p>"},{"location":"api/models/#senspy.TwoACResult.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Format result as string.</p> Source code in <code>senspy/twoac.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format result as string.\"\"\"\n    lines = []\n    lines.append(f\"Results for the 2-AC protocol with data {self.data}:\")\n    lines.append(\"\")\n\n    # Coefficients table\n    lines.append(f\"{'':12} {'Estimate':&gt;12} {'Std. Error':&gt;12}\")\n    tau_se = f\"{self.se_tau:.4f}\" if self.se_tau is not None else \"NA\"\n    dp_se = f\"{self.se_d_prime:.4f}\" if self.se_d_prime is not None else \"NA\"\n    lines.append(f\"{'tau':&lt;12} {self.tau:&gt;12.4f} {tau_se:&gt;12}\")\n    lines.append(f\"{'d_prime':&lt;12} {self.d_prime:&gt;12.4f} {dp_se:&gt;12}\")\n\n    # Confidence interval\n    if self.confint is not None:\n        lines.append(\"\")\n        stat_name = \"likelihood root\" if self.statistic == \"likelihood\" else \"Wald\"\n        lines.append(\n            f\"Two-sided {self.conf_level * 100:.1f}% confidence interval for d-prime \"\n            f\"based on the {stat_name} statistic:\"\n        )\n        lines.append(f\"  Lower: {self.confint[0]:.4f}  Upper: {self.confint[1]:.4f}\")\n\n    # Significance test\n    if self.stat_value is not None and self.p_value is not None:\n        lines.append(\"\")\n        lines.append(\"Significance test:\")\n        stat_name = \"Likelihood root\" if self.statistic == \"likelihood\" else \"Wald\"\n        lines.append(f\"  {stat_name} statistic = {self.stat_value:.4f}, p-value = {self.p_value:.4g}\")\n        alt_text = {\n            \"two.sided\": \"different from\",\n            \"less\": \"less than\",\n            \"greater\": \"greater than\",\n        }[self.alternative]\n        lines.append(f\"  Alternative hypothesis: d-prime is {alt_text} {self.d_prime_0}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/models/#same-different","title":"Same-Different","text":""},{"location":"api/models/#samediff","title":"samediff","text":""},{"location":"api/models/#senspy.samediff","title":"<code>samediff</code>","text":"<p>Same-Different protocol for sensory discrimination testing.</p> <p>The Same-Different protocol presents participants with pairs of samples that are either the same (identical products) or different (two different products). Participants judge each pair as \"same\" or \"different\".</p> <p>Data format: (same_same, diff_same, same_diff, diff_diff) - same_same: Same response to same samples - diff_same: Different response to same samples - same_diff: Same response to different samples - diff_diff: Different response to different samples</p>"},{"location":"api/models/#senspy.samediff.SameDiffResult","title":"<code>SameDiffResult(coefficients: np.ndarray, vcov: np.ndarray | None, log_likelihood: float, data: np.ndarray, case: float, convergence: int | None = None)</code>  <code>dataclass</code>","text":"<p>Result from Same-Different protocol analysis.</p> <p>Attributes:</p> Name Type Description <code>coefficients</code> <code>ndarray</code> <p>2x2 matrix with estimates and standard errors for (tau, delta).</p> <code>vcov</code> <code>ndarray | None</code> <p>2x2 variance-covariance matrix, or None if not computable.</p> <code>log_likelihood</code> <code>float</code> <p>Log-likelihood at the MLE.</p> <code>data</code> <code>ndarray</code> <p>The input data (same_same, diff_same, same_diff, diff_diff).</p> <code>case</code> <code>float</code> <p>Case indicator for boundary conditions.</p> <code>convergence</code> <code>int | None</code> <p>Convergence indicator from optimization (0 = converged).</p>"},{"location":"api/models/#senspy.samediff.SameDiffResult.delta","title":"<code>delta: float</code>  <code>property</code>","text":"<p>Delta (d-prime) estimate.</p>"},{"location":"api/models/#senspy.samediff.SameDiffResult.se_delta","title":"<code>se_delta: float | None</code>  <code>property</code>","text":"<p>Standard error of delta.</p>"},{"location":"api/models/#senspy.samediff.SameDiffResult.se_tau","title":"<code>se_tau: float | None</code>  <code>property</code>","text":"<p>Standard error of tau.</p>"},{"location":"api/models/#senspy.samediff.SameDiffResult.tau","title":"<code>tau: float</code>  <code>property</code>","text":"<p>Threshold parameter estimate.</p>"},{"location":"api/models/#senspy.samediff.SameDiffResult.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Format result as string.</p> Source code in <code>senspy/samediff.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format result as string.\"\"\"\n    lines = []\n    lines.append(\"Same-Different Test Results\")\n    lines.append(f\"Data: ss={self.data[0]}, ds={self.data[1]}, sd={self.data[2]}, dd={self.data[3]}\")\n    lines.append(\"\")\n\n    # Coefficients table\n    lines.append(f\"{'':12} {'Estimate':&gt;12} {'Std. Error':&gt;12}\")\n    tau_se = f\"{self.se_tau:.4f}\" if self.se_tau is not None else \"NA\"\n    delta_se = f\"{self.se_delta:.4f}\" if self.se_delta is not None else \"NA\"\n\n    tau_str = f\"{self.tau:.4f}\" if np.isfinite(self.tau) else (\"Inf\" if self.tau &gt; 0 else \"-Inf\")\n    delta_str = f\"{self.delta:.4f}\" if np.isfinite(self.delta) else (\"Inf\" if self.delta &gt; 0 else \"NA\" if np.isnan(self.delta) else \"-Inf\")\n\n    lines.append(f\"{'tau':&lt;12} {tau_str:&gt;12} {tau_se:&gt;12}\")\n    lines.append(f\"{'delta':&lt;12} {delta_str:&gt;12} {delta_se:&gt;12}\")\n\n    lines.append(\"\")\n    lines.append(f\"Log-likelihood: {self.log_likelihood:.4f}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/models/#senspy.samediff.samediff","title":"<code>samediff(nsamesame: int | None = None, ndiffsame: int | None = None, nsamediff: int | None = None, ndiffdiff: int | None = None, data: ArrayLike | None = None, vcov: bool = True) -&gt; SameDiffResult</code>","text":"<p>Analyze data from the Same-Different protocol.</p> <p>The Same-Different protocol presents participants with pairs of samples that are either the same or different. Participants judge each pair as \"same\" or \"different\".</p> <p>Parameters:</p> Name Type Description Default <code>nsamesame</code> <code>int</code> <p>Number of same-answers on same-samples.</p> <code>None</code> <code>ndiffsame</code> <code>int</code> <p>Number of different-answers on same-samples.</p> <code>None</code> <code>nsamediff</code> <code>int</code> <p>Number of same-answers on different-samples.</p> <code>None</code> <code>ndiffdiff</code> <code>int</code> <p>Number of different-answers on different-samples.</p> <code>None</code> <code>data</code> <code>array - like</code> <p>Alternative input: array of [nsamesame, ndiffsame, nsamediff, ndiffdiff].</p> <code>None</code> <code>vcov</code> <code>bool</code> <p>Whether to compute variance-covariance matrix.</p> <code>True</code> <p>Returns:</p> Type Description <code>SameDiffResult</code> <p>Result object with estimates and inference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy import samediff\n&gt;&gt;&gt; # 8 same-same, 5 diff-same, 4 same-diff, 9 diff-diff\n&gt;&gt;&gt; result = samediff(8, 5, 4, 9)\n&gt;&gt;&gt; print(f\"tau = {result.tau:.3f}, delta = {result.delta:.3f}\")\n</code></pre> Source code in <code>senspy/samediff.py</code> <pre><code>def samediff(\n    nsamesame: int | None = None,\n    ndiffsame: int | None = None,\n    nsamediff: int | None = None,\n    ndiffdiff: int | None = None,\n    data: ArrayLike | None = None,\n    vcov: bool = True,\n) -&gt; SameDiffResult:\n    \"\"\"Analyze data from the Same-Different protocol.\n\n    The Same-Different protocol presents participants with pairs of samples\n    that are either the same or different. Participants judge each pair as\n    \"same\" or \"different\".\n\n    Parameters\n    ----------\n    nsamesame : int, optional\n        Number of same-answers on same-samples.\n    ndiffsame : int, optional\n        Number of different-answers on same-samples.\n    nsamediff : int, optional\n        Number of same-answers on different-samples.\n    ndiffdiff : int, optional\n        Number of different-answers on different-samples.\n    data : array-like, optional\n        Alternative input: array of [nsamesame, ndiffsame, nsamediff, ndiffdiff].\n    vcov : bool, default True\n        Whether to compute variance-covariance matrix.\n\n    Returns\n    -------\n    SameDiffResult\n        Result object with estimates and inference.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from senspy import samediff\n    &gt;&gt;&gt; # 8 same-same, 5 diff-same, 4 same-diff, 9 diff-diff\n    &gt;&gt;&gt; result = samediff(8, 5, 4, 9)\n    &gt;&gt;&gt; print(f\"tau = {result.tau:.3f}, delta = {result.delta:.3f}\")\n    \"\"\"\n    # Handle input\n    if data is not None:\n        data = np.asarray(data, dtype=float)\n        if len(data) != 4:\n            raise ValueError(\"'data' must be a vector of length 4\")\n        ss, ds, sd, dd = data\n    elif all(x is not None for x in [nsamesame, ndiffsame, nsamediff, ndiffdiff]):\n        ss, ds, sd, dd = float(nsamesame), float(ndiffsame), float(nsamediff), float(ndiffdiff)\n        data = np.array([ss, ds, sd, dd])\n    else:\n        raise ValueError(\"Either provide all four counts or use 'data' parameter\")\n\n    # Validate data\n    if not all(x == int(x) and x &gt;= 0 for x in [ss, ds, sd, dd]):\n        raise ValueError(\"Data must be non-negative integers\")\n\n    zero_count = sum(1 for x in [ss, ds, sd, dd] if x == 0)\n    if zero_count &gt; 2:\n        raise ValueError(\"Not enough information in data when more than two entries are zero\")\n\n    # Initialize result containers\n    vcov_mat = np.full((2, 2), np.nan)\n    se = np.array([np.nan, np.nan])\n    conv = None\n    case = 0.0\n\n    # Check if simple estimation is possible\n    # delta is estimable if P(same|same) &gt; P(same|diff)\n    is_delta = (ss / (ds + ss) &gt; sd / (sd + dd)) if (ds + ss &gt; 0 and sd + dd &gt; 0) else False\n\n    # Estimation based on case\n    if is_delta and all(x &gt; 0 for x in [ss, ds, sd, dd]):\n        # Case 0: General case - all data positive and delta estimable\n        tau = _compute_tau(ss, ds)\n        psd = sd / (sd + dd)\n\n        # Find delta by root-finding\n        try:\n            result = optimize.brentq(_delta_root, 0, 10, args=(tau, psd))\n            delta = result\n        except ValueError:\n            # Root not found in interval, try larger range\n            try:\n                result = optimize.brentq(_delta_root, 0, 100, args=(tau, psd))\n                delta = result\n            except ValueError:\n                delta = np.nan\n\n        if np.isfinite(delta) and vcov:\n            # Compute Fisher information and vcov\n            try:\n                i11 = _fisher_info_11(tau, delta, ss, ds, sd, dd)\n                i12 = _fisher_info_12(tau, delta, sd, dd)\n                i22 = _fisher_info_22(tau, delta, sd, dd)\n\n                fisher = np.array([[i11, i12], [i12, i22]])\n                vcov_mat = np.linalg.inv(fisher)\n                se = np.sqrt(np.diag(vcov_mat))\n            except (np.linalg.LinAlgError, ValueError):\n                pass\n\n        log_lik = _ll_samediff(tau, delta, ss, ds, sd, dd)\n        case = 0.0\n\n    elif ss == 0 and sd == 0:\n        # Case 0.1: No same responses -&gt; tau = 0, delta = NA\n        tau = 0.0\n        delta = np.nan\n        log_lik = 0.0\n        case = 0.1\n\n    elif ds == 0 and dd == 0:\n        # Case 1: No different responses -&gt; tau = Inf, delta = NA\n        tau = np.inf\n        delta = np.nan\n        log_lik = 0.0\n        case = 1.0\n\n    elif ds == 0 and sd == 0:\n        # Case 1.2: tau = Inf, delta = Inf\n        tau = np.inf\n        delta = np.inf\n        log_lik = 0.0\n        case = 1.2\n\n    elif ss == 0 and ds == 0:\n        # Case 1.12: No same-sample data -&gt; optimize over tau, delta\n        tau = np.inf\n        delta = np.nan\n\n        # Optimize likelihood\n        result = optimize.minimize(\n            lambda p: -_ll_ds0(p, sd, dd),\n            x0=[5.0, 5.0],\n            method=\"L-BFGS-B\",\n            bounds=[(1e-4, 100), (1e-4, 100)],\n        )\n        log_lik = -result.fun\n        conv = result.status\n        case = 1.12\n\n    elif sd == 0 and dd == 0:\n        # Case 1.3: No different-sample data -&gt; optimize tau, delta = NA\n        delta = np.nan\n\n        # Optimize tau with bounds to enforce positivity\n        result = optimize.minimize(\n            lambda t: -_ll_delta_inf(t[0], ss, ds),\n            x0=[1.0],\n            method=\"L-BFGS-B\",\n            bounds=[(1e-6, None)],\n        )\n        tau = result.x[0]\n        log_lik = _ll_delta_inf(tau, ss, ds)\n        conv = 0 if result.success else 1\n\n        if vcov:\n            # Compute Hessian for SE\n            eps = 1e-5\n            ll_p = _ll_delta_inf(tau + eps, ss, ds)\n            ll_m = _ll_delta_inf(tau - eps, ss, ds)\n            ll_0 = _ll_delta_inf(tau, ss, ds)\n            hess = (ll_p - 2 * ll_0 + ll_m) / (eps ** 2)\n            if hess &lt; 0:\n                vcov_mat[0, 0] = -1 / hess\n                se[0] = np.sqrt(vcov_mat[0, 0])\n\n        case = 1.3\n\n    elif ds == 0:\n        # Case 1.22: ds = 0 -&gt; tau = Inf, delta = Inf\n        tau = np.inf\n        delta = np.inf\n\n        result = optimize.minimize(\n            lambda p: -_ll_ds0(p, sd, dd),\n            x0=[5.0, 5.0],\n            method=\"L-BFGS-B\",\n            bounds=[(1e-4, 100), (1e-4, 100)],\n        )\n        log_lik = -result.fun\n        conv = result.status\n        case = 1.22\n\n    elif sd == 0:\n        # Case 3: sd = 0 -&gt; delta = Inf, optimize tau\n        delta = np.inf\n\n        # Optimize tau with bounds to enforce positivity\n        result = optimize.minimize(\n            lambda t: -_ll_delta_inf(t[0], ss, ds),\n            x0=[1.0],\n            method=\"L-BFGS-B\",\n            bounds=[(1e-6, None)],\n        )\n        tau = result.x[0]\n        log_lik = _ll_delta_inf(tau, ss, ds)\n        conv = 0 if result.success else 1\n\n        if vcov:\n            eps = 1e-5\n            ll_p = _ll_delta_inf(tau + eps, ss, ds)\n            ll_m = _ll_delta_inf(tau - eps, ss, ds)\n            ll_0 = _ll_delta_inf(tau, ss, ds)\n            hess = (ll_p - 2 * ll_0 + ll_m) / (eps ** 2)\n            if hess &lt; 0:\n                vcov_mat[0, 0] = -1 / hess\n                se[0] = np.sqrt(vcov_mat[0, 0])\n\n        case = 3.0\n\n    elif dd == 0 or ss == 0 or not is_delta:\n        # Case 2: delta = 0, optimize tau\n        delta = 0.0\n\n        def neg_ll_delta_zero(t):\n            return -_ll_samediff(t[0], 0.0, ss, ds, sd, dd)\n\n        # Optimize tau with bounds to enforce positivity\n        result = optimize.minimize(\n            neg_ll_delta_zero,\n            x0=[1.0],\n            method=\"L-BFGS-B\",\n            bounds=[(1e-6, None)],\n        )\n        tau = result.x[0]\n        log_lik = _ll_samediff(tau, 0.0, ss, ds, sd, dd)\n        conv = 0 if result.success else 1\n\n        if vcov:\n            eps = 1e-5\n            ll_p = _ll_samediff(tau + eps, 0.0, ss, ds, sd, dd)\n            ll_m = _ll_samediff(tau - eps, 0.0, ss, ds, sd, dd)\n            ll_0 = _ll_samediff(tau, 0.0, ss, ds, sd, dd)\n            hess = (ll_p - 2 * ll_0 + ll_m) / (eps ** 2)\n            if hess &lt; 0:\n                vcov_mat[0, 0] = -1 / hess\n                se[0] = np.sqrt(vcov_mat[0, 0])\n\n        case = 2.0\n\n    else:\n        raise RuntimeError(\"Unexpected case in samediff estimation\")\n\n    # Build coefficient matrix\n    coef = np.full((2, 2), np.nan)\n    coef[0, 0] = tau\n    coef[1, 0] = delta\n    coef[:, 1] = se\n\n    return SameDiffResult(\n        coefficients=coef,\n        vcov=vcov_mat if vcov else None,\n        log_likelihood=log_lik,\n        data=data.astype(int),\n        case=case,\n        convergence=conv,\n    )\n</code></pre>"},{"location":"api/models/#samediffresult","title":"SameDiffResult","text":""},{"location":"api/models/#senspy.SameDiffResult","title":"<code>SameDiffResult(coefficients: np.ndarray, vcov: np.ndarray | None, log_likelihood: float, data: np.ndarray, case: float, convergence: int | None = None)</code>  <code>dataclass</code>","text":"<p>Result from Same-Different protocol analysis.</p> <p>Attributes:</p> Name Type Description <code>coefficients</code> <code>ndarray</code> <p>2x2 matrix with estimates and standard errors for (tau, delta).</p> <code>vcov</code> <code>ndarray | None</code> <p>2x2 variance-covariance matrix, or None if not computable.</p> <code>log_likelihood</code> <code>float</code> <p>Log-likelihood at the MLE.</p> <code>data</code> <code>ndarray</code> <p>The input data (same_same, diff_same, same_diff, diff_diff).</p> <code>case</code> <code>float</code> <p>Case indicator for boundary conditions.</p> <code>convergence</code> <code>int | None</code> <p>Convergence indicator from optimization (0 = converged).</p>"},{"location":"api/models/#senspy.SameDiffResult.delta","title":"<code>delta: float</code>  <code>property</code>","text":"<p>Delta (d-prime) estimate.</p>"},{"location":"api/models/#senspy.SameDiffResult.se_delta","title":"<code>se_delta: float | None</code>  <code>property</code>","text":"<p>Standard error of delta.</p>"},{"location":"api/models/#senspy.SameDiffResult.se_tau","title":"<code>se_tau: float | None</code>  <code>property</code>","text":"<p>Standard error of tau.</p>"},{"location":"api/models/#senspy.SameDiffResult.tau","title":"<code>tau: float</code>  <code>property</code>","text":"<p>Threshold parameter estimate.</p>"},{"location":"api/models/#senspy.SameDiffResult.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Format result as string.</p> Source code in <code>senspy/samediff.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format result as string.\"\"\"\n    lines = []\n    lines.append(\"Same-Different Test Results\")\n    lines.append(f\"Data: ss={self.data[0]}, ds={self.data[1]}, sd={self.data[2]}, dd={self.data[3]}\")\n    lines.append(\"\")\n\n    # Coefficients table\n    lines.append(f\"{'':12} {'Estimate':&gt;12} {'Std. Error':&gt;12}\")\n    tau_se = f\"{self.se_tau:.4f}\" if self.se_tau is not None else \"NA\"\n    delta_se = f\"{self.se_delta:.4f}\" if self.se_delta is not None else \"NA\"\n\n    tau_str = f\"{self.tau:.4f}\" if np.isfinite(self.tau) else (\"Inf\" if self.tau &gt; 0 else \"-Inf\")\n    delta_str = f\"{self.delta:.4f}\" if np.isfinite(self.delta) else (\"Inf\" if self.delta &gt; 0 else \"NA\" if np.isnan(self.delta) else \"-Inf\")\n\n    lines.append(f\"{'tau':&lt;12} {tau_str:&gt;12} {tau_se:&gt;12}\")\n    lines.append(f\"{'delta':&lt;12} {delta_str:&gt;12} {delta_se:&gt;12}\")\n\n    lines.append(\"\")\n    lines.append(f\"Log-likelihood: {self.log_likelihood:.4f}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/models/#degree-of-difference-dod","title":"Degree-of-Difference (DOD)","text":""},{"location":"api/models/#dod","title":"dod","text":""},{"location":"api/models/#senspy.dod","title":"<code>dod</code>","text":"<p>Degree of Difference (DOD) model for sensory discrimination.</p> <p>This module implements the Thurstonian model for degree-of-difference (DOD) data. The DOD method asks assessors to rate the degree of difference between pairs of samples on an ordinal scale. Same-pairs and different-pairs are presented, and the responses are used to estimate d' (d-prime).</p> <p>The model estimates: - d-prime: the discriminability parameter - tau: boundary parameters defining the rating scale cutoffs</p> References <p>Christensen, R.H.B. et al. (2011). A Thurstonian model for the degree of difference method with multiple response categories.</p>"},{"location":"api/models/#senspy.dod.DODControl","title":"<code>DODControl(grad_tol: float = 0.0001, integer_tol: float = 1e-08, get_vcov: bool = True, get_grad: bool = True, test_args: bool = True, do_warn: bool = True, opt_options: dict = dict())</code>  <code>dataclass</code>","text":"<p>Control parameters for DOD model fitting.</p> <p>Attributes:</p> Name Type Description <code>grad_tol</code> <code>float</code> <p>Tolerance for gradient convergence check. Default is 1e-4.</p> <code>integer_tol</code> <code>float</code> <p>Tolerance for checking if counts are integers. Default is 1e-8.</p> <code>get_vcov</code> <code>bool</code> <p>Whether to compute variance-covariance matrix. Default is True.</p> <code>get_grad</code> <code>bool</code> <p>Whether to compute and check gradient. Default is True.</p> <code>test_args</code> <code>bool</code> <p>Whether to test argument validity. Default is True.</p> <code>do_warn</code> <code>bool</code> <p>Whether to emit warnings. Default is True.</p> <code>opt_options</code> <code>dict</code> <p>Additional options passed to scipy.optimize.minimize. Default is empty dict.</p>"},{"location":"api/models/#senspy.dod.DODFitResult","title":"<code>DODFitResult(d_prime: float, tau: NDArray, log_lik: float, coefficients: NDArray, vcov: NDArray | None, gradient: NDArray | None, hessian: NDArray | None, data: NDArray, convergence: int)</code>  <code>dataclass</code>","text":"<p>Result from dod_fit function.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated d-prime (discriminability).</p> <code>tau</code> <code>NDArray</code> <p>Estimated boundary parameters.</p> <code>log_lik</code> <code>float</code> <p>Log-likelihood at the estimates.</p> <code>coefficients</code> <code>NDArray</code> <p>Combined coefficient vector [tau1, tau2, ..., d_prime].</p> <code>vcov</code> <code>NDArray | None</code> <p>Variance-covariance matrix of coefficients.</p> <code>gradient</code> <code>NDArray | None</code> <p>Gradient at convergence.</p> <code>hessian</code> <code>NDArray | None</code> <p>Hessian matrix at convergence.</p> <code>data</code> <code>NDArray</code> <p>The 2 x k data matrix (same-pairs, diff-pairs).</p> <code>convergence</code> <code>int</code> <p>Optimizer convergence code (0 = success).</p>"},{"location":"api/models/#senspy.dod.DODPowerResult","title":"<code>DODPowerResult(power: float, se_power: float, n_used: int, d_primeA: float, d_prime0: float, sample_size: tuple[int, int], nsim: int, alpha: float, statistic: str, alternative: str, tau: NDArray)</code>  <code>dataclass</code>","text":"<p>Result from dod_power function.</p> <p>Attributes:</p> Name Type Description <code>power</code> <code>float</code> <p>Estimated power (proportion of simulations with p &lt; alpha).</p> <code>se_power</code> <code>float</code> <p>Standard error of the power estimate.</p> <code>n_used</code> <code>int</code> <p>Number of simulations used (excluding failures).</p> <code>d_primeA</code> <code>float</code> <p>True d-prime value (alternative hypothesis).</p> <code>d_prime0</code> <code>float</code> <p>Null hypothesis d-prime.</p> <code>sample_size</code> <code>tuple[int, int]</code> <p>Sample sizes (same_pairs, diff_pairs).</p> <code>nsim</code> <code>int</code> <p>Total number of simulations requested.</p> <code>alpha</code> <code>float</code> <p>Significance level.</p> <code>statistic</code> <code>str</code> <p>Test statistic used.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis.</p> <code>tau</code> <code>NDArray</code> <p>Boundary parameters used for simulation.</p>"},{"location":"api/models/#senspy.dod.DODResult","title":"<code>DODResult(d_prime: float, tau: NDArray, log_lik: float, se_d_prime: float, se_tau: NDArray, conf_int: tuple[float, float], conf_level: float, conf_method: str, stat_value: float, p_value: float, statistic: str, alternative: str, d_prime0: float, data: NDArray, vcov: NDArray | None, coefficients: NDArray, convergence: int)</code>  <code>dataclass</code>","text":"<p>Result from dod function.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated d-prime (discriminability).</p> <code>tau</code> <code>NDArray</code> <p>Estimated boundary parameters.</p> <code>log_lik</code> <code>float</code> <p>Log-likelihood at the estimates.</p> <code>se_d_prime</code> <code>float</code> <p>Standard error of d-prime estimate.</p> <code>se_tau</code> <code>NDArray</code> <p>Standard errors of tau estimates.</p> <code>conf_int</code> <code>tuple[float, float]</code> <p>Confidence interval for d-prime.</p> <code>conf_level</code> <code>float</code> <p>Confidence level used for interval.</p> <code>conf_method</code> <code>str</code> <p>Method used for confidence interval (\"profile likelihood\" or \"Wald\").</p> <code>stat_value</code> <code>float</code> <p>Value of the test statistic.</p> <code>p_value</code> <code>float</code> <p>P-value for the hypothesis test.</p> <code>statistic</code> <code>str</code> <p>Type of test statistic used.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis (\"greater\", \"less\", \"two.sided\").</p> <code>d_prime0</code> <code>float</code> <p>Null hypothesis value for d-prime.</p> <code>data</code> <code>NDArray</code> <p>The 2 x k data matrix (same-pairs, diff-pairs).</p> <code>vcov</code> <code>NDArray | None</code> <p>Variance-covariance matrix of coefficients.</p> <code>coefficients</code> <code>NDArray</code> <p>Combined coefficient vector [tau1, tau2, ..., d_prime].</p> <code>convergence</code> <code>int</code> <p>Optimizer convergence code (0 = success).</p>"},{"location":"api/models/#senspy.dod.dod","title":"<code>dod(same: ArrayLike, diff: ArrayLike, d_prime0: float = 0.0, conf_level: float = 0.95, statistic: Literal['likelihood', 'Pearson', 'Wilcoxon', 'Wald'] = 'likelihood', alternative: Literal['difference', 'similarity', 'two.sided', 'less', 'greater'] = 'difference', control: DODControl | None = None) -&gt; DODResult</code>","text":"<p>Fit DOD model and perform hypothesis test.</p> <p>The Degree-of-Difference (DOD) model is a Thurstonian model for sensory discrimination where assessors rate the degree of difference between pairs of samples on an ordinal scale.</p> <p>Parameters:</p> Name Type Description Default <code>same</code> <code>ArrayLike</code> <p>Counts for same-pairs in each response category.</p> required <code>diff</code> <code>ArrayLike</code> <p>Counts for different-pairs in each response category.</p> required <code>d_prime0</code> <code>float</code> <p>Null hypothesis value for d-prime. Default is 0.</p> <code>0.0</code> <code>conf_level</code> <code>float</code> <p>Confidence level for interval. Default is 0.95.</p> <code>0.95</code> <code>statistic</code> <code>str</code> <p>Test statistic to use: - \"likelihood\": Likelihood ratio test (default) - \"Pearson\": Pearson chi-square test - \"Wilcoxon\": Wilcoxon rank-sum test (only for d_prime0=0) - \"Wald\": Wald test</p> <code>'likelihood'</code> <code>alternative</code> <code>str</code> <p>Alternative hypothesis: - \"difference\" or \"greater\": d-prime &gt; d_prime0 (default) - \"similarity\" or \"less\": d-prime &lt; d_prime0 - \"two.sided\": d-prime != d_prime0</p> <code>'difference'</code> <code>control</code> <code>DODControl | None</code> <p>Control parameters. If None, uses defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>DODResult</code> <p>Complete analysis results including estimates, confidence interval, and hypothesis test.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If arguments are invalid or incompatible.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; same = [20, 30, 25, 25]\n&gt;&gt;&gt; diff = [10, 20, 35, 35]\n&gt;&gt;&gt; result = dod(same, diff)\n&gt;&gt;&gt; print(f\"d-prime: {result.d_prime:.3f}\")\n&gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n</code></pre> Source code in <code>senspy/dod.py</code> <pre><code>def dod(\n    same: ArrayLike,\n    diff: ArrayLike,\n    d_prime0: float = 0.0,\n    conf_level: float = 0.95,\n    statistic: Literal[\"likelihood\", \"Pearson\", \"Wilcoxon\", \"Wald\"] = \"likelihood\",\n    alternative: Literal[\n        \"difference\", \"similarity\", \"two.sided\", \"less\", \"greater\"\n    ] = \"difference\",\n    control: DODControl | None = None,\n) -&gt; DODResult:\n    \"\"\"Fit DOD model and perform hypothesis test.\n\n    The Degree-of-Difference (DOD) model is a Thurstonian model for sensory\n    discrimination where assessors rate the degree of difference between\n    pairs of samples on an ordinal scale.\n\n    Parameters\n    ----------\n    same : ArrayLike\n        Counts for same-pairs in each response category.\n    diff : ArrayLike\n        Counts for different-pairs in each response category.\n    d_prime0 : float\n        Null hypothesis value for d-prime. Default is 0.\n    conf_level : float\n        Confidence level for interval. Default is 0.95.\n    statistic : str\n        Test statistic to use:\n        - \"likelihood\": Likelihood ratio test (default)\n        - \"Pearson\": Pearson chi-square test\n        - \"Wilcoxon\": Wilcoxon rank-sum test (only for d_prime0=0)\n        - \"Wald\": Wald test\n    alternative : str\n        Alternative hypothesis:\n        - \"difference\" or \"greater\": d-prime &gt; d_prime0 (default)\n        - \"similarity\" or \"less\": d-prime &lt; d_prime0\n        - \"two.sided\": d-prime != d_prime0\n    control : DODControl | None\n        Control parameters. If None, uses defaults.\n\n    Returns\n    -------\n    DODResult\n        Complete analysis results including estimates, confidence interval,\n        and hypothesis test.\n\n    Raises\n    ------\n    ValueError\n        If arguments are invalid or incompatible.\n\n    Examples\n    --------\n    &gt;&gt;&gt; same = [20, 30, 25, 25]\n    &gt;&gt;&gt; diff = [10, 20, 35, 35]\n    &gt;&gt;&gt; result = dod(same, diff)\n    &gt;&gt;&gt; print(f\"d-prime: {result.d_prime:.3f}\")\n    &gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n    \"\"\"\n    if control is None:\n        control = DODControl()\n\n    # Map alternative names\n    if alternative == \"difference\":\n        alternative = \"greater\"\n    elif alternative == \"similarity\":\n        alternative = \"less\"\n\n    # Validate arguments\n    if control.test_args:\n        if not (0 &lt; conf_level &lt; 1):\n            raise ValueError(\"conf_level must be between 0 and 1\")\n        if d_prime0 &lt; 0:\n            raise ValueError(\"d_prime0 must be non-negative\")\n        if np.isclose(d_prime0, 0.0) and alternative not in [\"difference\", \"greater\"]:\n            raise ValueError(\n                \"'alternative' has to be 'difference' or 'greater' if 'd_prime0' is 0\"\n            )\n        if not np.isclose(d_prime0, 0.0) and statistic == \"Wilcoxon\":\n            raise ValueError(\"Wilcoxon statistic only available with d_prime0 = 0\")\n\n    # Fit the model\n    fit = dod_fit(same, diff, control=control)\n\n    # Get cleaned data\n    same_arr = fit.data[0, :]\n    diff_arr = fit.data[1, :]\n    nlev = len(same_arr)\n    npar = len(fit.coefficients)\n\n    # Compute standard errors\n    se_d_prime = np.nan\n    se_tau = np.full(nlev - 1, np.nan)\n    if fit.d_prime &lt; 0.01 and control.get_vcov and control.do_warn:\n        import warnings\n\n        warnings.warn(\"d_prime &lt; 0.01: standard errors are unavailable\")\n    if fit.vcov is not None and np.all(np.isfinite(fit.vcov)):\n        se_all = np.sqrt(np.diag(fit.vcov))\n        se_tau = se_all[:-1]\n        se_d_prime = se_all[-1]\n\n    # Compute confidence interval\n    if statistic == \"Wald\":\n        conf_int = _wald_ci_dod(fit, conf_level)\n        conf_method = \"Wald\"\n    else:\n        conf_int = _profile_ci_dod(same_arr, diff_arr, fit, conf_level)\n        conf_method = \"profile likelihood\"\n\n    # Compute test statistic and p-value\n    stat_value = np.nan\n    p_value = np.nan\n\n    if statistic == \"Wilcoxon\":\n        # Wilcoxon rank-sum test\n        from scipy.stats import mannwhitneyu\n\n        # Create pseudo-data for Wilcoxon test\n        # Expand rating categories by their counts for each pair type\n        diff_pair_ratings = np.repeat(np.arange(1, nlev + 1), diff_arr.astype(int))\n        same_pair_ratings = np.repeat(np.arange(1, nlev + 1), same_arr.astype(int))\n\n        if alternative == \"two.sided\":\n            alt_mwu = \"two-sided\"\n        elif alternative == \"greater\":\n            alt_mwu = \"greater\"\n        else:\n            alt_mwu = \"less\"\n\n        try:\n            # Test if diff-pair ratings are stochastically greater than same-pair ratings\n            mwu_result = mannwhitneyu(diff_pair_ratings, same_pair_ratings, alternative=alt_mwu)\n            stat_value = mwu_result.statistic\n            p_value = mwu_result.pvalue\n        except ValueError:\n            pass\n\n    elif statistic == \"Wald\":\n        if np.isfinite(se_d_prime):\n            stat_value = (fit.d_prime - d_prime0) / se_d_prime\n            p_value = normal_pvalue(stat_value, alternative)\n\n    elif statistic in [\"likelihood\", \"Pearson\"]:\n        # Fit model under null hypothesis\n        ctrl0 = DODControl(\n            get_vcov=False, get_grad=False, test_args=False, do_warn=False\n        )\n        fit0 = dod_fit(same_arr, diff_arr, d_prime=d_prime0, control=ctrl0)\n        log_lik0 = fit0.log_lik\n\n        # Check convergence\n        if fit.log_lik &lt; log_lik0 and abs(fit.log_lik - log_lik0) &gt; 1e-6:\n            if control.do_warn:\n                import warnings\n\n                warnings.warn(\n                    \"Estimation of DOD model failed: likelihood did not increase\"\n                )\n\n        if statistic == \"likelihood\":\n            LR = 2 * (fit.log_lik - log_lik0)\n            if LR &lt; 0 and abs(LR) &lt; 1e-4:\n                LR = 0\n            if LR &gt;= 0:\n                stat_value = np.sign(fit.d_prime - d_prime0) * np.sqrt(LR)\n                p_value = normal_pvalue(stat_value, alternative)\n\n        else:  # Pearson\n            # Expected frequencies under alternative\n            freq = _par2prob_dod_internal(fit.tau, fit.d_prime) * np.array(\n                [[np.sum(same_arr)], [np.sum(diff_arr)]]\n            )\n            # Expected frequencies under null\n            freq0 = _par2prob_dod_internal(fit0.tau, fit0.d_prime) * np.array(\n                [[np.sum(same_arr)], [np.sum(diff_arr)]]\n            )\n\n            X2 = np.sum((freq - freq0) ** 2 / freq0)\n            stat_value = np.sign(fit.d_prime - d_prime0) * np.sqrt(X2)\n            p_value = normal_pvalue(stat_value, alternative)\n\n    return DODResult(\n        d_prime=fit.d_prime,\n        tau=fit.tau,\n        log_lik=fit.log_lik,\n        se_d_prime=se_d_prime,\n        se_tau=se_tau,\n        conf_int=conf_int,\n        conf_level=conf_level,\n        conf_method=conf_method,\n        stat_value=stat_value,\n        p_value=p_value,\n        statistic=statistic,\n        alternative=alternative,\n        d_prime0=d_prime0,\n        data=fit.data,\n        vcov=fit.vcov,\n        coefficients=fit.coefficients,\n        convergence=fit.convergence,\n    )\n</code></pre>"},{"location":"api/models/#senspy.dod.dod_fit","title":"<code>dod_fit(same: ArrayLike, diff: ArrayLike, tau: NDArray | None = None, d_prime: float | None = None, control: DODControl | None = None) -&gt; DODFitResult</code>","text":"<p>Fit DOD model (low-level function).</p> <p>This is the lower-level fitting function. Use <code>dod()</code> for the full analysis including hypothesis testing.</p> <p>Parameters:</p> Name Type Description Default <code>same</code> <code>ArrayLike</code> <p>Counts for same-pairs in each response category.</p> required <code>diff</code> <code>ArrayLike</code> <p>Counts for different-pairs in each response category.</p> required <code>tau</code> <code>NDArray | None</code> <p>Fixed boundary parameters. If None, estimated from data.</p> <code>None</code> <code>d_prime</code> <code>float | None</code> <p>Fixed d-prime value. If None, estimated from data.</p> <code>None</code> <code>control</code> <code>DODControl | None</code> <p>Control parameters. If None, uses defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>DODFitResult</code> <p>Fitting results including estimates and diagnostics.</p> Source code in <code>senspy/dod.py</code> <pre><code>def dod_fit(\n    same: ArrayLike,\n    diff: ArrayLike,\n    tau: NDArray | None = None,\n    d_prime: float | None = None,\n    control: DODControl | None = None,\n) -&gt; DODFitResult:\n    \"\"\"Fit DOD model (low-level function).\n\n    This is the lower-level fitting function. Use `dod()` for the full analysis\n    including hypothesis testing.\n\n    Parameters\n    ----------\n    same : ArrayLike\n        Counts for same-pairs in each response category.\n    diff : ArrayLike\n        Counts for different-pairs in each response category.\n    tau : NDArray | None\n        Fixed boundary parameters. If None, estimated from data.\n    d_prime : float | None\n        Fixed d-prime value. If None, estimated from data.\n    control : DODControl | None\n        Control parameters. If None, uses defaults.\n\n    Returns\n    -------\n    DODFitResult\n        Fitting results including estimates and diagnostics.\n    \"\"\"\n    if control is None:\n        control = DODControl()\n\n    # Validate data\n    if control.test_args:\n        same, diff = _validate_dod_data(same, diff, control.integer_tol)\n    else:\n        same = np.asarray(same, dtype=float)\n        diff = np.asarray(diff, dtype=float)\n\n    nlev = len(same)\n    data = np.vstack([same, diff])\n\n    # Initialize result variables\n    vcov = None\n    gradient = None\n    hessian = None\n    convergence = 0\n\n    # CASE 1: d_prime = 0 and tau = None\n    if tau is None and d_prime is not None and np.isclose(d_prime, 0.0):\n        est_tau = _dod_null_tau_internal(same, diff)\n        est_d_prime = d_prime\n\n    # CASE 2: Need to optimize tau, d_prime, or both\n    elif tau is None or d_prime is None:\n        if d_prime is None and tau is None:\n            # Estimate both tau and d_prime\n            start = np.concatenate([np.cumsum(_init_tau(nlev)), [1.0]])\n            par_names = [f\"tau{i+1}\" for i in range(nlev - 1)] + [\"d_prime\"]\n            npar = len(start)\n\n            def objfun(par):\n                return _dod_nll_internal(par[: npar - 1], par[npar - 1], same, diff)\n\n            case = \"both\"\n\n        elif tau is None and d_prime is not None:\n            # Estimate tau only\n            start = np.cumsum(_init_tau(nlev))\n            par_names = [f\"tau{i+1}\" for i in range(nlev - 1)]\n\n            def objfun(par):\n                return _dod_nll_internal(par, d_prime, same, diff)\n\n            case = \"tau\"\n\n        else:  # tau is not None and d_prime is None\n            # Estimate d_prime only\n            start = np.array([1.0])\n            par_names = [\"d_prime\"]\n\n            def objfun(par):\n                return _dod_nll_internal(tau, par[0], same, diff)\n\n            case = \"d_prime\"\n\n        # Optimization - use small positive lower bound to avoid numerical issues\n        result = optimize.minimize(\n            objfun,\n            start,\n            method=\"L-BFGS-B\",\n            bounds=[(1e-6, None)] * len(start),\n            options=control.opt_options,\n        )\n\n        par = result.x\n        convergence = 0 if result.success else result.status\n\n        # Compute gradient and Hessian if requested\n        if np.all(par &gt; 1e-2) and control.get_grad:\n            gradient = _numerical_gradient(objfun, par)\n\n            if not np.all(np.isfinite(gradient)):\n                if control.do_warn:\n                    import warnings\n\n                    warnings.warn(\"Cannot assess convergence: non-finite gradient\")\n            else:\n                if np.max(np.abs(gradient)) &gt; control.grad_tol and control.do_warn:\n                    import warnings\n\n                    warnings.warn(\n                        f\"Estimation failed with max(gradient) = {np.max(np.abs(gradient)):.2g} \"\n                        f\"(grad_tol = {control.grad_tol:.2g})\"\n                    )\n\n                if control.get_vcov:\n                    try:\n                        hessian = _numerical_hessian(objfun, par)\n                        if not np.all(np.isfinite(hessian)):\n                            if control.do_warn:\n                                import warnings\n\n                                warnings.warn(\"unable to compute Hessian\")\n                        else:\n                            # Check positive definiteness via Cholesky\n                            try:\n                                np.linalg.cholesky(hessian)\n                                vcov = np.linalg.inv(hessian)\n                            except np.linalg.LinAlgError:\n                                if control.do_warn:\n                                    import warnings\n\n                                    warnings.warn(\n                                        \"Model is ill-defined and may not have converged\"\n                                    )\n                    except Exception:\n                        if control.do_warn:\n                            import warnings\n\n                            warnings.warn(\"unable to compute Hessian\")\n\n        # Extract parameters based on case\n        if case == \"both\":\n            est_tau = par[: npar - 1]\n            est_d_prime = par[npar - 1]\n        elif case == \"tau\":\n            est_tau = par\n            est_d_prime = d_prime\n        else:  # case == \"d_prime\"\n            est_tau = tau\n            est_d_prime = par[0]\n\n    # CASE 3: Both tau and d_prime provided - just evaluate likelihood\n    else:\n        est_tau = np.asarray(tau, dtype=float)\n        est_d_prime = d_prime\n\n    # Compute log-likelihood\n    log_lik = -_dod_nll_internal(est_tau, est_d_prime, same, diff)\n\n    # Build coefficients vector\n    coefficients = np.concatenate([est_tau, [est_d_prime]])\n\n    return DODFitResult(\n        d_prime=est_d_prime,\n        tau=est_tau,\n        log_lik=log_lik,\n        coefficients=coefficients,\n        vcov=vcov,\n        gradient=gradient,\n        hessian=hessian,\n        data=data,\n        convergence=convergence,\n    )\n</code></pre>"},{"location":"api/models/#senspy.dod.dod_power","title":"<code>dod_power(d_primeA: float, d_prime0: float = 0.0, ncat: int = 4, sample_size: int | tuple[int, int] = 100, nsim: int = 1000, alpha: float = 0.05, method_tau: Literal['equi_prob', 'LR_max', 'se_min', 'user_defined'] = 'LR_max', statistic: Literal['likelihood', 'Wilcoxon', 'Pearson', 'Wald'] = 'likelihood', alternative: Literal['difference', 'similarity', 'two.sided', 'less', 'greater'] = 'difference', tau: NDArray | None = None, random_state: int | np.random.Generator | None = None) -&gt; DODPowerResult</code>","text":"<p>Compute power for DOD discrimination test via simulation.</p> <p>This function estimates the power of a DOD (Degree-of-Difference) test by simulating data under the alternative hypothesis and computing the proportion of simulations that would reject the null hypothesis.</p> <p>Parameters:</p> Name Type Description Default <code>d_primeA</code> <code>float</code> <p>True d-prime value (alternative hypothesis).</p> required <code>d_prime0</code> <code>float</code> <p>Null hypothesis d-prime value. Default is 0.</p> <code>0.0</code> <code>ncat</code> <code>int</code> <p>Number of response categories. Default is 4.</p> <code>4</code> <code>sample_size</code> <code>int | tuple[int, int]</code> <p>Sample size. If int, same size for both same-pairs and diff-pairs. If tuple, (same_size, diff_size). Default is 100.</p> <code>100</code> <code>nsim</code> <code>int</code> <p>Number of simulations. Default is 1000.</p> <code>1000</code> <code>alpha</code> <code>float</code> <p>Significance level. Default is 0.05.</p> <code>0.05</code> <code>method_tau</code> <code>str</code> <p>Method for determining tau values: - \"LR_max\": Maximum likelihood ratio (default) - \"equi_prob\": Equal category probabilities - \"se_min\": Minimum standard error - \"user_defined\": Use provided tau values</p> <code>'LR_max'</code> <code>statistic</code> <code>str</code> <p>Test statistic: - \"likelihood\": Likelihood ratio test (default) - \"Pearson\": Pearson chi-square - \"Wilcoxon\": Wilcoxon rank-sum test (requires d_prime0=0) - \"Wald\": Wald test</p> <code>'likelihood'</code> <code>alternative</code> <code>str</code> <p>Alternative hypothesis: - \"difference\" or \"greater\": d-prime &gt; d_prime0 (default) - \"similarity\" or \"less\": d-prime &lt; d_prime0 - \"two.sided\": d-prime != d_prime0</p> <code>'difference'</code> <code>tau</code> <code>NDArray | None</code> <p>Boundary parameters (required if method_tau=\"user_defined\").</p> <code>None</code> <code>random_state</code> <code>int | Generator | None</code> <p>Random state for reproducibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>DODPowerResult</code> <p>Power analysis results including power estimate and standard error.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid or incompatible.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = dod_power(d_primeA=1.0, sample_size=100, nsim=500)\n&gt;&gt;&gt; print(f\"Power: {result.power:.3f} (SE: {result.se_power:.3f})\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With specific tau values\n&gt;&gt;&gt; result = dod_power(\n...     d_primeA=1.5,\n...     method_tau=\"user_defined\",\n...     tau=np.array([0.5, 1.0, 1.5])\n... )\n</code></pre> Source code in <code>senspy/dod.py</code> <pre><code>def dod_power(\n    d_primeA: float,\n    d_prime0: float = 0.0,\n    ncat: int = 4,\n    sample_size: int | tuple[int, int] = 100,\n    nsim: int = 1000,\n    alpha: float = 0.05,\n    method_tau: Literal[\"equi_prob\", \"LR_max\", \"se_min\", \"user_defined\"] = \"LR_max\",\n    statistic: Literal[\"likelihood\", \"Wilcoxon\", \"Pearson\", \"Wald\"] = \"likelihood\",\n    alternative: Literal[\n        \"difference\", \"similarity\", \"two.sided\", \"less\", \"greater\"\n    ] = \"difference\",\n    tau: NDArray | None = None,\n    random_state: int | np.random.Generator | None = None,\n) -&gt; DODPowerResult:\n    \"\"\"Compute power for DOD discrimination test via simulation.\n\n    This function estimates the power of a DOD (Degree-of-Difference) test\n    by simulating data under the alternative hypothesis and computing the\n    proportion of simulations that would reject the null hypothesis.\n\n    Parameters\n    ----------\n    d_primeA : float\n        True d-prime value (alternative hypothesis).\n    d_prime0 : float\n        Null hypothesis d-prime value. Default is 0.\n    ncat : int\n        Number of response categories. Default is 4.\n    sample_size : int | tuple[int, int]\n        Sample size. If int, same size for both same-pairs and diff-pairs.\n        If tuple, (same_size, diff_size). Default is 100.\n    nsim : int\n        Number of simulations. Default is 1000.\n    alpha : float\n        Significance level. Default is 0.05.\n    method_tau : str\n        Method for determining tau values:\n        - \"LR_max\": Maximum likelihood ratio (default)\n        - \"equi_prob\": Equal category probabilities\n        - \"se_min\": Minimum standard error\n        - \"user_defined\": Use provided tau values\n    statistic : str\n        Test statistic:\n        - \"likelihood\": Likelihood ratio test (default)\n        - \"Pearson\": Pearson chi-square\n        - \"Wilcoxon\": Wilcoxon rank-sum test (requires d_prime0=0)\n        - \"Wald\": Wald test\n    alternative : str\n        Alternative hypothesis:\n        - \"difference\" or \"greater\": d-prime &gt; d_prime0 (default)\n        - \"similarity\" or \"less\": d-prime &lt; d_prime0\n        - \"two.sided\": d-prime != d_prime0\n    tau : NDArray | None\n        Boundary parameters (required if method_tau=\"user_defined\").\n    random_state : int | np.random.Generator | None\n        Random state for reproducibility.\n\n    Returns\n    -------\n    DODPowerResult\n        Power analysis results including power estimate and standard error.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid or incompatible.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = dod_power(d_primeA=1.0, sample_size=100, nsim=500)\n    &gt;&gt;&gt; print(f\"Power: {result.power:.3f} (SE: {result.se_power:.3f})\")\n\n    &gt;&gt;&gt; # With specific tau values\n    &gt;&gt;&gt; result = dod_power(\n    ...     d_primeA=1.5,\n    ...     method_tau=\"user_defined\",\n    ...     tau=np.array([0.5, 1.0, 1.5])\n    ... )\n    \"\"\"\n    # Validate inputs\n    if not (isinstance(d_primeA, (int, float)) and d_primeA &gt;= 0 and np.isfinite(d_primeA)):\n        raise ValueError(\"d_primeA must be a finite non-negative number\")\n    if not (isinstance(d_prime0, (int, float)) and d_prime0 &gt;= 0 and np.isfinite(d_prime0)):\n        raise ValueError(\"d_prime0 must be a finite non-negative number\")\n    if nsim &lt;= 0 or not isinstance(nsim, (int, float)):\n        raise ValueError(\"nsim must be a positive number\")\n    if not (0 &lt;= alpha &lt;= 1):\n        raise ValueError(\"alpha must be between 0 and 1\")\n\n    nsim = int(round(nsim))\n\n    # Handle sample size\n    if isinstance(sample_size, int):\n        size = (sample_size, sample_size)\n    else:\n        size = (int(round(sample_size[0])), int(round(sample_size[1])))\n\n    # Map alternative names\n    alt = alternative\n    if alt == \"difference\":\n        alt = \"greater\"\n    elif alt == \"similarity\":\n        alt = \"less\"\n\n    # Check d_primeA, d_prime0 consistency with alternative\n    if alt == \"greater\" and d_primeA &lt; d_prime0:\n        raise ValueError(\n            f\"Need d_primeA &gt;= d_prime0 when alternative is '{alternative}'\"\n        )\n    if alt == \"less\" and d_primeA &gt; d_prime0:\n        raise ValueError(\n            f\"Need d_primeA &lt;= d_prime0 when alternative is '{alternative}'\"\n        )\n\n    # Wilcoxon requires d_prime0 = 0\n    if statistic == \"Wilcoxon\" and not np.isclose(d_prime0, 0.0):\n        raise ValueError(\"Wilcoxon statistic only available with d_prime0=0\")\n\n    # Get tau values\n    if method_tau != \"user_defined\":\n        method_map = {\n            \"equi_prob\": \"equi_prob\",\n            \"LR_max\": \"LR_max\",\n            \"se_min\": \"se_min\",\n        }\n        tau_result = optimal_tau(\n            d_prime=d_primeA,\n            d_prime0=d_prime0,\n            ncat=ncat,\n            method=method_map[method_tau],\n            do_warn=False,\n        )\n        tau_arr = tau_result[\"tau\"]\n    else:\n        if tau is None:\n            raise ValueError(\"tau must be provided when method_tau='user_defined'\")\n        tau_arr = np.asarray(tau, dtype=float)\n\n    # Handle random state\n    if random_state is None:\n        rng = np.random.default_rng()\n    elif isinstance(random_state, int):\n        rng = np.random.default_rng(random_state)\n    else:\n        rng = random_state\n\n    # Run simulations\n    pvals = np.full(nsim, np.nan)\n    ctrl = DODControl(get_vcov=statistic == \"Wald\", get_grad=False, do_warn=False)\n\n    for i in range(nsim):\n        # Simulate data under alternative\n        data = dod_sim(\n            d_prime=d_primeA,\n            sample_size=size,\n            method_tau=\"user_defined\",\n            tau=tau_arr,\n            random_state=rng,\n        )\n        same_sim = data[0, :]\n        diff_sim = data[1, :]\n\n        if statistic == \"Wilcoxon\":\n            # Special case: Wilcoxon test without DOD fitting\n            from scipy.stats import mannwhitneyu\n\n            nlev = data.shape[1]\n            # Expand rating categories by their counts for each pair type\n            diff_pair_ratings = np.repeat(np.arange(1, nlev + 1), diff_sim.astype(int))\n            same_pair_ratings = np.repeat(np.arange(1, nlev + 1), same_sim.astype(int))\n\n            if alt == \"two.sided\":\n                alt_mwu = \"two-sided\"\n            elif alt == \"greater\":\n                alt_mwu = \"greater\"\n            else:\n                alt_mwu = \"less\"\n\n            try:\n                # Test if diff-pair ratings are stochastically greater than same-pair ratings\n                mwu_result = mannwhitneyu(diff_pair_ratings, same_pair_ratings, alternative=alt_mwu)\n                pvals[i] = mwu_result.pvalue\n            except ValueError:\n                pass\n\n        elif statistic == \"Wald\":\n            # Wald test\n            try:\n                fit = dod_fit(same_sim, diff_sim, control=ctrl)\n                if fit.vcov is not None and np.all(np.isfinite(fit.vcov)):\n                    std_err = np.sqrt(fit.vcov[-1, -1])\n                    if np.isfinite(std_err) and std_err &gt; 0:\n                        stat_value = (fit.d_prime - d_prime0) / std_err\n                        p_val = normal_pvalue(stat_value, alt)\n                        if hasattr(p_val, 'item'):\n                            p_val = p_val.item()\n                        pvals[i] = p_val\n            except (ValueError, RuntimeError, np.linalg.LinAlgError):\n                # Optimization or numerical errors - skip this simulation\n                pass\n\n        else:  # likelihood or Pearson\n            try:\n                result = dod(\n                    same_sim, diff_sim,\n                    d_prime0=d_prime0,\n                    alternative=alt,\n                    statistic=statistic,\n                    control=ctrl,\n                )\n                if result.convergence == 0:\n                    # p_value may be array, extract scalar\n                    p_val = result.p_value\n                    if hasattr(p_val, 'item'):\n                        p_val = p_val.item()\n                    pvals[i] = p_val\n            except (ValueError, RuntimeError, np.linalg.LinAlgError):\n                # Optimization or numerical errors - skip this simulation\n                pass\n\n    # Compute power\n    valid_pvals = pvals[~np.isnan(pvals)]\n    n_used = len(valid_pvals)\n\n    if n_used == 0:\n        power = np.nan\n        se_power = np.nan\n    else:\n        power = np.mean(valid_pvals &lt; alpha)\n        if power == 0 or power == 1 or not np.isfinite(power):\n            se_power = np.nan\n        else:\n            se_power = np.sqrt(power * (1 - power) / n_used)\n\n    return DODPowerResult(\n        power=power,\n        se_power=se_power,\n        n_used=n_used,\n        d_primeA=d_primeA,\n        d_prime0=d_prime0,\n        sample_size=size,\n        nsim=nsim,\n        alpha=alpha,\n        statistic=statistic,\n        alternative=alt,\n        tau=tau_arr,\n    )\n</code></pre>"},{"location":"api/models/#senspy.dod.dod_sim","title":"<code>dod_sim(d_prime: float, ncat: int = 4, sample_size: int | tuple[int, int] = 100, method_tau: Literal['equi_prob', 'LR_max', 'se_min', 'user_defined'] = 'equi_prob', tau: NDArray | None = None, d_prime0: float = 0.0, random_state: int | np.random.Generator | None = None) -&gt; NDArray</code>","text":"<p>Simulate DOD data.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime</code> <code>float</code> <p>True d-prime value (must be non-negative).</p> required <code>ncat</code> <code>int</code> <p>Number of response categories. Default is 4.</p> <code>4</code> <code>sample_size</code> <code>int | tuple[int, int]</code> <p>Sample size. If int, same size for both same-pairs and diff-pairs. If tuple, (same_size, diff_size). Default is 100.</p> <code>100</code> <code>method_tau</code> <code>str</code> <p>Method for determining tau values: - \"equi_prob\": Equal category probabilities - \"LR_max\": Maximum likelihood ratio - \"se_min\": Minimum standard error - \"user_defined\": Use provided tau values</p> <code>'equi_prob'</code> <code>tau</code> <code>NDArray | None</code> <p>Boundary parameters (required if method_tau=\"user_defined\").</p> <code>None</code> <code>d_prime0</code> <code>float</code> <p>Null hypothesis d-prime for optimal_tau. Default is 0.</p> <code>0.0</code> <code>random_state</code> <code>int | Generator | None</code> <p>Random state for reproducibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray</code> <p>A 2 x ncat matrix with row 0 = same-pairs, row 1 = diff-pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = dod_sim(d_prime=1.0, sample_size=100)\n&gt;&gt;&gt; print(data)\n</code></pre> Source code in <code>senspy/dod.py</code> <pre><code>def dod_sim(\n    d_prime: float,\n    ncat: int = 4,\n    sample_size: int | tuple[int, int] = 100,\n    method_tau: Literal[\"equi_prob\", \"LR_max\", \"se_min\", \"user_defined\"] = \"equi_prob\",\n    tau: NDArray | None = None,\n    d_prime0: float = 0.0,\n    random_state: int | np.random.Generator | None = None,\n) -&gt; NDArray:\n    \"\"\"Simulate DOD data.\n\n    Parameters\n    ----------\n    d_prime : float\n        True d-prime value (must be non-negative).\n    ncat : int\n        Number of response categories. Default is 4.\n    sample_size : int | tuple[int, int]\n        Sample size. If int, same size for both same-pairs and diff-pairs.\n        If tuple, (same_size, diff_size). Default is 100.\n    method_tau : str\n        Method for determining tau values:\n        - \"equi_prob\": Equal category probabilities\n        - \"LR_max\": Maximum likelihood ratio\n        - \"se_min\": Minimum standard error\n        - \"user_defined\": Use provided tau values\n    tau : NDArray | None\n        Boundary parameters (required if method_tau=\"user_defined\").\n    d_prime0 : float\n        Null hypothesis d-prime for optimal_tau. Default is 0.\n    random_state : int | np.random.Generator | None\n        Random state for reproducibility.\n\n    Returns\n    -------\n    NDArray\n        A 2 x ncat matrix with row 0 = same-pairs, row 1 = diff-pairs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; np.random.seed(42)\n    &gt;&gt;&gt; data = dod_sim(d_prime=1.0, sample_size=100)\n    &gt;&gt;&gt; print(data)\n    \"\"\"\n    if d_prime &lt; 0:\n        raise ValueError(\"d_prime must be non-negative\")\n    if d_prime0 &lt; 0:\n        raise ValueError(\"d_prime0 must be non-negative\")\n\n    # Handle random state\n    if random_state is None:\n        rng = np.random.default_rng()\n    elif isinstance(random_state, int):\n        rng = np.random.default_rng(random_state)\n    else:\n        rng = random_state\n\n    # Get tau\n    if method_tau != \"user_defined\":\n        if tau is not None:\n            import warnings\n\n            warnings.warn(\n                f\"'tau' is ignored when method_tau != 'user_defined' \"\n                f\"(method_tau was '{method_tau}')\"\n            )\n        # Map method names\n        method_map = {\n            \"equi_prob\": \"equi_prob\",\n            \"LR_max\": \"LR_max\",\n            \"se_min\": \"se_min\",\n        }\n        tau_result = optimal_tau(\n            d_prime=d_prime,\n            d_prime0=d_prime0,\n            ncat=ncat,\n            method=method_map[method_tau],\n            do_warn=False,\n        )\n        tau = tau_result[\"tau\"]\n    else:\n        if tau is None:\n            raise ValueError(\"tau must be provided when method_tau='user_defined'\")\n        tau = np.asarray(tau, dtype=float)\n\n    # Handle sample size\n    if isinstance(sample_size, int):\n        size_same = size_diff = sample_size\n    else:\n        size_same, size_diff = sample_size\n\n    # Get probabilities and simulate data\n    prob = par2prob_dod(tau, d_prime)\n\n    same_counts = rng.multinomial(size_same, prob[0, :])\n    diff_counts = rng.multinomial(size_diff, prob[1, :])\n\n    return np.vstack([same_counts, diff_counts])\n</code></pre>"},{"location":"api/models/#senspy.dod.optimal_tau","title":"<code>optimal_tau(d_prime: float, d_prime0: float = 0.0, ncat: int = 3, method: Literal['equi_prob', 'LR_max', 'se_min'] = 'equi_prob', tau_start: NDArray | None = None, equi_tol: float = 0.0001, grad_tol: float = 0.01, do_warn: bool = True) -&gt; dict</code>","text":"<p>Estimate optimal boundary parameters tau.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime</code> <code>float</code> <p>True d-prime value (must be non-negative).</p> required <code>d_prime0</code> <code>float</code> <p>Null hypothesis d-prime value. Default is 0.</p> <code>0.0</code> <code>ncat</code> <code>int</code> <p>Number of response categories. Default is 3.</p> <code>3</code> <code>method</code> <code>str</code> <p>Optimization criterion: - \"equi_prob\": Equal category probabilities (averaged over same/diff) - \"LR_max\": Maximum likelihood ratio statistic - \"se_min\": Minimum standard error of d-prime</p> <code>'equi_prob'</code> <code>tau_start</code> <code>NDArray | None</code> <p>Starting values for tau. Default uses init_tau().</p> <code>None</code> <code>equi_tol</code> <code>float</code> <p>Tolerance for equi_prob method. Default is 1e-4.</p> <code>0.0001</code> <code>grad_tol</code> <code>float</code> <p>Gradient tolerance. Default is 1e-2.</p> <code>0.01</code> <code>do_warn</code> <code>bool</code> <p>Whether to emit warnings. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys: tau, prob, tau_start, gradient, method</p> Source code in <code>senspy/dod.py</code> <pre><code>def optimal_tau(\n    d_prime: float,\n    d_prime0: float = 0.0,\n    ncat: int = 3,\n    method: Literal[\"equi_prob\", \"LR_max\", \"se_min\"] = \"equi_prob\",\n    tau_start: NDArray | None = None,\n    equi_tol: float = 1e-4,\n    grad_tol: float = 1e-2,\n    do_warn: bool = True,\n) -&gt; dict:\n    \"\"\"Estimate optimal boundary parameters tau.\n\n    Parameters\n    ----------\n    d_prime : float\n        True d-prime value (must be non-negative).\n    d_prime0 : float\n        Null hypothesis d-prime value. Default is 0.\n    ncat : int\n        Number of response categories. Default is 3.\n    method : str\n        Optimization criterion:\n        - \"equi_prob\": Equal category probabilities (averaged over same/diff)\n        - \"LR_max\": Maximum likelihood ratio statistic\n        - \"se_min\": Minimum standard error of d-prime\n    tau_start : NDArray | None\n        Starting values for tau. Default uses init_tau().\n    equi_tol : float\n        Tolerance for equi_prob method. Default is 1e-4.\n    grad_tol : float\n        Gradient tolerance. Default is 1e-2.\n    do_warn : bool\n        Whether to emit warnings. Default is True.\n\n    Returns\n    -------\n    dict\n        Dictionary with keys: tau, prob, tau_start, gradient, method\n    \"\"\"\n    if d_prime &lt; 0:\n        raise ValueError(\"d_prime must be non-negative\")\n    if ncat &lt; 2:\n        raise ValueError(\"ncat must be at least 2\")\n\n    ncat = int(round(ncat))\n\n    def tau2tpar(tau):\n        \"\"\"Convert tau to optimization parameterization.\"\"\"\n        return np.concatenate([[tau[0]], np.diff(tau)])\n\n    def tpar2tau(tpar):\n        \"\"\"Convert optimization parameterization to tau.\"\"\"\n        return np.cumsum(tpar)\n\n    # Define objective functions\n    def equi_prob_obj(tpar):\n        \"\"\"Objective for equal category probabilities.\"\"\"\n        tau = tpar2tau(tpar)\n        prob = _par2prob_dod_internal(tau, d_prime)\n        avg_prob = np.sum(prob, axis=0) / 2\n        target = 1.0 / ncat\n        return 1 + np.sum(((avg_prob - target) * 1e3) ** 2)\n\n    def se_min_obj(tpar):\n        \"\"\"Objective for minimum standard error of d-prime.\"\"\"\n        if not np.all(np.isfinite(tpar)):\n            return np.inf\n        tau = tpar2tau(tpar)\n        if np.any(tau &lt;= 0):\n            return np.inf\n\n        # Generate \"data\" from probabilities\n        data = _par2prob_dod_internal(tau, d_prime) * 100\n\n        # Compute Hessian numerically\n        par = np.concatenate([tau, [d_prime]])\n\n        def nll_func(p):\n            return _dod_nll_all(p, data[0, :], data[1, :])\n\n        try:\n            h = _numerical_hessian(nll_func, par)\n            if not np.all(np.isfinite(h)):\n                return np.inf\n            vcov = np.linalg.inv(h)\n            if not np.all(np.isfinite(vcov)):\n                return np.inf\n            return np.sqrt(vcov[-1, -1])\n        except (np.linalg.LinAlgError, ValueError):\n            return np.inf\n\n    def lr_max_obj(tpar):\n        \"\"\"Objective for maximum LR statistic (negative).\"\"\"\n        tau = tpar2tau(tpar)\n        if np.any(tau &lt;= 0):\n            return np.inf\n\n        # Limiting distribution of data given c(tau, d_prime)\n        data = _par2prob_dod_internal(tau, d_prime) * 100\n\n        # Log-lik at d_prime and at d_prime0\n        log_lik = -_dod_nll_internal(tau, d_prime, data[0, :], data[1, :])\n\n        if np.isclose(d_prime0, 0.0):\n            tau0 = _dod_null_tau_internal(data[0, :], data[1, :])\n            log_lik0 = -_dod_nll_internal(tau0, 0.0, data[0, :], data[1, :])\n        else:\n            # Need to fit model at d_prime0\n            fit0 = dod_fit(\n                data[0, :],\n                data[1, :],\n                d_prime=d_prime0,\n                control=DODControl(\n                    test_args=False, do_warn=False, get_grad=False, get_vcov=False\n                ),\n            )\n            log_lik0 = fit0.log_lik\n\n        return -(log_lik - log_lik0)  # Negative LR/2 statistic\n\n    # Select objective function\n    obj_funcs = {\n        \"equi_prob\": equi_prob_obj,\n        \"LR_max\": lr_max_obj,\n        \"se_min\": se_min_obj,\n    }\n    objfun = obj_funcs[method]\n\n    # Get starting values\n    if tau_start is not None:\n        tau_start = np.asarray(tau_start)\n        if len(tau_start) != ncat - 1:\n            raise ValueError(f\"tau_start must have length {ncat - 1}\")\n        if np.any(tau_start &lt;= 0):\n            raise ValueError(\"tau_start values must be positive\")\n        if len(tau_start) &gt; 1 and np.any(np.diff(tau_start) &lt;= 0):\n            raise ValueError(\"tau_start must be strictly increasing\")\n        start = tau2tpar(tau_start)\n    else:\n        start = _init_tau(ncat)\n\n    # Optimize\n    result = optimize.minimize(\n        objfun,\n        start,\n        method=\"L-BFGS-B\",\n        bounds=[(1e-4, None)] * len(start),\n    )\n\n    tau = tpar2tau(result.x)\n    prob = _par2prob_dod_internal(tau, d_prime)\n\n    # Check convergence for equi_prob method\n    if method == \"equi_prob\":\n        avg_prob = np.sum(prob, axis=0) / 2\n        diffs = np.abs(avg_prob - 1.0 / ncat)\n        if np.max(diffs) &gt; equi_tol and do_warn:\n            import warnings\n\n            warnings.warn(\n                f\"Estimation of tau failed with max(diffs) = {np.max(diffs):.2g} \"\n                f\"(equi_tol = {equi_tol:.2g})\"\n            )\n\n    # Compute gradient\n    grad = _numerical_gradient(objfun, result.x)\n    if np.max(np.abs(grad)) &gt; grad_tol and do_warn:\n        import warnings\n\n        warnings.warn(\n            f\"Estimation of tau failed with max(gradient) = {np.max(np.abs(grad)):.2g} \"\n            f\"(grad_tol = {grad_tol:.2g})\"\n        )\n\n    return {\n        \"tau\": tau,\n        \"prob\": prob,\n        \"tau_start\": np.cumsum(start),\n        \"gradient\": grad,\n        \"method\": method,\n    }\n</code></pre>"},{"location":"api/models/#senspy.dod.par2prob_dod","title":"<code>par2prob_dod(tau: NDArray, d_prime: float) -&gt; NDArray</code>","text":"<p>Convert DOD parameters to probability matrix.</p> <p>Computes the probability of each response category for same-pairs and different-pairs given the boundary parameters tau and d-prime.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>NDArray</code> <p>Boundary parameters (must be positive and increasing).</p> required <code>d_prime</code> <code>float</code> <p>Discriminability parameter (must be non-negative).</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>A 2 x (len(tau)+1) matrix where: - Row 0: probabilities for same-pairs - Row 1: probabilities for different-pairs</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid.</p> Source code in <code>senspy/dod.py</code> <pre><code>def par2prob_dod(tau: NDArray, d_prime: float) -&gt; NDArray:\n    \"\"\"Convert DOD parameters to probability matrix.\n\n    Computes the probability of each response category for same-pairs\n    and different-pairs given the boundary parameters tau and d-prime.\n\n    Parameters\n    ----------\n    tau : NDArray\n        Boundary parameters (must be positive and increasing).\n    d_prime : float\n        Discriminability parameter (must be non-negative).\n\n    Returns\n    -------\n    NDArray\n        A 2 x (len(tau)+1) matrix where:\n        - Row 0: probabilities for same-pairs\n        - Row 1: probabilities for different-pairs\n\n    Raises\n    ------\n    ValueError\n        If parameters are invalid.\n    \"\"\"\n    tau = np.asarray(tau, dtype=float)\n    if d_prime &lt; 0:\n        raise ValueError(\"d_prime must be non-negative\")\n    if len(tau) &lt; 1:\n        raise ValueError(\"tau must have at least one element\")\n    if np.any(tau &lt;= 0):\n        raise ValueError(\"tau values must be positive\")\n    if len(tau) &gt; 1 and np.any(np.diff(tau) &lt;= 0):\n        raise ValueError(\"tau values must be strictly increasing\")\n\n    return _par2prob_dod_internal(tau, d_prime)\n</code></pre>"},{"location":"api/models/#dod_fit","title":"dod_fit","text":""},{"location":"api/models/#senspy.dod_fit","title":"<code>dod_fit(same: ArrayLike, diff: ArrayLike, tau: NDArray | None = None, d_prime: float | None = None, control: DODControl | None = None) -&gt; DODFitResult</code>","text":"<p>Fit DOD model (low-level function).</p> <p>This is the lower-level fitting function. Use <code>dod()</code> for the full analysis including hypothesis testing.</p> <p>Parameters:</p> Name Type Description Default <code>same</code> <code>ArrayLike</code> <p>Counts for same-pairs in each response category.</p> required <code>diff</code> <code>ArrayLike</code> <p>Counts for different-pairs in each response category.</p> required <code>tau</code> <code>NDArray | None</code> <p>Fixed boundary parameters. If None, estimated from data.</p> <code>None</code> <code>d_prime</code> <code>float | None</code> <p>Fixed d-prime value. If None, estimated from data.</p> <code>None</code> <code>control</code> <code>DODControl | None</code> <p>Control parameters. If None, uses defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>DODFitResult</code> <p>Fitting results including estimates and diagnostics.</p> Source code in <code>senspy/dod.py</code> <pre><code>def dod_fit(\n    same: ArrayLike,\n    diff: ArrayLike,\n    tau: NDArray | None = None,\n    d_prime: float | None = None,\n    control: DODControl | None = None,\n) -&gt; DODFitResult:\n    \"\"\"Fit DOD model (low-level function).\n\n    This is the lower-level fitting function. Use `dod()` for the full analysis\n    including hypothesis testing.\n\n    Parameters\n    ----------\n    same : ArrayLike\n        Counts for same-pairs in each response category.\n    diff : ArrayLike\n        Counts for different-pairs in each response category.\n    tau : NDArray | None\n        Fixed boundary parameters. If None, estimated from data.\n    d_prime : float | None\n        Fixed d-prime value. If None, estimated from data.\n    control : DODControl | None\n        Control parameters. If None, uses defaults.\n\n    Returns\n    -------\n    DODFitResult\n        Fitting results including estimates and diagnostics.\n    \"\"\"\n    if control is None:\n        control = DODControl()\n\n    # Validate data\n    if control.test_args:\n        same, diff = _validate_dod_data(same, diff, control.integer_tol)\n    else:\n        same = np.asarray(same, dtype=float)\n        diff = np.asarray(diff, dtype=float)\n\n    nlev = len(same)\n    data = np.vstack([same, diff])\n\n    # Initialize result variables\n    vcov = None\n    gradient = None\n    hessian = None\n    convergence = 0\n\n    # CASE 1: d_prime = 0 and tau = None\n    if tau is None and d_prime is not None and np.isclose(d_prime, 0.0):\n        est_tau = _dod_null_tau_internal(same, diff)\n        est_d_prime = d_prime\n\n    # CASE 2: Need to optimize tau, d_prime, or both\n    elif tau is None or d_prime is None:\n        if d_prime is None and tau is None:\n            # Estimate both tau and d_prime\n            start = np.concatenate([np.cumsum(_init_tau(nlev)), [1.0]])\n            par_names = [f\"tau{i+1}\" for i in range(nlev - 1)] + [\"d_prime\"]\n            npar = len(start)\n\n            def objfun(par):\n                return _dod_nll_internal(par[: npar - 1], par[npar - 1], same, diff)\n\n            case = \"both\"\n\n        elif tau is None and d_prime is not None:\n            # Estimate tau only\n            start = np.cumsum(_init_tau(nlev))\n            par_names = [f\"tau{i+1}\" for i in range(nlev - 1)]\n\n            def objfun(par):\n                return _dod_nll_internal(par, d_prime, same, diff)\n\n            case = \"tau\"\n\n        else:  # tau is not None and d_prime is None\n            # Estimate d_prime only\n            start = np.array([1.0])\n            par_names = [\"d_prime\"]\n\n            def objfun(par):\n                return _dod_nll_internal(tau, par[0], same, diff)\n\n            case = \"d_prime\"\n\n        # Optimization - use small positive lower bound to avoid numerical issues\n        result = optimize.minimize(\n            objfun,\n            start,\n            method=\"L-BFGS-B\",\n            bounds=[(1e-6, None)] * len(start),\n            options=control.opt_options,\n        )\n\n        par = result.x\n        convergence = 0 if result.success else result.status\n\n        # Compute gradient and Hessian if requested\n        if np.all(par &gt; 1e-2) and control.get_grad:\n            gradient = _numerical_gradient(objfun, par)\n\n            if not np.all(np.isfinite(gradient)):\n                if control.do_warn:\n                    import warnings\n\n                    warnings.warn(\"Cannot assess convergence: non-finite gradient\")\n            else:\n                if np.max(np.abs(gradient)) &gt; control.grad_tol and control.do_warn:\n                    import warnings\n\n                    warnings.warn(\n                        f\"Estimation failed with max(gradient) = {np.max(np.abs(gradient)):.2g} \"\n                        f\"(grad_tol = {control.grad_tol:.2g})\"\n                    )\n\n                if control.get_vcov:\n                    try:\n                        hessian = _numerical_hessian(objfun, par)\n                        if not np.all(np.isfinite(hessian)):\n                            if control.do_warn:\n                                import warnings\n\n                                warnings.warn(\"unable to compute Hessian\")\n                        else:\n                            # Check positive definiteness via Cholesky\n                            try:\n                                np.linalg.cholesky(hessian)\n                                vcov = np.linalg.inv(hessian)\n                            except np.linalg.LinAlgError:\n                                if control.do_warn:\n                                    import warnings\n\n                                    warnings.warn(\n                                        \"Model is ill-defined and may not have converged\"\n                                    )\n                    except Exception:\n                        if control.do_warn:\n                            import warnings\n\n                            warnings.warn(\"unable to compute Hessian\")\n\n        # Extract parameters based on case\n        if case == \"both\":\n            est_tau = par[: npar - 1]\n            est_d_prime = par[npar - 1]\n        elif case == \"tau\":\n            est_tau = par\n            est_d_prime = d_prime\n        else:  # case == \"d_prime\"\n            est_tau = tau\n            est_d_prime = par[0]\n\n    # CASE 3: Both tau and d_prime provided - just evaluate likelihood\n    else:\n        est_tau = np.asarray(tau, dtype=float)\n        est_d_prime = d_prime\n\n    # Compute log-likelihood\n    log_lik = -_dod_nll_internal(est_tau, est_d_prime, same, diff)\n\n    # Build coefficients vector\n    coefficients = np.concatenate([est_tau, [est_d_prime]])\n\n    return DODFitResult(\n        d_prime=est_d_prime,\n        tau=est_tau,\n        log_lik=log_lik,\n        coefficients=coefficients,\n        vcov=vcov,\n        gradient=gradient,\n        hessian=hessian,\n        data=data,\n        convergence=convergence,\n    )\n</code></pre>"},{"location":"api/models/#dod_sim","title":"dod_sim","text":""},{"location":"api/models/#senspy.dod_sim","title":"<code>dod_sim(d_prime: float, ncat: int = 4, sample_size: int | tuple[int, int] = 100, method_tau: Literal['equi_prob', 'LR_max', 'se_min', 'user_defined'] = 'equi_prob', tau: NDArray | None = None, d_prime0: float = 0.0, random_state: int | np.random.Generator | None = None) -&gt; NDArray</code>","text":"<p>Simulate DOD data.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime</code> <code>float</code> <p>True d-prime value (must be non-negative).</p> required <code>ncat</code> <code>int</code> <p>Number of response categories. Default is 4.</p> <code>4</code> <code>sample_size</code> <code>int | tuple[int, int]</code> <p>Sample size. If int, same size for both same-pairs and diff-pairs. If tuple, (same_size, diff_size). Default is 100.</p> <code>100</code> <code>method_tau</code> <code>str</code> <p>Method for determining tau values: - \"equi_prob\": Equal category probabilities - \"LR_max\": Maximum likelihood ratio - \"se_min\": Minimum standard error - \"user_defined\": Use provided tau values</p> <code>'equi_prob'</code> <code>tau</code> <code>NDArray | None</code> <p>Boundary parameters (required if method_tau=\"user_defined\").</p> <code>None</code> <code>d_prime0</code> <code>float</code> <p>Null hypothesis d-prime for optimal_tau. Default is 0.</p> <code>0.0</code> <code>random_state</code> <code>int | Generator | None</code> <p>Random state for reproducibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray</code> <p>A 2 x ncat matrix with row 0 = same-pairs, row 1 = diff-pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = dod_sim(d_prime=1.0, sample_size=100)\n&gt;&gt;&gt; print(data)\n</code></pre> Source code in <code>senspy/dod.py</code> <pre><code>def dod_sim(\n    d_prime: float,\n    ncat: int = 4,\n    sample_size: int | tuple[int, int] = 100,\n    method_tau: Literal[\"equi_prob\", \"LR_max\", \"se_min\", \"user_defined\"] = \"equi_prob\",\n    tau: NDArray | None = None,\n    d_prime0: float = 0.0,\n    random_state: int | np.random.Generator | None = None,\n) -&gt; NDArray:\n    \"\"\"Simulate DOD data.\n\n    Parameters\n    ----------\n    d_prime : float\n        True d-prime value (must be non-negative).\n    ncat : int\n        Number of response categories. Default is 4.\n    sample_size : int | tuple[int, int]\n        Sample size. If int, same size for both same-pairs and diff-pairs.\n        If tuple, (same_size, diff_size). Default is 100.\n    method_tau : str\n        Method for determining tau values:\n        - \"equi_prob\": Equal category probabilities\n        - \"LR_max\": Maximum likelihood ratio\n        - \"se_min\": Minimum standard error\n        - \"user_defined\": Use provided tau values\n    tau : NDArray | None\n        Boundary parameters (required if method_tau=\"user_defined\").\n    d_prime0 : float\n        Null hypothesis d-prime for optimal_tau. Default is 0.\n    random_state : int | np.random.Generator | None\n        Random state for reproducibility.\n\n    Returns\n    -------\n    NDArray\n        A 2 x ncat matrix with row 0 = same-pairs, row 1 = diff-pairs.\n\n    Examples\n    --------\n    &gt;&gt;&gt; np.random.seed(42)\n    &gt;&gt;&gt; data = dod_sim(d_prime=1.0, sample_size=100)\n    &gt;&gt;&gt; print(data)\n    \"\"\"\n    if d_prime &lt; 0:\n        raise ValueError(\"d_prime must be non-negative\")\n    if d_prime0 &lt; 0:\n        raise ValueError(\"d_prime0 must be non-negative\")\n\n    # Handle random state\n    if random_state is None:\n        rng = np.random.default_rng()\n    elif isinstance(random_state, int):\n        rng = np.random.default_rng(random_state)\n    else:\n        rng = random_state\n\n    # Get tau\n    if method_tau != \"user_defined\":\n        if tau is not None:\n            import warnings\n\n            warnings.warn(\n                f\"'tau' is ignored when method_tau != 'user_defined' \"\n                f\"(method_tau was '{method_tau}')\"\n            )\n        # Map method names\n        method_map = {\n            \"equi_prob\": \"equi_prob\",\n            \"LR_max\": \"LR_max\",\n            \"se_min\": \"se_min\",\n        }\n        tau_result = optimal_tau(\n            d_prime=d_prime,\n            d_prime0=d_prime0,\n            ncat=ncat,\n            method=method_map[method_tau],\n            do_warn=False,\n        )\n        tau = tau_result[\"tau\"]\n    else:\n        if tau is None:\n            raise ValueError(\"tau must be provided when method_tau='user_defined'\")\n        tau = np.asarray(tau, dtype=float)\n\n    # Handle sample size\n    if isinstance(sample_size, int):\n        size_same = size_diff = sample_size\n    else:\n        size_same, size_diff = sample_size\n\n    # Get probabilities and simulate data\n    prob = par2prob_dod(tau, d_prime)\n\n    same_counts = rng.multinomial(size_same, prob[0, :])\n    diff_counts = rng.multinomial(size_diff, prob[1, :])\n\n    return np.vstack([same_counts, diff_counts])\n</code></pre>"},{"location":"api/models/#dodresult","title":"DODResult","text":""},{"location":"api/models/#senspy.DODResult","title":"<code>DODResult(d_prime: float, tau: NDArray, log_lik: float, se_d_prime: float, se_tau: NDArray, conf_int: tuple[float, float], conf_level: float, conf_method: str, stat_value: float, p_value: float, statistic: str, alternative: str, d_prime0: float, data: NDArray, vcov: NDArray | None, coefficients: NDArray, convergence: int)</code>  <code>dataclass</code>","text":"<p>Result from dod function.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated d-prime (discriminability).</p> <code>tau</code> <code>NDArray</code> <p>Estimated boundary parameters.</p> <code>log_lik</code> <code>float</code> <p>Log-likelihood at the estimates.</p> <code>se_d_prime</code> <code>float</code> <p>Standard error of d-prime estimate.</p> <code>se_tau</code> <code>NDArray</code> <p>Standard errors of tau estimates.</p> <code>conf_int</code> <code>tuple[float, float]</code> <p>Confidence interval for d-prime.</p> <code>conf_level</code> <code>float</code> <p>Confidence level used for interval.</p> <code>conf_method</code> <code>str</code> <p>Method used for confidence interval (\"profile likelihood\" or \"Wald\").</p> <code>stat_value</code> <code>float</code> <p>Value of the test statistic.</p> <code>p_value</code> <code>float</code> <p>P-value for the hypothesis test.</p> <code>statistic</code> <code>str</code> <p>Type of test statistic used.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis (\"greater\", \"less\", \"two.sided\").</p> <code>d_prime0</code> <code>float</code> <p>Null hypothesis value for d-prime.</p> <code>data</code> <code>NDArray</code> <p>The 2 x k data matrix (same-pairs, diff-pairs).</p> <code>vcov</code> <code>NDArray | None</code> <p>Variance-covariance matrix of coefficients.</p> <code>coefficients</code> <code>NDArray</code> <p>Combined coefficient vector [tau1, tau2, ..., d_prime].</p> <code>convergence</code> <code>int</code> <p>Optimizer convergence code (0 = success).</p>"},{"location":"api/models/#dodfitresult","title":"DODFitResult","text":""},{"location":"api/models/#senspy.DODFitResult","title":"<code>DODFitResult(d_prime: float, tau: NDArray, log_lik: float, coefficients: NDArray, vcov: NDArray | None, gradient: NDArray | None, hessian: NDArray | None, data: NDArray, convergence: int)</code>  <code>dataclass</code>","text":"<p>Result from dod_fit function.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated d-prime (discriminability).</p> <code>tau</code> <code>NDArray</code> <p>Estimated boundary parameters.</p> <code>log_lik</code> <code>float</code> <p>Log-likelihood at the estimates.</p> <code>coefficients</code> <code>NDArray</code> <p>Combined coefficient vector [tau1, tau2, ..., d_prime].</p> <code>vcov</code> <code>NDArray | None</code> <p>Variance-covariance matrix of coefficients.</p> <code>gradient</code> <code>NDArray | None</code> <p>Gradient at convergence.</p> <code>hessian</code> <code>NDArray | None</code> <p>Hessian matrix at convergence.</p> <code>data</code> <code>NDArray</code> <p>The 2 x k data matrix (same-pairs, diff-pairs).</p> <code>convergence</code> <code>int</code> <p>Optimizer convergence code (0 = success).</p>"},{"location":"api/models/#dodcontrol","title":"DODControl","text":""},{"location":"api/models/#senspy.DODControl","title":"<code>DODControl(grad_tol: float = 0.0001, integer_tol: float = 1e-08, get_vcov: bool = True, get_grad: bool = True, test_args: bool = True, do_warn: bool = True, opt_options: dict = dict())</code>  <code>dataclass</code>","text":"<p>Control parameters for DOD model fitting.</p> <p>Attributes:</p> Name Type Description <code>grad_tol</code> <code>float</code> <p>Tolerance for gradient convergence check. Default is 1e-4.</p> <code>integer_tol</code> <code>float</code> <p>Tolerance for checking if counts are integers. Default is 1e-8.</p> <code>get_vcov</code> <code>bool</code> <p>Whether to compute variance-covariance matrix. Default is True.</p> <code>get_grad</code> <code>bool</code> <p>Whether to compute and check gradient. Default is True.</p> <code>test_args</code> <code>bool</code> <p>Whether to test argument validity. Default is True.</p> <code>do_warn</code> <code>bool</code> <p>Whether to emit warnings. Default is True.</p> <code>opt_options</code> <code>dict</code> <p>Additional options passed to scipy.optimize.minimize. Default is empty dict.</p>"},{"location":"api/plotting/","title":"Plotting Functions","text":"<p>All plotting functions return Plotly <code>Figure</code> objects that can be displayed in Jupyter notebooks or exported to various formats.</p>"},{"location":"api/plotting/#roc-curves","title":"ROC Curves","text":""},{"location":"api/plotting/#plot_roc","title":"plot_roc","text":""},{"location":"api/plotting/#senspy.plot_roc","title":"<code>plot_roc(roc_result: ROCResult | None = None, d_prime: float | None = None, se_d: float | None = None, scale: float = 1.0, n_points: int = 1000, ci_alpha: float = 0.05, title: str = 'ROC Curve', show_diagonal: bool = True, show_auc: bool = True, width: int = 600, height: int = 500) -&gt; go.Figure</code>","text":"<p>Plot ROC curve with optional confidence bands.</p> <p>Parameters:</p> Name Type Description Default <code>roc_result</code> <code>ROCResult</code> <p>Pre-computed ROC result from <code>roc()</code> function. If provided, d_prime and se_d are ignored.</p> <code>None</code> <code>d_prime</code> <code>float</code> <p>D-prime value to compute ROC curve. Required if roc_result not provided.</p> <code>None</code> <code>se_d</code> <code>float</code> <p>Standard error of d-prime for confidence bands.</p> <code>None</code> <code>scale</code> <code>float</code> <p>Scale parameter for unequal variance model.</p> <code>1.0</code> <code>n_points</code> <code>int</code> <p>Number of points on the curve.</p> <code>1000</code> <code>ci_alpha</code> <code>float</code> <p>Alpha level for confidence interval.</p> <code>0.05</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>\"ROC Curve\"</code> <code>show_diagonal</code> <code>bool</code> <p>Show the chance diagonal line.</p> <code>True</code> <code>show_auc</code> <code>bool</code> <p>Display AUC value in the plot.</p> <code>True</code> <code>width</code> <code>int</code> <p>Figure width in pixels.</p> <code>600</code> <code>height</code> <code>int</code> <p>Figure height in pixels.</p> <code>500</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy.plotting import plot_roc\n&gt;&gt;&gt; fig = plot_roc(d_prime=1.5, se_d=0.2)\n&gt;&gt;&gt; fig.show()\n</code></pre> Source code in <code>senspy/plotting.py</code> <pre><code>def plot_roc(\n    roc_result: ROCResult | None = None,\n    d_prime: float | None = None,\n    se_d: float | None = None,\n    scale: float = 1.0,\n    n_points: int = 1000,\n    ci_alpha: float = 0.05,\n    title: str = \"ROC Curve\",\n    show_diagonal: bool = True,\n    show_auc: bool = True,\n    width: int = 600,\n    height: int = 500,\n) -&gt; go.Figure:\n    \"\"\"Plot ROC curve with optional confidence bands.\n\n    Parameters\n    ----------\n    roc_result : ROCResult, optional\n        Pre-computed ROC result from `roc()` function. If provided,\n        d_prime and se_d are ignored.\n    d_prime : float, optional\n        D-prime value to compute ROC curve. Required if roc_result not provided.\n    se_d : float, optional\n        Standard error of d-prime for confidence bands.\n    scale : float, default 1.0\n        Scale parameter for unequal variance model.\n    n_points : int, default 1000\n        Number of points on the curve.\n    ci_alpha : float, default 0.05\n        Alpha level for confidence interval.\n    title : str, default \"ROC Curve\"\n        Plot title.\n    show_diagonal : bool, default True\n        Show the chance diagonal line.\n    show_auc : bool, default True\n        Display AUC value in the plot.\n    width : int, default 600\n        Figure width in pixels.\n    height : int, default 500\n        Figure height in pixels.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from senspy.plotting import plot_roc\n    &gt;&gt;&gt; fig = plot_roc(d_prime=1.5, se_d=0.2)\n    &gt;&gt;&gt; fig.show()\n    \"\"\"\n    from .roc import auc as compute_auc\n    from .roc import roc as compute_roc\n\n    # Get or compute ROC data\n    if roc_result is not None:\n        fpr = roc_result.fpr\n        tpr = roc_result.tpr\n        lower = roc_result.lower\n        upper = roc_result.upper\n        d = roc_result.d_prime\n        scale = roc_result.scale\n    elif d_prime is not None:\n        result = compute_roc(\n            d=d_prime,\n            se_d=se_d,\n            scale=scale,\n            n_points=n_points,\n            ci_alpha=ci_alpha,\n        )\n        fpr = result.fpr\n        tpr = result.tpr\n        lower = result.lower\n        upper = result.upper\n        d = d_prime\n    else:\n        raise ValueError(\"Either roc_result or d_prime must be provided\")\n\n    fig = go.Figure()\n\n    # Add confidence band if available\n    if lower is not None and upper is not None:\n        fig.add_trace(\n            go.Scatter(\n                x=np.concatenate([fpr, fpr[::-1]]),\n                y=np.concatenate([upper, lower[::-1]]),\n                fill=\"toself\",\n                fillcolor=COLORS[\"ci_fill\"],\n                line=dict(color=\"rgba(255,255,255,0)\"),\n                hoverinfo=\"skip\",\n                name=f\"{int((1 - ci_alpha) * 100)}% CI\",\n                showlegend=True,\n            )\n        )\n\n    # Add ROC curve\n    fig.add_trace(\n        go.Scatter(\n            x=fpr,\n            y=tpr,\n            mode=\"lines\",\n            name=f\"ROC (d' = {d:.2f})\",\n            line=dict(color=COLORS[\"primary\"], width=2),\n            hovertemplate=\"FPR: %{x:.3f}&lt;br&gt;TPR: %{y:.3f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    # Add diagonal reference line\n    if show_diagonal:\n        fig.add_trace(\n            go.Scatter(\n                x=[0, 1],\n                y=[0, 1],\n                mode=\"lines\",\n                name=\"Chance\",\n                line=dict(color=COLORS[\"reference\"], width=1, dash=\"dash\"),\n                hoverinfo=\"skip\",\n            )\n        )\n\n    # Compute and display AUC\n    if show_auc:\n        auc_result = compute_auc(d=d, se_d=se_d, scale=scale, ci_alpha=ci_alpha)\n        auc_text = f\"AUC = {auc_result.value:.3f}\"\n        if auc_result.lower is not None:\n            auc_text += f\"&lt;br&gt;[{auc_result.lower:.3f}, {auc_result.upper:.3f}]\"\n\n        fig.add_annotation(\n            x=0.95,\n            y=0.05,\n            xref=\"paper\",\n            yref=\"paper\",\n            text=auc_text,\n            showarrow=False,\n            font=dict(size=12),\n            align=\"right\",\n            bgcolor=\"rgba(255, 255, 255, 0.8)\",\n            bordercolor=COLORS[\"primary\"],\n            borderwidth=1,\n            borderpad=4,\n        )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"False Positive Rate\",\n        yaxis_title=\"True Positive Rate\",\n        width=width,\n        height=height,\n        xaxis=dict(range=[0, 1], constrain=\"domain\"),\n        yaxis=dict(range=[0, 1], scaleanchor=\"x\", scaleratio=1),\n        legend=dict(x=0.02, y=0.98, bgcolor=\"rgba(255,255,255,0.8)\"),\n        hovermode=\"closest\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/plotting/#psychometric-functions","title":"Psychometric Functions","text":""},{"location":"api/plotting/#plot_psychometric","title":"plot_psychometric","text":""},{"location":"api/plotting/#senspy.plot_psychometric","title":"<code>plot_psychometric(method: str = 'triangle', d_prime_range: tuple[float, float] = (0, 4), n_points: int = 200, show_guessing: bool = True, title: str | None = None, width: int = 700, height: int = 500) -&gt; go.Figure</code>","text":"<p>Plot psychometric function for a discrimination protocol.</p> <p>Shows the relationship between d-prime and proportion correct (Pc) for a given sensory discrimination protocol.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Protocol name: \"triangle\", \"duotrio\", \"twoAFC\", \"threeAFC\", \"tetrad\", \"hexad\", \"twofive\", \"twofiveF\".</p> <code>\"triangle\"</code> <code>d_prime_range</code> <code>tuple</code> <p>Range of d-prime values to plot.</p> <code>(0, 4)</code> <code>n_points</code> <code>int</code> <p>Number of points on the curve.</p> <code>200</code> <code>show_guessing</code> <code>bool</code> <p>Show horizontal line at guessing probability.</p> <code>True</code> <code>title</code> <code>str</code> <p>Plot title. If None, auto-generated from method.</p> <code>None</code> <code>width</code> <code>int</code> <p>Figure width in pixels.</p> <code>700</code> <code>height</code> <code>int</code> <p>Figure height in pixels.</p> <code>500</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy.plotting import plot_psychometric\n&gt;&gt;&gt; fig = plot_psychometric(method=\"triangle\")\n&gt;&gt;&gt; fig.show()\n</code></pre> Source code in <code>senspy/plotting.py</code> <pre><code>def plot_psychometric(\n    method: str = \"triangle\",\n    d_prime_range: tuple[float, float] = (0, 4),\n    n_points: int = 200,\n    show_guessing: bool = True,\n    title: str | None = None,\n    width: int = 700,\n    height: int = 500,\n) -&gt; go.Figure:\n    \"\"\"Plot psychometric function for a discrimination protocol.\n\n    Shows the relationship between d-prime and proportion correct (Pc)\n    for a given sensory discrimination protocol.\n\n    Parameters\n    ----------\n    method : str, default \"triangle\"\n        Protocol name: \"triangle\", \"duotrio\", \"twoAFC\", \"threeAFC\",\n        \"tetrad\", \"hexad\", \"twofive\", \"twofiveF\".\n    d_prime_range : tuple, default (0, 4)\n        Range of d-prime values to plot.\n    n_points : int, default 200\n        Number of points on the curve.\n    show_guessing : bool, default True\n        Show horizontal line at guessing probability.\n    title : str, optional\n        Plot title. If None, auto-generated from method.\n    width : int, default 700\n        Figure width in pixels.\n    height : int, default 500\n        Figure height in pixels.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from senspy.plotting import plot_psychometric\n    &gt;&gt;&gt; fig = plot_psychometric(method=\"triangle\")\n    &gt;&gt;&gt; fig.show()\n    \"\"\"\n    from .links import get_link\n\n    link = get_link(method)\n    d_values = np.linspace(d_prime_range[0], d_prime_range[1], n_points)\n    pc_values = link.linkinv(d_values)\n\n    if title is None:\n        title = f\"Psychometric Function: {method.capitalize()} Protocol\"\n\n    fig = go.Figure()\n\n    # Add guessing line\n    if show_guessing:\n        fig.add_hline(\n            y=link.p_guess,\n            line_dash=\"dash\",\n            line_color=COLORS[\"reference\"],\n            annotation_text=f\"Guessing (Pg = {link.p_guess:.3f})\",\n            annotation_position=\"bottom right\",\n        )\n\n    # Add psychometric curve\n    fig.add_trace(\n        go.Scatter(\n            x=d_values,\n            y=pc_values,\n            mode=\"lines\",\n            name=method.capitalize(),\n            line=dict(color=COLORS[\"primary\"], width=2.5),\n            hovertemplate=\"d' = %{x:.2f}&lt;br&gt;Pc = %{y:.3f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"d-prime (d')\",\n        yaxis_title=\"Proportion Correct (Pc)\",\n        width=width,\n        height=height,\n        xaxis=dict(range=d_prime_range),\n        yaxis=dict(range=[0, 1]),\n        showlegend=False,\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/plotting/#plot_psychometric_comparison","title":"plot_psychometric_comparison","text":""},{"location":"api/plotting/#senspy.plot_psychometric_comparison","title":"<code>plot_psychometric_comparison(methods: list[str] | None = None, d_prime_range: tuple[float, float] = (0, 4), n_points: int = 200, title: str = 'Psychometric Functions Comparison', width: int = 800, height: int = 500) -&gt; go.Figure</code>","text":"<p>Compare psychometric functions across multiple protocols.</p> <p>Parameters:</p> Name Type Description Default <code>methods</code> <code>list of str</code> <p>Protocols to compare. Default is common protocols.</p> <code>None</code> <code>d_prime_range</code> <code>tuple</code> <p>Range of d-prime values.</p> <code>(0, 4)</code> <code>n_points</code> <code>int</code> <p>Number of points per curve.</p> <code>200</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>\"Psychometric Functions Comparison\"</code> <code>width</code> <code>int</code> <p>Figure width in pixels.</p> <code>800</code> <code>height</code> <code>int</code> <p>Figure height in pixels.</p> <code>500</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object.</p> Source code in <code>senspy/plotting.py</code> <pre><code>def plot_psychometric_comparison(\n    methods: list[str] | None = None,\n    d_prime_range: tuple[float, float] = (0, 4),\n    n_points: int = 200,\n    title: str = \"Psychometric Functions Comparison\",\n    width: int = 800,\n    height: int = 500,\n) -&gt; go.Figure:\n    \"\"\"Compare psychometric functions across multiple protocols.\n\n    Parameters\n    ----------\n    methods : list of str, optional\n        Protocols to compare. Default is common protocols.\n    d_prime_range : tuple, default (0, 4)\n        Range of d-prime values.\n    n_points : int, default 200\n        Number of points per curve.\n    title : str, default \"Psychometric Functions Comparison\"\n        Plot title.\n    width : int, default 800\n        Figure width in pixels.\n    height : int, default 500\n        Figure height in pixels.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure object.\n    \"\"\"\n    from .links import get_link\n\n    if methods is None:\n        methods = [\"triangle\", \"duotrio\", \"twoAFC\", \"threeAFC\", \"tetrad\"]\n\n    # Plotly's default color sequence\n    colors = [\n        \"#1f77b4\",\n        \"#ff7f0e\",\n        \"#2ca02c\",\n        \"#d62728\",\n        \"#9467bd\",\n        \"#8c564b\",\n        \"#e377c2\",\n        \"#7f7f7f\",\n    ]\n\n    d_values = np.linspace(d_prime_range[0], d_prime_range[1], n_points)\n\n    fig = go.Figure()\n\n    for i, method in enumerate(methods):\n        link = get_link(method)\n        pc_values = link.linkinv(d_values)\n        color = colors[i % len(colors)]\n\n        fig.add_trace(\n            go.Scatter(\n                x=d_values,\n                y=pc_values,\n                mode=\"lines\",\n                name=f\"{method} (Pg={link.p_guess:.2f})\",\n                line=dict(color=color, width=2),\n                hovertemplate=f\"{method}&lt;br&gt;d' = %{{x:.2f}}&lt;br&gt;Pc = %{{y:.3f}}&lt;extra&gt;&lt;/extra&gt;\",\n            )\n        )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"d-prime (d')\",\n        yaxis_title=\"Proportion Correct (Pc)\",\n        width=width,\n        height=height,\n        xaxis=dict(range=d_prime_range),\n        yaxis=dict(range=[0, 1]),\n        legend=dict(x=0.02, y=0.98, bgcolor=\"rgba(255,255,255,0.8)\"),\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/plotting/#signal-detection","title":"Signal Detection","text":""},{"location":"api/plotting/#plot_sdt_distributions","title":"plot_sdt_distributions","text":""},{"location":"api/plotting/#senspy.plot_sdt_distributions","title":"<code>plot_sdt_distributions(d_prime: float = 1.5, show_criterion: bool = True, criterion: float | None = None, title: str | None = None, width: int = 700, height: int = 400) -&gt; go.Figure</code>","text":"<p>Plot Signal Detection Theory distributions.</p> <p>Shows the noise and signal distributions with optional criterion line and shaded hit/false alarm regions.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime</code> <code>float</code> <p>Separation between distributions (d-prime).</p> <code>1.5</code> <code>show_criterion</code> <code>bool</code> <p>Show decision criterion line.</p> <code>True</code> <code>criterion</code> <code>float</code> <p>Criterion location. Default is midpoint (d'/2).</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>None</code> <code>width</code> <code>int</code> <p>Figure width in pixels.</p> <code>700</code> <code>height</code> <code>int</code> <p>Figure height in pixels.</p> <code>400</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object.</p> Source code in <code>senspy/plotting.py</code> <pre><code>def plot_sdt_distributions(\n    d_prime: float = 1.5,\n    show_criterion: bool = True,\n    criterion: float | None = None,\n    title: str | None = None,\n    width: int = 700,\n    height: int = 400,\n) -&gt; go.Figure:\n    \"\"\"Plot Signal Detection Theory distributions.\n\n    Shows the noise and signal distributions with optional criterion\n    line and shaded hit/false alarm regions.\n\n    Parameters\n    ----------\n    d_prime : float, default 1.5\n        Separation between distributions (d-prime).\n    show_criterion : bool, default True\n        Show decision criterion line.\n    criterion : float, optional\n        Criterion location. Default is midpoint (d'/2).\n    title : str, optional\n        Plot title.\n    width : int, default 700\n        Figure width in pixels.\n    height : int, default 400\n        Figure height in pixels.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure object.\n    \"\"\"\n    if criterion is None:\n        criterion = d_prime / 2\n\n    if title is None:\n        title = f\"Signal Detection Theory (d' = {d_prime:.2f})\"\n\n    x = np.linspace(-4, d_prime + 4, 500)\n    noise_dist = stats.norm.pdf(x, loc=0)\n    signal_dist = stats.norm.pdf(x, loc=d_prime)\n\n    fig = go.Figure()\n\n    # Noise distribution\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=noise_dist,\n            mode=\"lines\",\n            name=\"Noise\",\n            line=dict(color=COLORS[\"noise\"], width=2),\n            fill=\"tozeroy\",\n            fillcolor=\"rgba(44, 160, 44, 0.2)\",\n        )\n    )\n\n    # Signal distribution\n    fig.add_trace(\n        go.Scatter(\n            x=x,\n            y=signal_dist,\n            mode=\"lines\",\n            name=\"Signal\",\n            line=dict(color=COLORS[\"signal\"], width=2),\n            fill=\"tozeroy\",\n            fillcolor=\"rgba(214, 39, 40, 0.2)\",\n        )\n    )\n\n    # Criterion line\n    if show_criterion:\n        fig.add_vline(\n            x=criterion,\n            line_dash=\"dash\",\n            line_color=COLORS[\"primary\"],\n            line_width=2,\n            annotation_text=f\"c = {criterion:.2f}\",\n            annotation_position=\"top\",\n        )\n\n        # Add hit rate and FA rate annotations\n        hit_rate = 1 - stats.norm.cdf(criterion, loc=d_prime)\n        fa_rate = 1 - stats.norm.cdf(criterion, loc=0)\n\n        fig.add_annotation(\n            x=criterion + 0.5,\n            y=max(noise_dist) * 0.7,\n            text=f\"Hit = {hit_rate:.2f}&lt;br&gt;FA = {fa_rate:.2f}\",\n            showarrow=False,\n            font=dict(size=11),\n            bgcolor=\"rgba(255,255,255,0.8)\",\n        )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Sensory Magnitude\",\n        yaxis_title=\"Probability Density\",\n        width=width,\n        height=height,\n        legend=dict(x=0.02, y=0.98),\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/plotting/#statistical-plots","title":"Statistical Plots","text":""},{"location":"api/plotting/#plot_profile_likelihood","title":"plot_profile_likelihood","text":""},{"location":"api/plotting/#senspy.plot_profile_likelihood","title":"<code>plot_profile_likelihood(d_prime_values: ArrayLike, log_likelihood_values: ArrayLike, levels: tuple[float, ...] = (0.95, 0.99), title: str = 'Profile Likelihood', relative: bool = True, width: int = 600, height: int = 450) -&gt; go.Figure</code>","text":"<p>Plot profile likelihood with confidence levels.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime_values</code> <code>array_like</code> <p>D-prime values at which likelihood was evaluated.</p> required <code>log_likelihood_values</code> <code>array_like</code> <p>Log-likelihood values (will be normalized to relative likelihood).</p> required <code>levels</code> <code>tuple of float</code> <p>Confidence levels to display as horizontal lines.</p> <code>(0.95, 0.99)</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>\"Profile Likelihood\"</code> <code>relative</code> <code>bool</code> <p>Plot relative likelihood (max = 1) vs raw log-likelihood.</p> <code>True</code> <code>width</code> <code>int</code> <p>Figure width in pixels.</p> <code>600</code> <code>height</code> <code>int</code> <p>Figure height in pixels.</p> <code>450</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object.</p> Source code in <code>senspy/plotting.py</code> <pre><code>def plot_profile_likelihood(\n    d_prime_values: ArrayLike,\n    log_likelihood_values: ArrayLike,\n    levels: tuple[float, ...] = (0.95, 0.99),\n    title: str = \"Profile Likelihood\",\n    relative: bool = True,\n    width: int = 600,\n    height: int = 450,\n) -&gt; go.Figure:\n    \"\"\"Plot profile likelihood with confidence levels.\n\n    Parameters\n    ----------\n    d_prime_values : array_like\n        D-prime values at which likelihood was evaluated.\n    log_likelihood_values : array_like\n        Log-likelihood values (will be normalized to relative likelihood).\n    levels : tuple of float, default (0.95, 0.99)\n        Confidence levels to display as horizontal lines.\n    title : str, default \"Profile Likelihood\"\n        Plot title.\n    relative : bool, default True\n        Plot relative likelihood (max = 1) vs raw log-likelihood.\n    width : int, default 600\n        Figure width in pixels.\n    height : int, default 450\n        Figure height in pixels.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure object.\n    \"\"\"\n    d_values = np.asarray(d_prime_values)\n    ll_values = np.asarray(log_likelihood_values)\n\n    # Compute relative likelihood\n    max_ll = np.max(ll_values)\n    if relative:\n        rel_ll = np.exp(ll_values - max_ll)\n        y_values = rel_ll\n        yaxis_title = \"Relative Likelihood\"\n        y_range = [0, 1.05]\n    else:\n        y_values = ll_values\n        yaxis_title = \"Log-Likelihood\"\n        y_range = None\n\n    fig = go.Figure()\n\n    # Add profile curve\n    fig.add_trace(\n        go.Scatter(\n            x=d_values,\n            y=y_values,\n            mode=\"lines\",\n            name=\"Profile\",\n            line=dict(color=COLORS[\"primary\"], width=2),\n            hovertemplate=\"d' = %{x:.3f}&lt;br&gt;L = %{y:.4f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    # Add confidence level lines\n    for level in levels:\n        cutoff = np.exp(-stats.chi2.ppf(level, df=1) / 2)\n        if relative:\n            fig.add_hline(\n                y=cutoff,\n                line_dash=\"dot\",\n                line_color=COLORS[\"reference\"],\n                annotation_text=f\"{int(level * 100)}% CI\",\n                annotation_position=\"right\",\n            )\n\n    # Mark MLE\n    mle_idx = np.argmax(ll_values)\n    mle_d = d_values[mle_idx]\n    fig.add_vline(\n        x=mle_d,\n        line_dash=\"dash\",\n        line_color=COLORS[\"secondary\"],\n        line_width=1,\n        annotation_text=f\"MLE = {mle_d:.3f}\",\n        annotation_position=\"top\",\n    )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"d-prime (d')\",\n        yaxis_title=yaxis_title,\n        width=width,\n        height=height,\n        yaxis=dict(range=y_range) if y_range else {},\n        showlegend=False,\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/plotting/#plot_power_curve","title":"plot_power_curve","text":""},{"location":"api/plotting/#senspy.plot_power_curve","title":"<code>plot_power_curve(d_prime_values: ArrayLike, power_values: ArrayLike, target_power: float = 0.8, title: str = 'Power Curve', width: int = 600, height: int = 450) -&gt; go.Figure</code>","text":"<p>Plot power as a function of effect size (d-prime).</p> <p>Parameters:</p> Name Type Description Default <code>d_prime_values</code> <code>array_like</code> <p>D-prime values.</p> required <code>power_values</code> <code>array_like</code> <p>Corresponding power values.</p> required <code>target_power</code> <code>float</code> <p>Target power level to highlight.</p> <code>0.8</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>\"Power Curve\"</code> <code>width</code> <code>int</code> <p>Figure width in pixels.</p> <code>600</code> <code>height</code> <code>int</code> <p>Figure height in pixels.</p> <code>450</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object.</p> Source code in <code>senspy/plotting.py</code> <pre><code>def plot_power_curve(\n    d_prime_values: ArrayLike,\n    power_values: ArrayLike,\n    target_power: float = 0.8,\n    title: str = \"Power Curve\",\n    width: int = 600,\n    height: int = 450,\n) -&gt; go.Figure:\n    \"\"\"Plot power as a function of effect size (d-prime).\n\n    Parameters\n    ----------\n    d_prime_values : array_like\n        D-prime values.\n    power_values : array_like\n        Corresponding power values.\n    target_power : float, default 0.8\n        Target power level to highlight.\n    title : str, default \"Power Curve\"\n        Plot title.\n    width : int, default 600\n        Figure width in pixels.\n    height : int, default 450\n        Figure height in pixels.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure object.\n    \"\"\"\n    d_values = np.asarray(d_prime_values)\n    p_values = np.asarray(power_values)\n\n    fig = go.Figure()\n\n    # Add power curve\n    fig.add_trace(\n        go.Scatter(\n            x=d_values,\n            y=p_values,\n            mode=\"lines\",\n            name=\"Power\",\n            line=dict(color=COLORS[\"primary\"], width=2.5),\n            hovertemplate=\"d' = %{x:.2f}&lt;br&gt;Power = %{y:.3f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    # Add target power line\n    fig.add_hline(\n        y=target_power,\n        line_dash=\"dash\",\n        line_color=COLORS[\"secondary\"],\n        annotation_text=f\"Target = {target_power}\",\n        annotation_position=\"right\",\n    )\n\n    # Add alpha line\n    fig.add_hline(\n        y=0.05,\n        line_dash=\"dot\",\n        line_color=COLORS[\"reference\"],\n        annotation_text=\"\u03b1 = 0.05\",\n        annotation_position=\"right\",\n    )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"d-prime (d')\",\n        yaxis_title=\"Power\",\n        width=width,\n        height=height,\n        yaxis=dict(range=[0, 1]),\n        showlegend=False,\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/plotting/#plot_sample_size_curve","title":"plot_sample_size_curve","text":""},{"location":"api/plotting/#senspy.plot_sample_size_curve","title":"<code>plot_sample_size_curve(power_values: ArrayLike, sample_sizes: ArrayLike, target_power: float = 0.8, title: str = 'Sample Size vs Power', width: int = 600, height: int = 450) -&gt; go.Figure</code>","text":"<p>Plot sample size as a function of target power.</p> <p>Parameters:</p> Name Type Description Default <code>power_values</code> <code>array_like</code> <p>Target power values.</p> required <code>sample_sizes</code> <code>array_like</code> <p>Required sample sizes.</p> required <code>target_power</code> <code>float</code> <p>Highlight this power level.</p> <code>0.8</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>\"Sample Size vs Power\"</code> <code>width</code> <code>int</code> <p>Figure width in pixels.</p> <code>600</code> <code>height</code> <code>int</code> <p>Figure height in pixels.</p> <code>450</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure object.</p> Source code in <code>senspy/plotting.py</code> <pre><code>def plot_sample_size_curve(\n    power_values: ArrayLike,\n    sample_sizes: ArrayLike,\n    target_power: float = 0.8,\n    title: str = \"Sample Size vs Power\",\n    width: int = 600,\n    height: int = 450,\n) -&gt; go.Figure:\n    \"\"\"Plot sample size as a function of target power.\n\n    Parameters\n    ----------\n    power_values : array_like\n        Target power values.\n    sample_sizes : array_like\n        Required sample sizes.\n    target_power : float, default 0.8\n        Highlight this power level.\n    title : str, default \"Sample Size vs Power\"\n        Plot title.\n    width : int, default 600\n        Figure width in pixels.\n    height : int, default 450\n        Figure height in pixels.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure object.\n    \"\"\"\n    p_values = np.asarray(power_values)\n    n_values = np.asarray(sample_sizes)\n\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Scatter(\n            x=p_values,\n            y=n_values,\n            mode=\"lines\",\n            name=\"Sample Size\",\n            line=dict(color=COLORS[\"primary\"], width=2.5),\n            hovertemplate=\"Power = %{x:.2f}&lt;br&gt;N = %{y:.0f}&lt;extra&gt;&lt;/extra&gt;\",\n        )\n    )\n\n    # Mark target power\n    if target_power in p_values or True:\n        # Interpolate to find sample size at target power\n        target_n = np.interp(target_power, p_values, n_values)\n        fig.add_vline(\n            x=target_power,\n            line_dash=\"dash\",\n            line_color=COLORS[\"secondary\"],\n        )\n        fig.add_annotation(\n            x=target_power,\n            y=target_n,\n            text=f\"N = {target_n:.0f}\",\n            showarrow=True,\n            arrowhead=2,\n            ax=40,\n            ay=-40,\n        )\n\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"Power\",\n        yaxis_title=\"Sample Size (N)\",\n        width=width,\n        height=height,\n        xaxis=dict(range=[0, 1]),\n        showlegend=False,\n        hovermode=\"x unified\",\n    )\n\n    return fig\n</code></pre>"},{"location":"api/power/","title":"Power &amp; Sample Size","text":""},{"location":"api/power/#power-analysis","title":"Power Analysis","text":""},{"location":"api/power/#discrim_power","title":"discrim_power","text":""},{"location":"api/power/#senspy.discrim_power","title":"<code>discrim_power(pd_a: float, sample_size: int, *, pd_0: float = 0.0, alpha: float = 0.05, p_guess: float = 0.5, test: str = 'difference', statistic: str = 'exact') -&gt; float</code>","text":"<p>Compute power for a discrimination test.</p> <p>Parameters:</p> Name Type Description Default <code>pd_a</code> <code>float</code> <p>True proportion of discriminators (alternative hypothesis). Must be between 0 and 1.</p> required <code>sample_size</code> <code>int</code> <p>Number of trials in the experiment.</p> required <code>pd_0</code> <code>float</code> <p>Null hypothesis proportion of discriminators.</p> <code>0.0</code> <code>alpha</code> <code>float</code> <p>Significance level (Type I error rate).</p> <code>0.05</code> <code>p_guess</code> <code>float</code> <p>Guessing probability for the protocol (e.g., 1/3 for triangle).</p> <code>0.5</code> <code>test</code> <code>str</code> <p>Type of test: \"difference\" (H1: pd &gt; pd_0) or \"similarity\" (H1: pd &lt; pd_0).</p> <code>\"difference\"</code> <code>statistic</code> <code>str</code> <p>Method for computing power: - \"exact\": Exact binomial test - \"normal\": Normal approximation - \"cont.normal\": Normal approximation with continuity correction</p> <code>\"exact\"</code> <p>Returns:</p> Type Description <code>float</code> <p>Statistical power (probability of rejecting H0 when H1 is true).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Power for triangle test with 30% discriminators, n=100\n&gt;&gt;&gt; discrim_power(pd_a=0.3, sample_size=100, p_guess=1/3)\n0.869...\n</code></pre> <pre><code>&gt;&gt;&gt; # Power for similarity test\n&gt;&gt;&gt; discrim_power(pd_a=0.1, sample_size=100, pd_0=0.3,\n...               p_guess=1/3, test=\"similarity\")\n0.78...\n</code></pre> Notes <p>Corresponds to <code>discrimPwr()</code> in sensR.</p> Source code in <code>senspy/power.py</code> <pre><code>def discrim_power(\n    pd_a: float,\n    sample_size: int,\n    *,\n    pd_0: float = 0.0,\n    alpha: float = 0.05,\n    p_guess: float = 0.5,\n    test: str = \"difference\",\n    statistic: str = \"exact\",\n) -&gt; float:\n    \"\"\"Compute power for a discrimination test.\n\n    Parameters\n    ----------\n    pd_a : float\n        True proportion of discriminators (alternative hypothesis).\n        Must be between 0 and 1.\n    sample_size : int\n        Number of trials in the experiment.\n    pd_0 : float, default 0.0\n        Null hypothesis proportion of discriminators.\n    alpha : float, default 0.05\n        Significance level (Type I error rate).\n    p_guess : float, default 0.5\n        Guessing probability for the protocol (e.g., 1/3 for triangle).\n    test : str, default \"difference\"\n        Type of test: \"difference\" (H1: pd &gt; pd_0) or\n        \"similarity\" (H1: pd &lt; pd_0).\n    statistic : str, default \"exact\"\n        Method for computing power:\n        - \"exact\": Exact binomial test\n        - \"normal\": Normal approximation\n        - \"cont.normal\": Normal approximation with continuity correction\n\n    Returns\n    -------\n    float\n        Statistical power (probability of rejecting H0 when H1 is true).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Power for triangle test with 30% discriminators, n=100\n    &gt;&gt;&gt; discrim_power(pd_a=0.3, sample_size=100, p_guess=1/3)\n    0.869...\n\n    &gt;&gt;&gt; # Power for similarity test\n    &gt;&gt;&gt; discrim_power(pd_a=0.1, sample_size=100, pd_0=0.3,\n    ...               p_guess=1/3, test=\"similarity\")\n    0.78...\n\n    Notes\n    -----\n    Corresponds to `discrimPwr()` in sensR.\n    \"\"\"\n    # Validate inputs\n    if not 0 &lt;= pd_a &lt;= 1:\n        raise ValueError(\"'pd_a' must be between 0 and 1\")\n    if not 0 &lt;= pd_0 &lt;= 1:\n        raise ValueError(\"'pd_0' must be between 0 and 1\")\n    if not isinstance(sample_size, (int, np.integer)) or sample_size &lt;= 0:\n        raise ValueError(\"'sample_size' must be a positive integer\")\n    if not 0 &lt; alpha &lt; 1:\n        raise ValueError(\"'alpha' must be between 0 and 1\")\n    if not 0 &lt;= p_guess &lt; 1:\n        raise ValueError(\"'p_guess' must be between 0 and 1\")\n\n    sample_size = int(sample_size)\n\n    # Validate test type\n    test = test.lower()\n    if test not in (\"difference\", \"similarity\"):\n        raise ValueError(\"'test' must be 'difference' or 'similarity'\")\n\n    # Validate pd_a vs pd_0 for test type\n    if test == \"difference\" and pd_a &lt; pd_0:\n        raise ValueError(\"'pd_a' must be &gt;= 'pd_0' for difference tests\")\n    if test == \"similarity\" and pd_a &gt; pd_0:\n        raise ValueError(\"'pd_a' must be &lt;= 'pd_0' for similarity tests\")\n\n    # Validate statistic\n    statistic = _normalize_statistic(\n        statistic, {\"exact\", \"normal\", \"cont_normal\"}\n    )\n\n    # Compute power\n    if statistic == \"normal\":\n        return _normal_power(pd_a, pd_0, sample_size, alpha, p_guess, test, False)\n    elif statistic == \"cont_normal\":\n        return _normal_power(pd_a, pd_0, sample_size, alpha, p_guess, test, True)\n    else:  # exact\n        return _exact_power(pd_a, pd_0, sample_size, alpha, p_guess, test)\n</code></pre>"},{"location":"api/power/#dprime_power","title":"dprime_power","text":""},{"location":"api/power/#senspy.dprime_power","title":"<code>dprime_power(d_prime_a: float, sample_size: int, method: str | Protocol = 'triangle', *, d_prime_0: float = 0.0, alpha: float = 0.05, test: str = 'difference', statistic: str = 'exact') -&gt; float</code>","text":"<p>Compute power for a discrimination test using d-prime.</p> <p>This is a convenience wrapper around <code>discrim_power()</code> that converts d-prime values to proportion of discriminators.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime_a</code> <code>float</code> <p>True d-prime value (alternative hypothesis). Must be non-negative.</p> required <code>sample_size</code> <code>int</code> <p>Number of trials in the experiment.</p> required <code>method</code> <code>str or Protocol</code> <p>Discrimination protocol.</p> <code>\"triangle\"</code> <code>d_prime_0</code> <code>float</code> <p>Null hypothesis d-prime value.</p> <code>0.0</code> <code>alpha</code> <code>float</code> <p>Significance level (Type I error rate).</p> <code>0.05</code> <code>test</code> <code>str</code> <p>Type of test: \"difference\" or \"similarity\".</p> <code>\"difference\"</code> <code>statistic</code> <code>str</code> <p>Method for computing power: \"exact\", \"normal\", or \"cont.normal\".</p> <code>\"exact\"</code> <p>Returns:</p> Type Description <code>float</code> <p>Statistical power.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Power for triangle test with d'=1.5, n=100\n&gt;&gt;&gt; dprime_power(d_prime_a=1.5, sample_size=100, method=\"triangle\")\n0.86...\n</code></pre> <pre><code>&gt;&gt;&gt; # Power for 2-AFC test\n&gt;&gt;&gt; dprime_power(d_prime_a=1.0, sample_size=50, method=\"twoafc\")\n0.74...\n</code></pre> Notes <p>Corresponds to <code>d.primePwr()</code> in sensR.</p> Source code in <code>senspy/power.py</code> <pre><code>def dprime_power(\n    d_prime_a: float,\n    sample_size: int,\n    method: str | Protocol = \"triangle\",\n    *,\n    d_prime_0: float = 0.0,\n    alpha: float = 0.05,\n    test: str = \"difference\",\n    statistic: str = \"exact\",\n) -&gt; float:\n    \"\"\"Compute power for a discrimination test using d-prime.\n\n    This is a convenience wrapper around `discrim_power()` that converts\n    d-prime values to proportion of discriminators.\n\n    Parameters\n    ----------\n    d_prime_a : float\n        True d-prime value (alternative hypothesis). Must be non-negative.\n    sample_size : int\n        Number of trials in the experiment.\n    method : str or Protocol, default \"triangle\"\n        Discrimination protocol.\n    d_prime_0 : float, default 0.0\n        Null hypothesis d-prime value.\n    alpha : float, default 0.05\n        Significance level (Type I error rate).\n    test : str, default \"difference\"\n        Type of test: \"difference\" or \"similarity\".\n    statistic : str, default \"exact\"\n        Method for computing power: \"exact\", \"normal\", or \"cont.normal\".\n\n    Returns\n    -------\n    float\n        Statistical power.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Power for triangle test with d'=1.5, n=100\n    &gt;&gt;&gt; dprime_power(d_prime_a=1.5, sample_size=100, method=\"triangle\")\n    0.86...\n\n    &gt;&gt;&gt; # Power for 2-AFC test\n    &gt;&gt;&gt; dprime_power(d_prime_a=1.0, sample_size=50, method=\"twoafc\")\n    0.74...\n\n    Notes\n    -----\n    Corresponds to `d.primePwr()` in sensR.\n    \"\"\"\n    # Validate d-prime values\n    if d_prime_a &lt; 0:\n        raise ValueError(\"'d_prime_a' must be non-negative\")\n    if d_prime_0 &lt; 0:\n        raise ValueError(\"'d_prime_0' must be non-negative\")\n\n    # Parse protocol\n    protocol = parse_protocol(method)\n    p_guess = protocol.p_guess\n\n    # Defensive check (protocols should never have p_guess &gt;= 1)\n    if p_guess &gt;= 1:\n        raise ValueError(\"Protocol p_guess must be &lt; 1\")\n\n    # Convert d-prime to pc, then to pd\n    pc_a = psy_fun(d_prime_a, method=protocol)[0]\n    pc_0 = psy_fun(d_prime_0, method=protocol)[0]\n\n    pd_a = (pc_a - p_guess) / (1 - p_guess)\n    pd_0 = (pc_0 - p_guess) / (1 - p_guess)\n\n    return discrim_power(\n        pd_a=pd_a,\n        sample_size=sample_size,\n        pd_0=pd_0,\n        alpha=alpha,\n        p_guess=p_guess,\n        test=test,\n        statistic=statistic,\n    )\n</code></pre>"},{"location":"api/power/#dod_power","title":"dod_power","text":""},{"location":"api/power/#senspy.dod_power","title":"<code>dod_power(d_primeA: float, d_prime0: float = 0.0, ncat: int = 4, sample_size: int | tuple[int, int] = 100, nsim: int = 1000, alpha: float = 0.05, method_tau: Literal['equi_prob', 'LR_max', 'se_min', 'user_defined'] = 'LR_max', statistic: Literal['likelihood', 'Wilcoxon', 'Pearson', 'Wald'] = 'likelihood', alternative: Literal['difference', 'similarity', 'two.sided', 'less', 'greater'] = 'difference', tau: NDArray | None = None, random_state: int | np.random.Generator | None = None) -&gt; DODPowerResult</code>","text":"<p>Compute power for DOD discrimination test via simulation.</p> <p>This function estimates the power of a DOD (Degree-of-Difference) test by simulating data under the alternative hypothesis and computing the proportion of simulations that would reject the null hypothesis.</p> <p>Parameters:</p> Name Type Description Default <code>d_primeA</code> <code>float</code> <p>True d-prime value (alternative hypothesis).</p> required <code>d_prime0</code> <code>float</code> <p>Null hypothesis d-prime value. Default is 0.</p> <code>0.0</code> <code>ncat</code> <code>int</code> <p>Number of response categories. Default is 4.</p> <code>4</code> <code>sample_size</code> <code>int | tuple[int, int]</code> <p>Sample size. If int, same size for both same-pairs and diff-pairs. If tuple, (same_size, diff_size). Default is 100.</p> <code>100</code> <code>nsim</code> <code>int</code> <p>Number of simulations. Default is 1000.</p> <code>1000</code> <code>alpha</code> <code>float</code> <p>Significance level. Default is 0.05.</p> <code>0.05</code> <code>method_tau</code> <code>str</code> <p>Method for determining tau values: - \"LR_max\": Maximum likelihood ratio (default) - \"equi_prob\": Equal category probabilities - \"se_min\": Minimum standard error - \"user_defined\": Use provided tau values</p> <code>'LR_max'</code> <code>statistic</code> <code>str</code> <p>Test statistic: - \"likelihood\": Likelihood ratio test (default) - \"Pearson\": Pearson chi-square - \"Wilcoxon\": Wilcoxon rank-sum test (requires d_prime0=0) - \"Wald\": Wald test</p> <code>'likelihood'</code> <code>alternative</code> <code>str</code> <p>Alternative hypothesis: - \"difference\" or \"greater\": d-prime &gt; d_prime0 (default) - \"similarity\" or \"less\": d-prime &lt; d_prime0 - \"two.sided\": d-prime != d_prime0</p> <code>'difference'</code> <code>tau</code> <code>NDArray | None</code> <p>Boundary parameters (required if method_tau=\"user_defined\").</p> <code>None</code> <code>random_state</code> <code>int | Generator | None</code> <p>Random state for reproducibility.</p> <code>None</code> <p>Returns:</p> Type Description <code>DODPowerResult</code> <p>Power analysis results including power estimate and standard error.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid or incompatible.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = dod_power(d_primeA=1.0, sample_size=100, nsim=500)\n&gt;&gt;&gt; print(f\"Power: {result.power:.3f} (SE: {result.se_power:.3f})\")\n</code></pre> <pre><code>&gt;&gt;&gt; # With specific tau values\n&gt;&gt;&gt; result = dod_power(\n...     d_primeA=1.5,\n...     method_tau=\"user_defined\",\n...     tau=np.array([0.5, 1.0, 1.5])\n... )\n</code></pre> Source code in <code>senspy/dod.py</code> <pre><code>def dod_power(\n    d_primeA: float,\n    d_prime0: float = 0.0,\n    ncat: int = 4,\n    sample_size: int | tuple[int, int] = 100,\n    nsim: int = 1000,\n    alpha: float = 0.05,\n    method_tau: Literal[\"equi_prob\", \"LR_max\", \"se_min\", \"user_defined\"] = \"LR_max\",\n    statistic: Literal[\"likelihood\", \"Wilcoxon\", \"Pearson\", \"Wald\"] = \"likelihood\",\n    alternative: Literal[\n        \"difference\", \"similarity\", \"two.sided\", \"less\", \"greater\"\n    ] = \"difference\",\n    tau: NDArray | None = None,\n    random_state: int | np.random.Generator | None = None,\n) -&gt; DODPowerResult:\n    \"\"\"Compute power for DOD discrimination test via simulation.\n\n    This function estimates the power of a DOD (Degree-of-Difference) test\n    by simulating data under the alternative hypothesis and computing the\n    proportion of simulations that would reject the null hypothesis.\n\n    Parameters\n    ----------\n    d_primeA : float\n        True d-prime value (alternative hypothesis).\n    d_prime0 : float\n        Null hypothesis d-prime value. Default is 0.\n    ncat : int\n        Number of response categories. Default is 4.\n    sample_size : int | tuple[int, int]\n        Sample size. If int, same size for both same-pairs and diff-pairs.\n        If tuple, (same_size, diff_size). Default is 100.\n    nsim : int\n        Number of simulations. Default is 1000.\n    alpha : float\n        Significance level. Default is 0.05.\n    method_tau : str\n        Method for determining tau values:\n        - \"LR_max\": Maximum likelihood ratio (default)\n        - \"equi_prob\": Equal category probabilities\n        - \"se_min\": Minimum standard error\n        - \"user_defined\": Use provided tau values\n    statistic : str\n        Test statistic:\n        - \"likelihood\": Likelihood ratio test (default)\n        - \"Pearson\": Pearson chi-square\n        - \"Wilcoxon\": Wilcoxon rank-sum test (requires d_prime0=0)\n        - \"Wald\": Wald test\n    alternative : str\n        Alternative hypothesis:\n        - \"difference\" or \"greater\": d-prime &gt; d_prime0 (default)\n        - \"similarity\" or \"less\": d-prime &lt; d_prime0\n        - \"two.sided\": d-prime != d_prime0\n    tau : NDArray | None\n        Boundary parameters (required if method_tau=\"user_defined\").\n    random_state : int | np.random.Generator | None\n        Random state for reproducibility.\n\n    Returns\n    -------\n    DODPowerResult\n        Power analysis results including power estimate and standard error.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid or incompatible.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = dod_power(d_primeA=1.0, sample_size=100, nsim=500)\n    &gt;&gt;&gt; print(f\"Power: {result.power:.3f} (SE: {result.se_power:.3f})\")\n\n    &gt;&gt;&gt; # With specific tau values\n    &gt;&gt;&gt; result = dod_power(\n    ...     d_primeA=1.5,\n    ...     method_tau=\"user_defined\",\n    ...     tau=np.array([0.5, 1.0, 1.5])\n    ... )\n    \"\"\"\n    # Validate inputs\n    if not (isinstance(d_primeA, (int, float)) and d_primeA &gt;= 0 and np.isfinite(d_primeA)):\n        raise ValueError(\"d_primeA must be a finite non-negative number\")\n    if not (isinstance(d_prime0, (int, float)) and d_prime0 &gt;= 0 and np.isfinite(d_prime0)):\n        raise ValueError(\"d_prime0 must be a finite non-negative number\")\n    if nsim &lt;= 0 or not isinstance(nsim, (int, float)):\n        raise ValueError(\"nsim must be a positive number\")\n    if not (0 &lt;= alpha &lt;= 1):\n        raise ValueError(\"alpha must be between 0 and 1\")\n\n    nsim = int(round(nsim))\n\n    # Handle sample size\n    if isinstance(sample_size, int):\n        size = (sample_size, sample_size)\n    else:\n        size = (int(round(sample_size[0])), int(round(sample_size[1])))\n\n    # Map alternative names\n    alt = alternative\n    if alt == \"difference\":\n        alt = \"greater\"\n    elif alt == \"similarity\":\n        alt = \"less\"\n\n    # Check d_primeA, d_prime0 consistency with alternative\n    if alt == \"greater\" and d_primeA &lt; d_prime0:\n        raise ValueError(\n            f\"Need d_primeA &gt;= d_prime0 when alternative is '{alternative}'\"\n        )\n    if alt == \"less\" and d_primeA &gt; d_prime0:\n        raise ValueError(\n            f\"Need d_primeA &lt;= d_prime0 when alternative is '{alternative}'\"\n        )\n\n    # Wilcoxon requires d_prime0 = 0\n    if statistic == \"Wilcoxon\" and not np.isclose(d_prime0, 0.0):\n        raise ValueError(\"Wilcoxon statistic only available with d_prime0=0\")\n\n    # Get tau values\n    if method_tau != \"user_defined\":\n        method_map = {\n            \"equi_prob\": \"equi_prob\",\n            \"LR_max\": \"LR_max\",\n            \"se_min\": \"se_min\",\n        }\n        tau_result = optimal_tau(\n            d_prime=d_primeA,\n            d_prime0=d_prime0,\n            ncat=ncat,\n            method=method_map[method_tau],\n            do_warn=False,\n        )\n        tau_arr = tau_result[\"tau\"]\n    else:\n        if tau is None:\n            raise ValueError(\"tau must be provided when method_tau='user_defined'\")\n        tau_arr = np.asarray(tau, dtype=float)\n\n    # Handle random state\n    if random_state is None:\n        rng = np.random.default_rng()\n    elif isinstance(random_state, int):\n        rng = np.random.default_rng(random_state)\n    else:\n        rng = random_state\n\n    # Run simulations\n    pvals = np.full(nsim, np.nan)\n    ctrl = DODControl(get_vcov=statistic == \"Wald\", get_grad=False, do_warn=False)\n\n    for i in range(nsim):\n        # Simulate data under alternative\n        data = dod_sim(\n            d_prime=d_primeA,\n            sample_size=size,\n            method_tau=\"user_defined\",\n            tau=tau_arr,\n            random_state=rng,\n        )\n        same_sim = data[0, :]\n        diff_sim = data[1, :]\n\n        if statistic == \"Wilcoxon\":\n            # Special case: Wilcoxon test without DOD fitting\n            from scipy.stats import mannwhitneyu\n\n            nlev = data.shape[1]\n            # Expand rating categories by their counts for each pair type\n            diff_pair_ratings = np.repeat(np.arange(1, nlev + 1), diff_sim.astype(int))\n            same_pair_ratings = np.repeat(np.arange(1, nlev + 1), same_sim.astype(int))\n\n            if alt == \"two.sided\":\n                alt_mwu = \"two-sided\"\n            elif alt == \"greater\":\n                alt_mwu = \"greater\"\n            else:\n                alt_mwu = \"less\"\n\n            try:\n                # Test if diff-pair ratings are stochastically greater than same-pair ratings\n                mwu_result = mannwhitneyu(diff_pair_ratings, same_pair_ratings, alternative=alt_mwu)\n                pvals[i] = mwu_result.pvalue\n            except ValueError:\n                pass\n\n        elif statistic == \"Wald\":\n            # Wald test\n            try:\n                fit = dod_fit(same_sim, diff_sim, control=ctrl)\n                if fit.vcov is not None and np.all(np.isfinite(fit.vcov)):\n                    std_err = np.sqrt(fit.vcov[-1, -1])\n                    if np.isfinite(std_err) and std_err &gt; 0:\n                        stat_value = (fit.d_prime - d_prime0) / std_err\n                        p_val = normal_pvalue(stat_value, alt)\n                        if hasattr(p_val, 'item'):\n                            p_val = p_val.item()\n                        pvals[i] = p_val\n            except (ValueError, RuntimeError, np.linalg.LinAlgError):\n                # Optimization or numerical errors - skip this simulation\n                pass\n\n        else:  # likelihood or Pearson\n            try:\n                result = dod(\n                    same_sim, diff_sim,\n                    d_prime0=d_prime0,\n                    alternative=alt,\n                    statistic=statistic,\n                    control=ctrl,\n                )\n                if result.convergence == 0:\n                    # p_value may be array, extract scalar\n                    p_val = result.p_value\n                    if hasattr(p_val, 'item'):\n                        p_val = p_val.item()\n                    pvals[i] = p_val\n            except (ValueError, RuntimeError, np.linalg.LinAlgError):\n                # Optimization or numerical errors - skip this simulation\n                pass\n\n    # Compute power\n    valid_pvals = pvals[~np.isnan(pvals)]\n    n_used = len(valid_pvals)\n\n    if n_used == 0:\n        power = np.nan\n        se_power = np.nan\n    else:\n        power = np.mean(valid_pvals &lt; alpha)\n        if power == 0 or power == 1 or not np.isfinite(power):\n            se_power = np.nan\n        else:\n            se_power = np.sqrt(power * (1 - power) / n_used)\n\n    return DODPowerResult(\n        power=power,\n        se_power=se_power,\n        n_used=n_used,\n        d_primeA=d_primeA,\n        d_prime0=d_prime0,\n        sample_size=size,\n        nsim=nsim,\n        alpha=alpha,\n        statistic=statistic,\n        alternative=alt,\n        tau=tau_arr,\n    )\n</code></pre>"},{"location":"api/power/#sample-size-calculation","title":"Sample Size Calculation","text":""},{"location":"api/power/#discrim_sample_size","title":"discrim_sample_size","text":""},{"location":"api/power/#senspy.discrim_sample_size","title":"<code>discrim_sample_size(pd_a: float, *, pd_0: float = 0.0, target_power: float = 0.9, alpha: float = 0.05, p_guess: float = 0.5, test: str = 'difference', statistic: str = 'exact') -&gt; int</code>","text":"<p>Compute required sample size for a discrimination test.</p> <p>Parameters:</p> Name Type Description Default <code>pd_a</code> <code>float</code> <p>True proportion of discriminators (alternative hypothesis).</p> required <code>pd_0</code> <code>float</code> <p>Null hypothesis proportion of discriminators.</p> <code>0.0</code> <code>target_power</code> <code>float</code> <p>Desired statistical power.</p> <code>0.90</code> <code>alpha</code> <code>float</code> <p>Significance level.</p> <code>0.05</code> <code>p_guess</code> <code>float</code> <p>Guessing probability for the protocol.</p> <code>0.5</code> <code>test</code> <code>str</code> <p>Type of test: \"difference\" or \"similarity\".</p> <code>\"difference\"</code> <code>statistic</code> <code>str</code> <p>Method for computing sample size: - \"exact\": First n where exact power exceeds target - \"stable.exact\": Stable n (power stays above target) - \"normal\": Normal approximation - \"cont.normal\": Normal approximation with continuity correction</p> <code>\"exact\"</code> <p>Returns:</p> Type Description <code>int</code> <p>Required sample size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Sample size for triangle test with 30% discriminators\n&gt;&gt;&gt; discrim_sample_size(pd_a=0.3, p_guess=1/3)\n85\n</code></pre> <pre><code>&gt;&gt;&gt; # Using normal approximation\n&gt;&gt;&gt; discrim_sample_size(pd_a=0.3, p_guess=1/3, statistic=\"normal\")\n83\n</code></pre> Notes <p>Corresponds to <code>discrimSS()</code> in sensR.</p> <p>The \"exact\" method finds the first sample size where power exceeds the target. Due to the discrete nature of the binomial distribution, power may drop below target for slightly larger n. Use \"stable.exact\" if you need power to remain above target for all larger sample sizes.</p> Source code in <code>senspy/power.py</code> <pre><code>def discrim_sample_size(\n    pd_a: float,\n    *,\n    pd_0: float = 0.0,\n    target_power: float = 0.90,\n    alpha: float = 0.05,\n    p_guess: float = 0.5,\n    test: str = \"difference\",\n    statistic: str = \"exact\",\n) -&gt; int:\n    \"\"\"Compute required sample size for a discrimination test.\n\n    Parameters\n    ----------\n    pd_a : float\n        True proportion of discriminators (alternative hypothesis).\n    pd_0 : float, default 0.0\n        Null hypothesis proportion of discriminators.\n    target_power : float, default 0.90\n        Desired statistical power.\n    alpha : float, default 0.05\n        Significance level.\n    p_guess : float, default 0.5\n        Guessing probability for the protocol.\n    test : str, default \"difference\"\n        Type of test: \"difference\" or \"similarity\".\n    statistic : str, default \"exact\"\n        Method for computing sample size:\n        - \"exact\": First n where exact power exceeds target\n        - \"stable.exact\": Stable n (power stays above target)\n        - \"normal\": Normal approximation\n        - \"cont.normal\": Normal approximation with continuity correction\n\n    Returns\n    -------\n    int\n        Required sample size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Sample size for triangle test with 30% discriminators\n    &gt;&gt;&gt; discrim_sample_size(pd_a=0.3, p_guess=1/3)\n    85\n\n    &gt;&gt;&gt; # Using normal approximation\n    &gt;&gt;&gt; discrim_sample_size(pd_a=0.3, p_guess=1/3, statistic=\"normal\")\n    83\n\n    Notes\n    -----\n    Corresponds to `discrimSS()` in sensR.\n\n    The \"exact\" method finds the first sample size where power exceeds\n    the target. Due to the discrete nature of the binomial distribution,\n    power may drop below target for slightly larger n. Use \"stable.exact\"\n    if you need power to remain above target for all larger sample sizes.\n    \"\"\"\n    # Validate inputs\n    if not 0 &lt; pd_a &lt;= 1:\n        raise ValueError(\"'pd_a' must be between 0 and 1 (exclusive of 0)\")\n    if not 0 &lt;= pd_0 &lt; 1:\n        raise ValueError(\"'pd_0' must be between 0 and 1\")\n    if not 0 &lt; target_power &lt; 1:\n        raise ValueError(\"'target_power' must be between 0 and 1\")\n    if not 0 &lt; alpha &lt; 1:\n        raise ValueError(\"'alpha' must be between 0 and 1\")\n    if not 0 &lt;= p_guess &lt; 1:\n        raise ValueError(\"'p_guess' must be between 0 and 1\")\n\n    # Validate test type\n    test = test.lower()\n    if test not in (\"difference\", \"similarity\"):\n        raise ValueError(\"'test' must be 'difference' or 'similarity'\")\n\n    # Validate pd_a vs pd_0 for test type\n    if test == \"difference\" and pd_a &lt;= pd_0:\n        raise ValueError(\"'pd_a' must be &gt; 'pd_0' for difference tests\")\n    if test == \"similarity\" and pd_a &gt;= pd_0:\n        raise ValueError(\"'pd_a' must be &lt; 'pd_0' for similarity tests\")\n\n    # Validate statistic\n    statistic = _normalize_statistic(\n        statistic, {\"exact\", \"stable_exact\", \"normal\", \"cont_normal\"}\n    )\n\n    # Compute sample size\n    if statistic == \"normal\":\n        return _normal_sample_size(\n            pd_a, pd_0, target_power, alpha, p_guess, test, continuity=False\n        )\n    elif statistic == \"cont_normal\":\n        return _normal_sample_size(\n            pd_a, pd_0, target_power, alpha, p_guess, test, continuity=True\n        )\n    elif statistic == \"exact\":\n        return _exact_sample_size(\n            pd_a, pd_0, target_power, alpha, p_guess, test, stable=False\n        )\n    else:  # stable_exact\n        return _exact_sample_size(\n            pd_a, pd_0, target_power, alpha, p_guess, test, stable=True\n        )\n</code></pre>"},{"location":"api/power/#dprime_sample_size","title":"dprime_sample_size","text":""},{"location":"api/power/#senspy.dprime_sample_size","title":"<code>dprime_sample_size(d_prime_a: float, method: str | Protocol = 'triangle', *, d_prime_0: float = 0.0, target_power: float = 0.9, alpha: float = 0.05, test: str = 'difference', statistic: str = 'exact') -&gt; int</code>","text":"<p>Compute required sample size using d-prime.</p> <p>This is a convenience wrapper around <code>discrim_sample_size()</code> that converts d-prime values to proportion of discriminators.</p> <p>Parameters:</p> Name Type Description Default <code>d_prime_a</code> <code>float</code> <p>True d-prime value (alternative hypothesis).</p> required <code>method</code> <code>str or Protocol</code> <p>Discrimination protocol.</p> <code>\"triangle\"</code> <code>d_prime_0</code> <code>float</code> <p>Null hypothesis d-prime value.</p> <code>0.0</code> <code>target_power</code> <code>float</code> <p>Desired statistical power.</p> <code>0.90</code> <code>alpha</code> <code>float</code> <p>Significance level.</p> <code>0.05</code> <code>test</code> <code>str</code> <p>Type of test: \"difference\" or \"similarity\".</p> <code>\"difference\"</code> <code>statistic</code> <code>str</code> <p>Method: \"exact\", \"stable.exact\", \"normal\", or \"cont.normal\".</p> <code>\"exact\"</code> <p>Returns:</p> Type Description <code>int</code> <p>Required sample size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Sample size for triangle test with d'=1.5\n&gt;&gt;&gt; dprime_sample_size(d_prime_a=1.5, method=\"triangle\")\n62\n</code></pre> <pre><code>&gt;&gt;&gt; # Sample size for 2-AFC test with 80% power\n&gt;&gt;&gt; dprime_sample_size(d_prime_a=1.0, method=\"twoafc\", target_power=0.80)\n47\n</code></pre> Notes <p>Corresponds to <code>d.primeSS()</code> in sensR.</p> Source code in <code>senspy/power.py</code> <pre><code>def dprime_sample_size(\n    d_prime_a: float,\n    method: str | Protocol = \"triangle\",\n    *,\n    d_prime_0: float = 0.0,\n    target_power: float = 0.90,\n    alpha: float = 0.05,\n    test: str = \"difference\",\n    statistic: str = \"exact\",\n) -&gt; int:\n    \"\"\"Compute required sample size using d-prime.\n\n    This is a convenience wrapper around `discrim_sample_size()` that\n    converts d-prime values to proportion of discriminators.\n\n    Parameters\n    ----------\n    d_prime_a : float\n        True d-prime value (alternative hypothesis).\n    method : str or Protocol, default \"triangle\"\n        Discrimination protocol.\n    d_prime_0 : float, default 0.0\n        Null hypothesis d-prime value.\n    target_power : float, default 0.90\n        Desired statistical power.\n    alpha : float, default 0.05\n        Significance level.\n    test : str, default \"difference\"\n        Type of test: \"difference\" or \"similarity\".\n    statistic : str, default \"exact\"\n        Method: \"exact\", \"stable.exact\", \"normal\", or \"cont.normal\".\n\n    Returns\n    -------\n    int\n        Required sample size.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Sample size for triangle test with d'=1.5\n    &gt;&gt;&gt; dprime_sample_size(d_prime_a=1.5, method=\"triangle\")\n    62\n\n    &gt;&gt;&gt; # Sample size for 2-AFC test with 80% power\n    &gt;&gt;&gt; dprime_sample_size(d_prime_a=1.0, method=\"twoafc\", target_power=0.80)\n    47\n\n    Notes\n    -----\n    Corresponds to `d.primeSS()` in sensR.\n    \"\"\"\n    # Validate d-prime values\n    if d_prime_a &lt;= 0:\n        raise ValueError(\"'d_prime_a' must be positive\")\n    if d_prime_0 &lt; 0:\n        raise ValueError(\"'d_prime_0' must be non-negative\")\n\n    # Parse protocol\n    protocol = parse_protocol(method)\n    p_guess = protocol.p_guess\n\n    # Defensive check (protocols should never have p_guess &gt;= 1)\n    if p_guess &gt;= 1:\n        raise ValueError(\"Protocol p_guess must be &lt; 1\")\n\n    # Convert d-prime to pc, then to pd\n    pc_a = psy_fun(d_prime_a, method=protocol)[0]\n    pc_0 = psy_fun(d_prime_0, method=protocol)[0]\n\n    pd_a = (pc_a - p_guess) / (1 - p_guess)\n    pd_0 = (pc_0 - p_guess) / (1 - p_guess)\n\n    return discrim_sample_size(\n        pd_a=pd_a,\n        pd_0=pd_0,\n        target_power=target_power,\n        alpha=alpha,\n        p_guess=p_guess,\n        test=test,\n        statistic=statistic,\n    )\n</code></pre>"},{"location":"api/power/#d-prime-hypothesis-testing","title":"D-Prime Hypothesis Testing","text":""},{"location":"api/power/#dprime_test","title":"dprime_test","text":""},{"location":"api/power/#senspy.dprime_test","title":"<code>dprime_test(correct: ArrayLike, total: ArrayLike, protocol: ArrayLike, conf_level: float = 0.95, dprime0: float = 0.0, statistic: Literal['likelihood', 'Wald'] = 'likelihood', alternative: Literal['difference', 'similarity', 'two.sided', 'less', 'greater'] = 'difference', estim: Literal['ML', 'weighted_avg'] = 'ML') -&gt; DprimeTestResult</code>","text":"<p>Test if a common d-prime equals a specified value.</p> <p>This function estimates a common d-prime from multiple groups (potentially using different protocols) and tests whether it equals a specified null hypothesis value.</p> <p>Parameters:</p> Name Type Description Default <code>correct</code> <code>ArrayLike</code> <p>Number of correct responses in each group.</p> required <code>total</code> <code>ArrayLike</code> <p>Total number of trials in each group.</p> required <code>protocol</code> <code>ArrayLike</code> <p>Protocol name for each group (e.g., \"triangle\", \"duotrio\").</p> required <code>conf_level</code> <code>float</code> <p>Confidence level for interval. Default is 0.95.</p> <code>0.95</code> <code>dprime0</code> <code>float</code> <p>Null hypothesis value for d-prime. Default is 0.</p> <code>0.0</code> <code>statistic</code> <code>str</code> <p>Test statistic: \"likelihood\" or \"Wald\". Default is \"likelihood\".</p> <code>'likelihood'</code> <code>alternative</code> <code>str</code> <p>Alternative hypothesis: - \"difference\" or \"greater\": d-prime &gt; dprime0 (default) - \"similarity\" or \"less\": d-prime &lt; dprime0 - \"two.sided\": d-prime != dprime0</p> <code>'difference'</code> <code>estim</code> <code>str</code> <p>Estimation method: \"ML\" or \"weighted_avg\". Default is \"ML\".</p> <code>'ML'</code> <p>Returns:</p> Type Description <code>DprimeTestResult</code> <p>Test results including estimate, confidence interval, and p-value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = dprime_test(\n...     correct=[60, 45, 55],\n...     total=[100, 100, 100],\n...     protocol=[\"triangle\", \"duotrio\", \"twoAFC\"]\n... )\n&gt;&gt;&gt; print(f\"Common d-prime: {result.d_prime:.3f}\")\n&gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n</code></pre> Source code in <code>senspy/dprime_tests.py</code> <pre><code>def dprime_test(\n    correct: ArrayLike,\n    total: ArrayLike,\n    protocol: ArrayLike,\n    conf_level: float = 0.95,\n    dprime0: float = 0.0,\n    statistic: Literal[\"likelihood\", \"Wald\"] = \"likelihood\",\n    alternative: Literal[\n        \"difference\", \"similarity\", \"two.sided\", \"less\", \"greater\"\n    ] = \"difference\",\n    estim: Literal[\"ML\", \"weighted_avg\"] = \"ML\",\n) -&gt; DprimeTestResult:\n    \"\"\"Test if a common d-prime equals a specified value.\n\n    This function estimates a common d-prime from multiple groups (potentially\n    using different protocols) and tests whether it equals a specified null\n    hypothesis value.\n\n    Parameters\n    ----------\n    correct : ArrayLike\n        Number of correct responses in each group.\n    total : ArrayLike\n        Total number of trials in each group.\n    protocol : ArrayLike\n        Protocol name for each group (e.g., \"triangle\", \"duotrio\").\n    conf_level : float\n        Confidence level for interval. Default is 0.95.\n    dprime0 : float\n        Null hypothesis value for d-prime. Default is 0.\n    statistic : str\n        Test statistic: \"likelihood\" or \"Wald\". Default is \"likelihood\".\n    alternative : str\n        Alternative hypothesis:\n        - \"difference\" or \"greater\": d-prime &gt; dprime0 (default)\n        - \"similarity\" or \"less\": d-prime &lt; dprime0\n        - \"two.sided\": d-prime != dprime0\n    estim : str\n        Estimation method: \"ML\" or \"weighted_avg\". Default is \"ML\".\n\n    Returns\n    -------\n    DprimeTestResult\n        Test results including estimate, confidence interval, and p-value.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = dprime_test(\n    ...     correct=[60, 45, 55],\n    ...     total=[100, 100, 100],\n    ...     protocol=[\"triangle\", \"duotrio\", \"twoAFC\"]\n    ... )\n    &gt;&gt;&gt; print(f\"Common d-prime: {result.d_prime:.3f}\")\n    &gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n    \"\"\"\n    # Map alternative names\n    if alternative == \"difference\":\n        alternative = \"greater\"\n    elif alternative == \"similarity\":\n        alternative = \"less\"\n\n    # Validate\n    if not 0 &lt; conf_level &lt; 1:\n        raise ValueError(\"conf_level must be between 0 and 1\")\n    if dprime0 &lt; 0:\n        raise ValueError(\"dprime0 must be non-negative\")\n    if np.isclose(dprime0, 0.0) and alternative not in [\"difference\", \"greater\"]:\n        raise ValueError(\n            \"'alternative' has to be 'difference'/'greater' if 'dprime0' is 0\"\n        )\n\n    # Get data table\n    data = dprime_table(correct, total, protocol)\n\n    # Estimate common d-prime\n    d_exp, se_exp, nll_exp = _dprime_estim(data, estim)\n\n    # Compute test statistic\n    if statistic == \"likelihood\":\n        nll_0 = _dprime_nll(dprime0, data)\n        if nll_exp is None:\n            nll_exp = _dprime_nll(d_exp, data)\n        LR = 2 * (nll_0 - nll_exp)\n        # Signed likelihood root statistic\n        stat_value = np.sign(d_exp - dprime0) * np.sqrt(abs(LR))\n    else:  # Wald\n        if not np.all(np.isfinite([d_exp, se_exp])):\n            raise ValueError(\n                \"Boundary cases occurred: use 'statistic = likelihood' instead\"\n            )\n        stat_value = (d_exp - dprime0) / se_exp\n\n    # Compute p-value\n    p_value = normal_pvalue(stat_value, alternative)\n\n    # Compute Wald confidence interval\n    alpha = (1 - conf_level) / 2\n    z = norm.ppf(1 - alpha)\n    ci_lower = max(0, d_exp - z * se_exp) if np.isfinite(se_exp) else np.nan\n    ci_upper = d_exp + z * se_exp if np.isfinite(se_exp) else np.nan\n    conf_int = (ci_lower, ci_upper)\n\n    return DprimeTestResult(\n        d_prime=d_exp,\n        se_d_prime=se_exp,\n        conf_int=conf_int,\n        conf_level=conf_level,\n        conf_method=\"Wald\",\n        stat_value=stat_value,\n        p_value=p_value,\n        statistic=statistic,\n        alternative=alternative,\n        dprime0=dprime0,\n        estim=estim,\n        data=data,\n    )\n</code></pre>"},{"location":"api/power/#dprime_compare","title":"dprime_compare","text":""},{"location":"api/power/#senspy.dprime_compare","title":"<code>dprime_compare(correct: ArrayLike, total: ArrayLike, protocol: ArrayLike, conf_level: float = 0.95, statistic: Literal['likelihood', 'Pearson', 'Wald.p', 'Wald.d'] = 'likelihood', estim: Literal['ML', 'weighted_avg'] = 'ML') -&gt; DprimeCompareResult</code>","text":"<p>Test if all d-primes are equal (any-difference test).</p> <p>This function tests the null hypothesis that all groups share a common d-prime value against the alternative that at least two differ.</p> <p>Parameters:</p> Name Type Description Default <code>correct</code> <code>ArrayLike</code> <p>Number of correct responses in each group.</p> required <code>total</code> <code>ArrayLike</code> <p>Total number of trials in each group.</p> required <code>protocol</code> <code>ArrayLike</code> <p>Protocol name for each group.</p> required <code>conf_level</code> <code>float</code> <p>Confidence level for interval. Default is 0.95.</p> <code>0.95</code> <code>statistic</code> <code>str</code> <p>Test statistic: - \"likelihood\": Likelihood ratio test (default) - \"Pearson\": Pearson chi-square - \"Wald.p\": Wald test on proportions - \"Wald.d\": Wald test on d-primes</p> <code>'likelihood'</code> <code>estim</code> <code>str</code> <p>Estimation method: \"ML\" or \"weighted_avg\". Default is \"ML\".</p> <code>'ML'</code> <p>Returns:</p> Type Description <code>DprimeCompareResult</code> <p>Test results including chi-square statistic, df, and p-value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = dprime_compare(\n...     correct=[60, 45, 55],\n...     total=[100, 100, 100],\n...     protocol=[\"triangle\", \"duotrio\", \"twoAFC\"]\n... )\n&gt;&gt;&gt; print(f\"Chi-square: {result.stat_value:.3f}, df={result.df}\")\n&gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n</code></pre> Source code in <code>senspy/dprime_tests.py</code> <pre><code>def dprime_compare(\n    correct: ArrayLike,\n    total: ArrayLike,\n    protocol: ArrayLike,\n    conf_level: float = 0.95,\n    statistic: Literal[\"likelihood\", \"Pearson\", \"Wald.p\", \"Wald.d\"] = \"likelihood\",\n    estim: Literal[\"ML\", \"weighted_avg\"] = \"ML\",\n) -&gt; DprimeCompareResult:\n    \"\"\"Test if all d-primes are equal (any-difference test).\n\n    This function tests the null hypothesis that all groups share a common\n    d-prime value against the alternative that at least two differ.\n\n    Parameters\n    ----------\n    correct : ArrayLike\n        Number of correct responses in each group.\n    total : ArrayLike\n        Total number of trials in each group.\n    protocol : ArrayLike\n        Protocol name for each group.\n    conf_level : float\n        Confidence level for interval. Default is 0.95.\n    statistic : str\n        Test statistic:\n        - \"likelihood\": Likelihood ratio test (default)\n        - \"Pearson\": Pearson chi-square\n        - \"Wald.p\": Wald test on proportions\n        - \"Wald.d\": Wald test on d-primes\n    estim : str\n        Estimation method: \"ML\" or \"weighted_avg\". Default is \"ML\".\n\n    Returns\n    -------\n    DprimeCompareResult\n        Test results including chi-square statistic, df, and p-value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = dprime_compare(\n    ...     correct=[60, 45, 55],\n    ...     total=[100, 100, 100],\n    ...     protocol=[\"triangle\", \"duotrio\", \"twoAFC\"]\n    ... )\n    &gt;&gt;&gt; print(f\"Chi-square: {result.stat_value:.3f}, df={result.df}\")\n    &gt;&gt;&gt; print(f\"p-value: {result.p_value:.4f}\")\n    \"\"\"\n    # Validate\n    if not 0 &lt; conf_level &lt; 1:\n        raise ValueError(\"conf_level must be between 0 and 1\")\n\n    # Get data table\n    data = dprime_table(correct, total, protocol)\n    n_groups = len(data)\n\n    # Estimate common d-prime and get test result for conf interval\n    d_exp, se_exp, _ = _dprime_estim(data, estim)\n\n    # Get confidence interval from dprime_test\n    test_result = dprime_test(\n        correct, total, protocol,\n        conf_level=conf_level,\n        dprime0=0,\n        statistic=\"likelihood\",\n        estim=estim,\n    )\n\n    # Compute chi-square test statistic\n    x = np.array([row.correct for row in data])\n    n = np.array([row.total for row in data])\n    O = np.concatenate([x, n - x])\n\n    protocols = [row.protocol for row in data]\n\n    if statistic == \"likelihood\":\n        # Expected counts under common d-prime\n        # psy_fun returns arrays, so we extract scalar values\n        p_exp = np.array([psy_fun(d_exp, p).item() for p in protocols])\n        E = np.concatenate([n * p_exp, n * (1 - p_exp)])\n        # Avoid log(0)\n        O_safe = np.maximum(O, 1e-10)\n        E_safe = np.maximum(E, 1e-10)\n        X = 2 * np.sum(O_safe * np.log(O_safe / E_safe))\n\n    elif statistic == \"Pearson\":\n        p_exp = np.array([psy_fun(d_exp, p).item() for p in protocols])\n        E = np.concatenate([n * p_exp, n * (1 - p_exp)])\n        E_safe = np.maximum(E, 1e-10)\n        X = np.sum((O - E) ** 2 / E_safe)\n\n    elif statistic == \"Wald.p\":\n        p_exp = np.array([psy_fun(d_exp, p).item() for p in protocols])\n        p_hat = np.array([row.p_hat for row in data])\n        var_p = p_hat * (1 - p_hat) / n\n        var_p_safe = np.maximum(var_p, 1e-10)\n        X = np.sum((p_hat - p_exp) ** 2 / var_p_safe)\n\n    elif statistic == \"Wald.d\":\n        dprimes = np.array([row.dprime for row in data])\n        se_dprimes = np.array([row.se_dprime for row in data])\n        if not np.all(np.isfinite(np.concatenate([dprimes, se_dprimes]))):\n            raise ValueError(\n                \"Boundary cases occurred: use 'likelihood' or 'Pearson' instead\"\n            )\n        X = np.sum(((dprimes - d_exp) / se_dprimes) ** 2)\n\n    else:\n        raise ValueError(f\"Unknown statistic: {statistic}\")\n\n    # Degrees of freedom\n    df = n_groups - 1\n\n    # P-value from chi-square distribution\n    if df &gt;= 1:\n        from scipy.stats import chi2\n        p_value = chi2.sf(X, df)\n    else:\n        p_value = np.nan\n\n    return DprimeCompareResult(\n        d_prime=d_exp,\n        se_d_prime=se_exp,\n        conf_int=test_result.conf_int,\n        conf_level=conf_level,\n        conf_method=test_result.conf_method,\n        stat_value=X,\n        df=df,\n        p_value=p_value,\n        statistic=statistic,\n        estim=estim,\n        data=data,\n    )\n</code></pre>"},{"location":"api/power/#posthoc","title":"posthoc","text":""},{"location":"api/power/#senspy.posthoc","title":"<code>posthoc(result: DprimeCompareResult | DprimeTestResult, alpha: float = 0.05, test: Literal['pairwise', 'common', 'base', 'zero'] | float = 'pairwise', base: int = 1, alternative: Literal['two.sided', 'less', 'greater'] = 'two.sided', statistic: Literal['likelihood', 'Wald'] = 'likelihood', padj_method: Literal['holm', 'bonferroni', 'none'] = 'holm') -&gt; PosthocResult</code>","text":"<p>Perform post-hoc comparisons of d-primes.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>DprimeCompareResult | DprimeTestResult</code> <p>Result from dprime_compare() or dprime_test().</p> required <code>alpha</code> <code>float</code> <p>Significance level for letter display. Default is 0.05.</p> <code>0.05</code> <code>test</code> <code>str | float</code> <p>Type of comparison: - \"pairwise\": All pairwise differences - \"common\": Compare each to common d-prime - \"base\": Compare each to base group - \"zero\": Compare each to zero - float: Compare each to this value</p> <code>'pairwise'</code> <code>base</code> <code>int</code> <p>Index of base group (1-indexed) for \"base\" test. Default is 1.</p> <code>1</code> <code>alternative</code> <code>str</code> <p>Alternative hypothesis. Default is \"two.sided\".</p> <code>'two.sided'</code> <code>statistic</code> <code>str</code> <p>Test statistic. Default is \"likelihood\".</p> <code>'likelihood'</code> <code>padj_method</code> <code>str</code> <p>P-value adjustment method. Default is \"holm\".</p> <code>'holm'</code> <p>Returns:</p> Type Description <code>PosthocResult</code> <p>Post-hoc comparison results.</p> Source code in <code>senspy/dprime_tests.py</code> <pre><code>def posthoc(\n    result: DprimeCompareResult | DprimeTestResult,\n    alpha: float = 0.05,\n    test: Literal[\"pairwise\", \"common\", \"base\", \"zero\"] | float = \"pairwise\",\n    base: int = 1,\n    alternative: Literal[\"two.sided\", \"less\", \"greater\"] = \"two.sided\",\n    statistic: Literal[\"likelihood\", \"Wald\"] = \"likelihood\",\n    padj_method: Literal[\"holm\", \"bonferroni\", \"none\"] = \"holm\",\n) -&gt; PosthocResult:\n    \"\"\"Perform post-hoc comparisons of d-primes.\n\n    Parameters\n    ----------\n    result : DprimeCompareResult | DprimeTestResult\n        Result from dprime_compare() or dprime_test().\n    alpha : float\n        Significance level for letter display. Default is 0.05.\n    test : str | float\n        Type of comparison:\n        - \"pairwise\": All pairwise differences\n        - \"common\": Compare each to common d-prime\n        - \"base\": Compare each to base group\n        - \"zero\": Compare each to zero\n        - float: Compare each to this value\n    base : int\n        Index of base group (1-indexed) for \"base\" test. Default is 1.\n    alternative : str\n        Alternative hypothesis. Default is \"two.sided\".\n    statistic : str\n        Test statistic. Default is \"likelihood\".\n    padj_method : str\n        P-value adjustment method. Default is \"holm\".\n\n    Returns\n    -------\n    PosthocResult\n        Post-hoc comparison results.\n    \"\"\"\n    data = result.data\n    n = len(data)\n\n    # Determine dprime0 for testing\n    if isinstance(test, (int, float)) and not isinstance(test, bool):\n        dprime0 = float(test)\n        test_type = \"value\"\n    elif test == \"zero\":\n        dprime0 = 0.0\n        test_type = \"zero\"\n    elif test == \"base\":\n        dprime0 = data[base - 1].dprime  # 1-indexed\n        test_type = \"base\"\n    else:\n        dprime0 = 0.0\n        test_type = test\n\n    posthoc_results = []\n    stat_values = []\n\n    if test_type in [\"pairwise\", \"base\"]:\n        # Pairwise or Dunnett-style comparisons\n        if test_type == \"pairwise\":\n            # All pairs\n            pairs = [(i, j) for i in range(n) for j in range(i + 1, n)]\n        else:\n            # Compare to base\n            pairs = [(base - 1, j) for j in range(n) if j != base - 1]\n\n        for i, j in pairs:\n            diff = data[i].dprime - data[j].dprime\n            se_diff = np.sqrt(data[i].se_dprime**2 + data[j].se_dprime**2)\n\n            if statistic == \"likelihood\":\n                # Likelihood ratio for pairwise test\n                # Fit common model to just these two groups\n                sub_correct = [data[i].correct, data[j].correct]\n                sub_total = [data[i].total, data[j].total]\n                sub_protocol = [data[i].protocol, data[j].protocol]\n                sub_data = dprime_table(sub_correct, sub_total, sub_protocol)\n\n                # NLL under null (common d-prime)\n                nll_0 = optimize.minimize_scalar(\n                    lambda dp: _dprime_nll(dp, sub_data),\n                    bounds=(0, 10),\n                    method=\"bounded\",\n                ).fun\n\n                # NLL under alternative (separate d-primes)\n                # Use safe log-likelihood to handle perfect scores\n                nll_i = -_safe_binomial_loglik(\n                    data[i].correct, data[i].total, data[i].p_hat\n                )\n                nll_j = -_safe_binomial_loglik(\n                    data[j].correct, data[j].total, data[j].p_hat\n                )\n                nll_alt = nll_i + nll_j\n\n                LR = -2 * (nll_alt - nll_0)\n                stat = np.sign(diff) * np.sqrt(abs(LR))\n            else:\n                stat = diff / se_diff if np.isfinite(se_diff) else np.nan\n\n            stat_values.append(stat)\n            name = f\"group{i+1} - group{j+1}\"\n            posthoc_results.append({\n                \"name\": name,\n                \"estimate\": diff,\n                \"se\": se_diff,\n                \"stat_value\": stat,\n            })\n\n    else:\n        # Compare each group to dprime0 (common, zero, or value)\n        for i in range(n):\n            diff = data[i].dprime - dprime0\n            se = data[i].se_dprime\n\n            if statistic == \"likelihood\" and test_type == \"common\":\n                # Compare to common d-prime using likelihood\n                sub_data = [data[i]]\n                d_common = result.d_prime\n                nll_0 = _dprime_nll(d_common, sub_data)\n                nll_alt = _dprime_nll(data[i].dprime, sub_data)\n                LR = -2 * (nll_alt - nll_0)\n                stat = np.sign(diff) * np.sqrt(abs(LR))\n            else:\n                stat = diff / se if np.isfinite(se) else np.nan\n\n            stat_values.append(stat)\n            posthoc_results.append({\n                \"name\": f\"group{i+1}\",\n                \"estimate\": data[i].dprime,\n                \"se\": se,\n                \"stat_value\": stat,\n            })\n\n    # Compute p-values\n    pvals = np.array([normal_pvalue(s, alternative) for s in stat_values])\n    adjusted_pvals = _p_adjust(pvals, padj_method)\n\n    for i, res in enumerate(posthoc_results):\n        res[\"p_value\"] = adjusted_pvals[i]\n\n    # Generate letter display for pairwise comparisons\n    letters = None\n    if test_type == \"pairwise\" and alternative == \"two.sided\":\n        signifs = {\n            res[\"name\"]: res[\"p_value\"] &lt; alpha for res in posthoc_results\n        }\n        letters = _get_letters(signifs)\n\n    return PosthocResult(\n        posthoc=posthoc_results,\n        test=test_type,\n        alternative=alternative,\n        padj_method=padj_method,\n        letters=letters,\n        base_result=result,\n    )\n</code></pre>"},{"location":"api/power/#dprime_table","title":"dprime_table","text":""},{"location":"api/power/#senspy.dprime_table","title":"<code>dprime_table(correct: ArrayLike, total: ArrayLike, protocol: ArrayLike, restrict_above_guess: bool = True) -&gt; list[DprimeTableRow]</code>","text":"<p>Create a table of d-prime estimates for each group.</p> <p>Parameters:</p> Name Type Description Default <code>correct</code> <code>ArrayLike</code> <p>Number of correct responses in each group.</p> required <code>total</code> <code>ArrayLike</code> <p>Total number of trials in each group.</p> required <code>protocol</code> <code>ArrayLike</code> <p>Protocol name for each group.</p> required <code>restrict_above_guess</code> <code>bool</code> <p>If True, restrict p_hat to be at least p_guess. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[DprimeTableRow]</code> <p>List of rows with per-group estimates.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid.</p> Source code in <code>senspy/dprime_tests.py</code> <pre><code>def dprime_table(\n    correct: ArrayLike,\n    total: ArrayLike,\n    protocol: ArrayLike,\n    restrict_above_guess: bool = True,\n) -&gt; list[DprimeTableRow]:\n    \"\"\"Create a table of d-prime estimates for each group.\n\n    Parameters\n    ----------\n    correct : ArrayLike\n        Number of correct responses in each group.\n    total : ArrayLike\n        Total number of trials in each group.\n    protocol : ArrayLike\n        Protocol name for each group.\n    restrict_above_guess : bool\n        If True, restrict p_hat to be at least p_guess. Default is True.\n\n    Returns\n    -------\n    list[DprimeTableRow]\n        List of rows with per-group estimates.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid.\n    \"\"\"\n    correct = np.asarray(correct, dtype=int)\n    total = np.asarray(total, dtype=int)\n    protocol = np.asarray(protocol, dtype=str)\n\n    # Validate inputs\n    if not (len(correct) == len(total) == len(protocol)):\n        raise ValueError(\"correct, total, and protocol must have the same length\")\n    if len(correct) &lt; 1:\n        raise ValueError(\"Need at least one group\")\n    if np.any(correct &lt; 0) or np.any(total &lt;= 0):\n        raise ValueError(\"Invalid counts: need correct &gt;= 0 and total &gt; 0\")\n    if np.any(correct &gt; total):\n        raise ValueError(\"correct cannot exceed total\")\n\n    valid_protocols = [\n        \"triangle\", \"duotrio\", \"threeAFC\", \"twoAFC\", \"tetrad\",\n        \"hexad\", \"twofive\", \"twofiveF\"\n    ]\n    for p in protocol:\n        if p not in valid_protocols:\n            raise ValueError(f\"Invalid protocol: {p}. Must be one of {valid_protocols}\")\n\n    rows = []\n    for i in range(len(correct)):\n        x = correct[i]\n        n = total[i]\n        prot = protocol[i]\n        p_guess = _get_p_guess(prot)\n\n        # Compute p_hat\n        p_hat = x / n\n        if restrict_above_guess and p_hat &lt; p_guess:\n            p_hat = p_guess\n\n        se_p_hat = np.sqrt(p_hat * (1 - p_hat) / n)\n\n        # Compute d-prime\n        dprime = psy_inv(p_hat, prot)\n\n        # Compute se(d-prime) using delta method\n        deriv = psy_deriv(dprime, prot)\n        if np.isfinite(deriv) and deriv &gt; 0:\n            se_dprime = se_p_hat / deriv\n        else:\n            se_dprime = np.nan\n\n        rows.append(\n            DprimeTableRow(\n                correct=x,\n                total=n,\n                protocol=prot,\n                p_hat=p_hat,\n                se_p_hat=se_p_hat,\n                dprime=dprime,\n                se_dprime=se_dprime,\n            )\n        )\n\n    return rows\n</code></pre>"},{"location":"api/power/#result-classes","title":"Result Classes","text":""},{"location":"api/power/#dprimetestresult","title":"DprimeTestResult","text":""},{"location":"api/power/#senspy.DprimeTestResult","title":"<code>DprimeTestResult(d_prime: float, se_d_prime: float, conf_int: tuple[float, float], conf_level: float, conf_method: str, stat_value: float, p_value: float, statistic: str, alternative: str, dprime0: float, estim: str, data: list[DprimeTableRow])</code>  <code>dataclass</code>","text":"<p>Result from dprime_test function.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated common d-prime.</p> <code>se_d_prime</code> <code>float</code> <p>Standard error of the common d-prime.</p> <code>conf_int</code> <code>tuple[float, float]</code> <p>Confidence interval for common d-prime.</p> <code>conf_level</code> <code>float</code> <p>Confidence level used.</p> <code>conf_method</code> <code>str</code> <p>Method used for confidence interval.</p> <code>stat_value</code> <code>float</code> <p>Value of the test statistic.</p> <code>p_value</code> <code>float</code> <p>P-value for the hypothesis test.</p> <code>statistic</code> <code>str</code> <p>Type of test statistic used.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis.</p> <code>dprime0</code> <code>float</code> <p>Null hypothesis value for d-prime.</p> <code>estim</code> <code>str</code> <p>Estimation method used (\"ML\" or \"weighted_avg\").</p> <code>data</code> <code>list[DprimeTableRow]</code> <p>The input data with per-group estimates.</p>"},{"location":"api/power/#dprimecompareresult","title":"DprimeCompareResult","text":""},{"location":"api/power/#senspy.DprimeCompareResult","title":"<code>DprimeCompareResult(d_prime: float, se_d_prime: float, conf_int: tuple[float, float], conf_level: float, conf_method: str, stat_value: float, df: int, p_value: float, statistic: str, estim: str, data: list[DprimeTableRow])</code>  <code>dataclass</code>","text":"<p>Result from dprime_compare function.</p> <p>Attributes:</p> Name Type Description <code>d_prime</code> <code>float</code> <p>Estimated common d-prime.</p> <code>se_d_prime</code> <code>float</code> <p>Standard error of the common d-prime.</p> <code>conf_int</code> <code>tuple[float, float]</code> <p>Confidence interval for common d-prime.</p> <code>conf_level</code> <code>float</code> <p>Confidence level used.</p> <code>conf_method</code> <code>str</code> <p>Method used for confidence interval.</p> <code>stat_value</code> <code>float</code> <p>Chi-square test statistic.</p> <code>df</code> <code>int</code> <p>Degrees of freedom.</p> <code>p_value</code> <code>float</code> <p>P-value for the any-difference test.</p> <code>statistic</code> <code>str</code> <p>Type of test statistic used.</p> <code>estim</code> <code>str</code> <p>Estimation method used.</p> <code>data</code> <code>list[DprimeTableRow]</code> <p>The input data with per-group estimates.</p>"},{"location":"api/power/#posthocresult","title":"PosthocResult","text":""},{"location":"api/power/#senspy.PosthocResult","title":"<code>PosthocResult(posthoc: list[dict], test: str, alternative: str, padj_method: str, letters: dict[str, str] | None, base_result: DprimeCompareResult | DprimeTestResult)</code>  <code>dataclass</code>","text":"<p>Result from posthoc function.</p> <p>Attributes:</p> Name Type Description <code>posthoc</code> <code>list[dict]</code> <p>Post-hoc comparison results.</p> <code>test</code> <code>str</code> <p>Type of post-hoc test performed.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis.</p> <code>padj_method</code> <code>str</code> <p>P-value adjustment method.</p> <code>letters</code> <code>dict[str, str] | None</code> <p>Letter display for pairwise comparisons (if applicable).</p> <code>base_result</code> <code>DprimeCompareResult | DprimeTestResult</code> <p>The original comparison result.</p>"},{"location":"api/power/#dodpowerresult","title":"DODPowerResult","text":""},{"location":"api/power/#senspy.DODPowerResult","title":"<code>DODPowerResult(power: float, se_power: float, n_used: int, d_primeA: float, d_prime0: float, sample_size: tuple[int, int], nsim: int, alpha: float, statistic: str, alternative: str, tau: NDArray)</code>  <code>dataclass</code>","text":"<p>Result from dod_power function.</p> <p>Attributes:</p> Name Type Description <code>power</code> <code>float</code> <p>Estimated power (proportion of simulations with p &lt; alpha).</p> <code>se_power</code> <code>float</code> <p>Standard error of the power estimate.</p> <code>n_used</code> <code>int</code> <p>Number of simulations used (excluding failures).</p> <code>d_primeA</code> <code>float</code> <p>True d-prime value (alternative hypothesis).</p> <code>d_prime0</code> <code>float</code> <p>Null hypothesis d-prime.</p> <code>sample_size</code> <code>tuple[int, int]</code> <p>Sample sizes (same_pairs, diff_pairs).</p> <code>nsim</code> <code>int</code> <p>Total number of simulations requested.</p> <code>alpha</code> <code>float</code> <p>Significance level.</p> <code>statistic</code> <code>str</code> <p>Test statistic used.</p> <code>alternative</code> <code>str</code> <p>Alternative hypothesis.</p> <code>tau</code> <code>NDArray</code> <p>Boundary parameters used for simulation.</p>"},{"location":"api/roc/","title":"ROC &amp; Signal Detection","text":""},{"location":"api/roc/#functions","title":"Functions","text":""},{"location":"api/roc/#sdt","title":"sdt","text":"<p>Signal Detection Theory transform for rating scale data.</p>"},{"location":"api/roc/#senspy.sdt","title":"<code>sdt(table: ArrayLike, method: Literal['probit', 'logit'] = 'probit') -&gt; NDArray[np.float64]</code>","text":"<p>Compute Signal Detection Theory d-prime from a contingency table.</p> <p>Performs the empirical probit or logit transform on a 2 x J table of observations to compute d-prime values at each response criterion.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>array_like</code> <p>A 2 x J matrix where rows represent signal/noise conditions and columns represent J ordered response categories. Row 1 is typically the \"signal\" distribution, row 2 is the \"noise\" distribution.</p> required <code>method</code> <code>('probit', 'logit')</code> <p>The transform method to use: - \"probit\": Uses the normal quantile function (qnorm) - \"logit\": Uses the logistic transform</p> <code>\"probit\"</code> <p>Returns:</p> Name Type Description <code>result</code> <code>ndarray</code> <p>A (J-1) x 3 matrix with columns: - z(Hit rate): Transformed cumulative hit rates - z(False alarm rate): Transformed cumulative false alarm rates - d-prime: The difference z(Hit) - z(FA) at each criterion</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If table is not a 2-row matrix or method is not recognized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from senspy import sdt\n&gt;&gt;&gt; # Example: rating scale data with 5 categories\n&gt;&gt;&gt; table = np.array([[10, 20, 30, 25, 15],   # Signal trials\n...                   [30, 25, 20, 15, 10]])  # Noise trials\n&gt;&gt;&gt; result = sdt(table)\n&gt;&gt;&gt; print(result)\n</code></pre> Notes <p>The d-prime at each criterion represents the separation between the signal and noise distributions at that point on the ROC curve.</p> Source code in <code>senspy/roc.py</code> <pre><code>def sdt(\n    table: ArrayLike,\n    method: Literal[\"probit\", \"logit\"] = \"probit\",\n) -&gt; NDArray[np.float64]:\n    \"\"\"Compute Signal Detection Theory d-prime from a contingency table.\n\n    Performs the empirical probit or logit transform on a 2 x J table of\n    observations to compute d-prime values at each response criterion.\n\n    Parameters\n    ----------\n    table : array_like\n        A 2 x J matrix where rows represent signal/noise conditions and\n        columns represent J ordered response categories. Row 1 is typically\n        the \"signal\" distribution, row 2 is the \"noise\" distribution.\n    method : {\"probit\", \"logit\"}, default \"probit\"\n        The transform method to use:\n        - \"probit\": Uses the normal quantile function (qnorm)\n        - \"logit\": Uses the logistic transform\n\n    Returns\n    -------\n    result : ndarray\n        A (J-1) x 3 matrix with columns:\n        - z(Hit rate): Transformed cumulative hit rates\n        - z(False alarm rate): Transformed cumulative false alarm rates\n        - d-prime: The difference z(Hit) - z(FA) at each criterion\n\n    Raises\n    ------\n    ValueError\n        If table is not a 2-row matrix or method is not recognized.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from senspy import sdt\n    &gt;&gt;&gt; # Example: rating scale data with 5 categories\n    &gt;&gt;&gt; table = np.array([[10, 20, 30, 25, 15],   # Signal trials\n    ...                   [30, 25, 20, 15, 10]])  # Noise trials\n    &gt;&gt;&gt; result = sdt(table)\n    &gt;&gt;&gt; print(result)\n\n    Notes\n    -----\n    The d-prime at each criterion represents the separation between the\n    signal and noise distributions at that point on the ROC curve.\n    \"\"\"\n    table = np.asarray(table, dtype=np.float64)\n\n    if table.ndim != 2 or table.shape[0] != 2:\n        raise ValueError(\"table must be a 2 x J matrix (2 rows, J columns)\")\n\n    if method not in (\"probit\", \"logit\"):\n        raise ValueError(f\"method '{method}' not recognized; use 'probit' or 'logit'\")\n\n    def transform_row(x: NDArray) -&gt; NDArray:\n        \"\"\"Apply cumulative transform to a row.\"\"\"\n        cs = np.cumsum(x)\n        total = cs[-1]\n\n        if method == \"probit\":\n            # Cumulative proportions (excluding last point which is always 1)\n            cp = cs[:-1] / total\n            return stats.norm.ppf(cp)\n        else:  # logit\n            return np.log(cs[:-1] / (total - cs[:-1]))\n\n    # Apply transform to each row\n    z_signal = transform_row(table[0, :])  # z(Hit rate)\n    z_noise = transform_row(table[1, :])  # z(False alarm rate)\n\n    # Compute d-prime at each criterion\n    d_prime = z_signal - z_noise\n\n    # Build result matrix\n    n_criteria = table.shape[1] - 1\n    result = np.column_stack([z_signal, z_noise, d_prime])\n\n    return result\n</code></pre>"},{"location":"api/roc/#roc","title":"roc","text":"<p>Compute ROC curve from d-prime.</p>"},{"location":"api/roc/#senspy.roc","title":"<code>roc</code>","text":"<p>ROC curves, AUC, and Signal Detection Theory functions.</p> <p>This module provides functions for: - SDT (Signal Detection Theory) d-prime computation from contingency tables - ROC (Receiver Operating Characteristic) curve generation - AUC (Area Under the Curve) computation with confidence intervals</p> References <p>Macmillan, N.A. &amp; Creelman, C.D. (2005). Detection Theory: A User's Guide.     2nd ed. Lawrence Erlbaum Associates.</p>"},{"location":"api/roc/#senspy.roc.AUCResult","title":"<code>AUCResult(value: float, lower: float | None = None, upper: float | None = None, ci_alpha: float | None = None)</code>  <code>dataclass</code>","text":"<p>Result from AUC computation.</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>float</code> <p>The Area Under the ROC Curve.</p> <code>lower</code> <code>float or None</code> <p>Lower bound of confidence interval (if se_d provided).</p> <code>upper</code> <code>float or None</code> <p>Upper bound of confidence interval (if se_d provided).</p> <code>ci_alpha</code> <code>float or None</code> <p>Alpha level used for confidence interval.</p>"},{"location":"api/roc/#senspy.roc.ROCResult","title":"<code>ROCResult(fpr: NDArray[np.float64], tpr: NDArray[np.float64], lower: NDArray[np.float64] | None = None, upper: NDArray[np.float64] | None = None, d_prime: float = 0.0, scale: float = 1.0)</code>  <code>dataclass</code>","text":"<p>Result from ROC curve computation.</p> <p>Attributes:</p> Name Type Description <code>fpr</code> <code>ndarray</code> <p>False positive rates (x-axis of ROC curve).</p> <code>tpr</code> <code>ndarray</code> <p>True positive rates (y-axis of ROC curve).</p> <code>lower</code> <code>ndarray or None</code> <p>Lower confidence bound for TPR (if se_d provided).</p> <code>upper</code> <code>ndarray or None</code> <p>Upper confidence bound for TPR (if se_d provided).</p> <code>d_prime</code> <code>float</code> <p>The d-prime value used.</p> <code>scale</code> <code>float</code> <p>The scale parameter used.</p>"},{"location":"api/roc/#senspy.roc.auc","title":"<code>auc(d: float, se_d: float | None = None, scale: float = 1.0, ci_alpha: float = 0.05) -&gt; AUCResult</code>","text":"<p>Compute Area Under the ROC Curve from d-prime.</p> <p>Calculates the AUC analytically from the d-prime value, optionally with confidence intervals based on the standard error.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>float</code> <p>The d-prime (discriminability) value. Can be negative.</p> required <code>se_d</code> <code>float</code> <p>Standard error of d-prime. If provided, confidence intervals are computed.</p> <code>None</code> <code>scale</code> <code>float</code> <p>Scale parameter (ratio of noise to signal standard deviation). Must be positive.</p> <code>1.0</code> <code>ci_alpha</code> <code>float</code> <p>Significance level for confidence interval (e.g., 0.05 for 95% CI).</p> <code>0.05</code> <p>Returns:</p> Type Description <code>AUCResult</code> <p>Result containing AUC value and optional confidence bounds.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy import auc\n&gt;&gt;&gt; result = auc(d=1.5, se_d=0.2)\n&gt;&gt;&gt; print(f\"AUC: {result.value:.3f}\")\nAUC: 0.856\n&gt;&gt;&gt; print(f\"95% CI: [{result.lower:.3f}, {result.upper:.3f}]\")\n95% CI: [0.793, 0.905]\n</code></pre> Notes <p>The AUC is computed as:     AUC = \u03a6(d / \u221a(1 + scale\u00b2))</p> <p>where \u03a6 is the standard normal CDF. This formula assumes equal-variance Gaussian signal detection theory when scale=1.</p> <p>For unequal variance models (scale \u2260 1), the ROC curve is asymmetric and the formula accounts for the variance ratio.</p> Source code in <code>senspy/roc.py</code> <pre><code>def auc(\n    d: float,\n    se_d: float | None = None,\n    scale: float = 1.0,\n    ci_alpha: float = 0.05,\n) -&gt; AUCResult:\n    \"\"\"Compute Area Under the ROC Curve from d-prime.\n\n    Calculates the AUC analytically from the d-prime value, optionally\n    with confidence intervals based on the standard error.\n\n    Parameters\n    ----------\n    d : float\n        The d-prime (discriminability) value. Can be negative.\n    se_d : float, optional\n        Standard error of d-prime. If provided, confidence intervals\n        are computed.\n    scale : float, default 1.0\n        Scale parameter (ratio of noise to signal standard deviation).\n        Must be positive.\n    ci_alpha : float, default 0.05\n        Significance level for confidence interval (e.g., 0.05 for 95% CI).\n\n    Returns\n    -------\n    AUCResult\n        Result containing AUC value and optional confidence bounds.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from senspy import auc\n    &gt;&gt;&gt; result = auc(d=1.5, se_d=0.2)\n    &gt;&gt;&gt; print(f\"AUC: {result.value:.3f}\")\n    AUC: 0.856\n    &gt;&gt;&gt; print(f\"95% CI: [{result.lower:.3f}, {result.upper:.3f}]\")\n    95% CI: [0.793, 0.905]\n\n    Notes\n    -----\n    The AUC is computed as:\n        AUC = \u03a6(d / \u221a(1 + scale\u00b2))\n\n    where \u03a6 is the standard normal CDF. This formula assumes equal-variance\n    Gaussian signal detection theory when scale=1.\n\n    For unequal variance models (scale \u2260 1), the ROC curve is asymmetric\n    and the formula accounts for the variance ratio.\n    \"\"\"\n    # Validate inputs\n    if not np.isfinite(d):\n        raise ValueError(\"d must be a finite number\")\n    if scale &lt;= 0:\n        raise ValueError(\"scale must be positive\")\n    if not (0 &lt; ci_alpha &lt; 1):\n        raise ValueError(\"ci_alpha must be between 0 and 1\")\n\n    if se_d is not None:\n        if not np.isfinite(se_d) or se_d &lt; 0:\n            raise ValueError(\"se_d must be a non-negative finite number\")\n\n    # Compute AUC\n    scale_factor = np.sqrt(1 + scale**2)\n    auc_value = float(stats.norm.cdf(d / scale_factor))\n\n    # Compute confidence interval if se_d provided\n    lower = None\n    upper = None\n    result_ci_alpha = None\n\n    if se_d is not None:\n        tol = se_d * stats.norm.ppf(1 - ci_alpha / 2)\n        lower = float(stats.norm.cdf((d - tol) / scale_factor))\n        upper = float(stats.norm.cdf((d + tol) / scale_factor))\n        result_ci_alpha = ci_alpha\n\n    return AUCResult(\n        value=auc_value,\n        lower=lower,\n        upper=upper,\n        ci_alpha=result_ci_alpha,\n    )\n</code></pre>"},{"location":"api/roc/#senspy.roc.roc","title":"<code>roc(d: float, se_d: float | None = None, scale: float = 1.0, n_points: int = 1000, se_type: Literal['CI', 'SE'] = 'CI', ci_alpha: float = 0.05) -&gt; ROCResult</code>","text":"<p>Compute ROC curve from d-prime.</p> <p>Generates the theoretical ROC curve for a given d-prime value, optionally with confidence bands.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>float</code> <p>The d-prime (discriminability) value.</p> required <code>se_d</code> <code>float</code> <p>Standard error of d-prime. If provided, confidence bands are computed.</p> <code>None</code> <code>scale</code> <code>float</code> <p>Scale parameter (ratio of noise to signal standard deviation).</p> <code>1.0</code> <code>n_points</code> <code>int</code> <p>Number of points to generate on the ROC curve.</p> <code>1000</code> <code>se_type</code> <code>('CI', 'SE')</code> <p>Type of error band: - \"CI\": Confidence interval (se_d * z_alpha) - \"SE\": Standard error (se_d)</p> <code>\"CI\"</code> <code>ci_alpha</code> <code>float</code> <p>Significance level for confidence interval (only used if se_type=\"CI\").</p> <code>0.05</code> <p>Returns:</p> Type Description <code>ROCResult</code> <p>Result containing FPR, TPR arrays and optional confidence bounds.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy import roc\n&gt;&gt;&gt; result = roc(d=1.5, se_d=0.2)\n&gt;&gt;&gt; print(f\"AUC \u2248 {np.trapz(result.tpr, result.fpr):.3f}\")\n</code></pre> Notes <p>The ROC curve is computed using:     TPR = \u03a6((\u03a6\u207b\u00b9(FPR) + d) / scale)</p> <p>where \u03a6 is the standard normal CDF and \u03a6\u207b\u00b9 is the quantile function.</p> <p>For equal-variance SDT (scale=1), this simplifies to:     TPR = \u03a6(\u03a6\u207b\u00b9(FPR) + d)</p> Source code in <code>senspy/roc.py</code> <pre><code>def roc(\n    d: float,\n    se_d: float | None = None,\n    scale: float = 1.0,\n    n_points: int = 1000,\n    se_type: Literal[\"CI\", \"SE\"] = \"CI\",\n    ci_alpha: float = 0.05,\n) -&gt; ROCResult:\n    \"\"\"Compute ROC curve from d-prime.\n\n    Generates the theoretical ROC curve for a given d-prime value,\n    optionally with confidence bands.\n\n    Parameters\n    ----------\n    d : float\n        The d-prime (discriminability) value.\n    se_d : float, optional\n        Standard error of d-prime. If provided, confidence bands\n        are computed.\n    scale : float, default 1.0\n        Scale parameter (ratio of noise to signal standard deviation).\n    n_points : int, default 1000\n        Number of points to generate on the ROC curve.\n    se_type : {\"CI\", \"SE\"}, default \"CI\"\n        Type of error band:\n        - \"CI\": Confidence interval (se_d * z_alpha)\n        - \"SE\": Standard error (se_d)\n    ci_alpha : float, default 0.05\n        Significance level for confidence interval (only used if se_type=\"CI\").\n\n    Returns\n    -------\n    ROCResult\n        Result containing FPR, TPR arrays and optional confidence bounds.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from senspy import roc\n    &gt;&gt;&gt; result = roc(d=1.5, se_d=0.2)\n    &gt;&gt;&gt; print(f\"AUC \u2248 {np.trapz(result.tpr, result.fpr):.3f}\")\n\n    Notes\n    -----\n    The ROC curve is computed using:\n        TPR = \u03a6((\u03a6\u207b\u00b9(FPR) + d) / scale)\n\n    where \u03a6 is the standard normal CDF and \u03a6\u207b\u00b9 is the quantile function.\n\n    For equal-variance SDT (scale=1), this simplifies to:\n        TPR = \u03a6(\u03a6\u207b\u00b9(FPR) + d)\n    \"\"\"\n    # Validate inputs\n    if not np.isfinite(d):\n        raise ValueError(\"d must be a finite number\")\n    if scale &lt;= 0:\n        raise ValueError(\"scale must be positive\")\n    if n_points &lt; 2:\n        raise ValueError(\"n_points must be at least 2\")\n    if se_type not in (\"CI\", \"SE\"):\n        raise ValueError(\"se_type must be 'CI' or 'SE'\")\n\n    if se_d is not None:\n        if not np.isfinite(se_d) or se_d &lt; 0:\n            raise ValueError(\"se_d must be a non-negative finite number\")\n\n    # Generate FPR values\n    fpr = np.linspace(0, 1, n_points)\n\n    # Compute z-scores of FPR (avoiding infinities at 0 and 1)\n    # Use small offset to avoid -inf and +inf\n    fpr_safe = np.clip(fpr, 1e-10, 1 - 1e-10)\n    z_fpr = stats.norm.ppf(fpr_safe)\n\n    # Compute TPR\n    tpr = stats.norm.cdf((z_fpr + d) / scale)\n\n    # Compute confidence bands if se_d provided\n    lower = None\n    upper = None\n\n    if se_d is not None:\n        if se_type == \"CI\":\n            tol = se_d * stats.norm.ppf(1 - ci_alpha / 2)\n        else:  # SE\n            tol = se_d\n\n        lower = stats.norm.cdf((z_fpr + d - tol) / scale)\n        upper = stats.norm.cdf((z_fpr + d + tol) / scale)\n\n    return ROCResult(\n        fpr=fpr,\n        tpr=tpr,\n        lower=lower,\n        upper=upper,\n        d_prime=d,\n        scale=scale,\n    )\n</code></pre>"},{"location":"api/roc/#senspy.roc.sdt","title":"<code>sdt(table: ArrayLike, method: Literal['probit', 'logit'] = 'probit') -&gt; NDArray[np.float64]</code>","text":"<p>Compute Signal Detection Theory d-prime from a contingency table.</p> <p>Performs the empirical probit or logit transform on a 2 x J table of observations to compute d-prime values at each response criterion.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>array_like</code> <p>A 2 x J matrix where rows represent signal/noise conditions and columns represent J ordered response categories. Row 1 is typically the \"signal\" distribution, row 2 is the \"noise\" distribution.</p> required <code>method</code> <code>('probit', 'logit')</code> <p>The transform method to use: - \"probit\": Uses the normal quantile function (qnorm) - \"logit\": Uses the logistic transform</p> <code>\"probit\"</code> <p>Returns:</p> Name Type Description <code>result</code> <code>ndarray</code> <p>A (J-1) x 3 matrix with columns: - z(Hit rate): Transformed cumulative hit rates - z(False alarm rate): Transformed cumulative false alarm rates - d-prime: The difference z(Hit) - z(FA) at each criterion</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If table is not a 2-row matrix or method is not recognized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from senspy import sdt\n&gt;&gt;&gt; # Example: rating scale data with 5 categories\n&gt;&gt;&gt; table = np.array([[10, 20, 30, 25, 15],   # Signal trials\n...                   [30, 25, 20, 15, 10]])  # Noise trials\n&gt;&gt;&gt; result = sdt(table)\n&gt;&gt;&gt; print(result)\n</code></pre> Notes <p>The d-prime at each criterion represents the separation between the signal and noise distributions at that point on the ROC curve.</p> Source code in <code>senspy/roc.py</code> <pre><code>def sdt(\n    table: ArrayLike,\n    method: Literal[\"probit\", \"logit\"] = \"probit\",\n) -&gt; NDArray[np.float64]:\n    \"\"\"Compute Signal Detection Theory d-prime from a contingency table.\n\n    Performs the empirical probit or logit transform on a 2 x J table of\n    observations to compute d-prime values at each response criterion.\n\n    Parameters\n    ----------\n    table : array_like\n        A 2 x J matrix where rows represent signal/noise conditions and\n        columns represent J ordered response categories. Row 1 is typically\n        the \"signal\" distribution, row 2 is the \"noise\" distribution.\n    method : {\"probit\", \"logit\"}, default \"probit\"\n        The transform method to use:\n        - \"probit\": Uses the normal quantile function (qnorm)\n        - \"logit\": Uses the logistic transform\n\n    Returns\n    -------\n    result : ndarray\n        A (J-1) x 3 matrix with columns:\n        - z(Hit rate): Transformed cumulative hit rates\n        - z(False alarm rate): Transformed cumulative false alarm rates\n        - d-prime: The difference z(Hit) - z(FA) at each criterion\n\n    Raises\n    ------\n    ValueError\n        If table is not a 2-row matrix or method is not recognized.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from senspy import sdt\n    &gt;&gt;&gt; # Example: rating scale data with 5 categories\n    &gt;&gt;&gt; table = np.array([[10, 20, 30, 25, 15],   # Signal trials\n    ...                   [30, 25, 20, 15, 10]])  # Noise trials\n    &gt;&gt;&gt; result = sdt(table)\n    &gt;&gt;&gt; print(result)\n\n    Notes\n    -----\n    The d-prime at each criterion represents the separation between the\n    signal and noise distributions at that point on the ROC curve.\n    \"\"\"\n    table = np.asarray(table, dtype=np.float64)\n\n    if table.ndim != 2 or table.shape[0] != 2:\n        raise ValueError(\"table must be a 2 x J matrix (2 rows, J columns)\")\n\n    if method not in (\"probit\", \"logit\"):\n        raise ValueError(f\"method '{method}' not recognized; use 'probit' or 'logit'\")\n\n    def transform_row(x: NDArray) -&gt; NDArray:\n        \"\"\"Apply cumulative transform to a row.\"\"\"\n        cs = np.cumsum(x)\n        total = cs[-1]\n\n        if method == \"probit\":\n            # Cumulative proportions (excluding last point which is always 1)\n            cp = cs[:-1] / total\n            return stats.norm.ppf(cp)\n        else:  # logit\n            return np.log(cs[:-1] / (total - cs[:-1]))\n\n    # Apply transform to each row\n    z_signal = transform_row(table[0, :])  # z(Hit rate)\n    z_noise = transform_row(table[1, :])  # z(False alarm rate)\n\n    # Compute d-prime at each criterion\n    d_prime = z_signal - z_noise\n\n    # Build result matrix\n    n_criteria = table.shape[1] - 1\n    result = np.column_stack([z_signal, z_noise, d_prime])\n\n    return result\n</code></pre>"},{"location":"api/roc/#auc","title":"auc","text":"<p>Compute Area Under the ROC Curve.</p>"},{"location":"api/roc/#senspy.auc","title":"<code>auc(d: float, se_d: float | None = None, scale: float = 1.0, ci_alpha: float = 0.05) -&gt; AUCResult</code>","text":"<p>Compute Area Under the ROC Curve from d-prime.</p> <p>Calculates the AUC analytically from the d-prime value, optionally with confidence intervals based on the standard error.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>float</code> <p>The d-prime (discriminability) value. Can be negative.</p> required <code>se_d</code> <code>float</code> <p>Standard error of d-prime. If provided, confidence intervals are computed.</p> <code>None</code> <code>scale</code> <code>float</code> <p>Scale parameter (ratio of noise to signal standard deviation). Must be positive.</p> <code>1.0</code> <code>ci_alpha</code> <code>float</code> <p>Significance level for confidence interval (e.g., 0.05 for 95% CI).</p> <code>0.05</code> <p>Returns:</p> Type Description <code>AUCResult</code> <p>Result containing AUC value and optional confidence bounds.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from senspy import auc\n&gt;&gt;&gt; result = auc(d=1.5, se_d=0.2)\n&gt;&gt;&gt; print(f\"AUC: {result.value:.3f}\")\nAUC: 0.856\n&gt;&gt;&gt; print(f\"95% CI: [{result.lower:.3f}, {result.upper:.3f}]\")\n95% CI: [0.793, 0.905]\n</code></pre> Notes <p>The AUC is computed as:     AUC = \u03a6(d / \u221a(1 + scale\u00b2))</p> <p>where \u03a6 is the standard normal CDF. This formula assumes equal-variance Gaussian signal detection theory when scale=1.</p> <p>For unequal variance models (scale \u2260 1), the ROC curve is asymmetric and the formula accounts for the variance ratio.</p> Source code in <code>senspy/roc.py</code> <pre><code>def auc(\n    d: float,\n    se_d: float | None = None,\n    scale: float = 1.0,\n    ci_alpha: float = 0.05,\n) -&gt; AUCResult:\n    \"\"\"Compute Area Under the ROC Curve from d-prime.\n\n    Calculates the AUC analytically from the d-prime value, optionally\n    with confidence intervals based on the standard error.\n\n    Parameters\n    ----------\n    d : float\n        The d-prime (discriminability) value. Can be negative.\n    se_d : float, optional\n        Standard error of d-prime. If provided, confidence intervals\n        are computed.\n    scale : float, default 1.0\n        Scale parameter (ratio of noise to signal standard deviation).\n        Must be positive.\n    ci_alpha : float, default 0.05\n        Significance level for confidence interval (e.g., 0.05 for 95% CI).\n\n    Returns\n    -------\n    AUCResult\n        Result containing AUC value and optional confidence bounds.\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from senspy import auc\n    &gt;&gt;&gt; result = auc(d=1.5, se_d=0.2)\n    &gt;&gt;&gt; print(f\"AUC: {result.value:.3f}\")\n    AUC: 0.856\n    &gt;&gt;&gt; print(f\"95% CI: [{result.lower:.3f}, {result.upper:.3f}]\")\n    95% CI: [0.793, 0.905]\n\n    Notes\n    -----\n    The AUC is computed as:\n        AUC = \u03a6(d / \u221a(1 + scale\u00b2))\n\n    where \u03a6 is the standard normal CDF. This formula assumes equal-variance\n    Gaussian signal detection theory when scale=1.\n\n    For unequal variance models (scale \u2260 1), the ROC curve is asymmetric\n    and the formula accounts for the variance ratio.\n    \"\"\"\n    # Validate inputs\n    if not np.isfinite(d):\n        raise ValueError(\"d must be a finite number\")\n    if scale &lt;= 0:\n        raise ValueError(\"scale must be positive\")\n    if not (0 &lt; ci_alpha &lt; 1):\n        raise ValueError(\"ci_alpha must be between 0 and 1\")\n\n    if se_d is not None:\n        if not np.isfinite(se_d) or se_d &lt; 0:\n            raise ValueError(\"se_d must be a non-negative finite number\")\n\n    # Compute AUC\n    scale_factor = np.sqrt(1 + scale**2)\n    auc_value = float(stats.norm.cdf(d / scale_factor))\n\n    # Compute confidence interval if se_d provided\n    lower = None\n    upper = None\n    result_ci_alpha = None\n\n    if se_d is not None:\n        tol = se_d * stats.norm.ppf(1 - ci_alpha / 2)\n        lower = float(stats.norm.cdf((d - tol) / scale_factor))\n        upper = float(stats.norm.cdf((d + tol) / scale_factor))\n        result_ci_alpha = ci_alpha\n\n    return AUCResult(\n        value=auc_value,\n        lower=lower,\n        upper=upper,\n        ci_alpha=result_ci_alpha,\n    )\n</code></pre>"},{"location":"api/roc/#result-classes","title":"Result Classes","text":""},{"location":"api/roc/#rocresult","title":"ROCResult","text":""},{"location":"api/roc/#senspy.ROCResult","title":"<code>ROCResult(fpr: NDArray[np.float64], tpr: NDArray[np.float64], lower: NDArray[np.float64] | None = None, upper: NDArray[np.float64] | None = None, d_prime: float = 0.0, scale: float = 1.0)</code>  <code>dataclass</code>","text":"<p>Result from ROC curve computation.</p> <p>Attributes:</p> Name Type Description <code>fpr</code> <code>ndarray</code> <p>False positive rates (x-axis of ROC curve).</p> <code>tpr</code> <code>ndarray</code> <p>True positive rates (y-axis of ROC curve).</p> <code>lower</code> <code>ndarray or None</code> <p>Lower confidence bound for TPR (if se_d provided).</p> <code>upper</code> <code>ndarray or None</code> <p>Upper confidence bound for TPR (if se_d provided).</p> <code>d_prime</code> <code>float</code> <p>The d-prime value used.</p> <code>scale</code> <code>float</code> <p>The scale parameter used.</p>"},{"location":"api/roc/#aucresult","title":"AUCResult","text":""},{"location":"api/roc/#senspy.AUCResult","title":"<code>AUCResult(value: float, lower: float | None = None, upper: float | None = None, ci_alpha: float | None = None)</code>  <code>dataclass</code>","text":"<p>Result from AUC computation.</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>float</code> <p>The Area Under the ROC Curve.</p> <code>lower</code> <code>float or None</code> <p>Lower bound of confidence interval (if se_d provided).</p> <code>upper</code> <code>float or None</code> <p>Upper bound of confidence interval (if se_d provided).</p> <code>ci_alpha</code> <code>float or None</code> <p>Alpha level used for confidence interval.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>NumPy, SciPy, Pandas</li> <li>Plotly (for interactive visualizations)</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install senspy\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/aigorahub/sensPy.git\ncd sensPy\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#using-uv-recommended","title":"Using uv (recommended)","text":"<pre><code>uv pip install senspy\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import senspy\nprint(senspy.__version__)\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For development:</p> <pre><code>pip install senspy[dev]\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks you through the basic usage of sensPy for sensory discrimination analysis.</p>"},{"location":"getting-started/quickstart/#basic-discrimination-analysis","title":"Basic Discrimination Analysis","text":"<p>The <code>discrim()</code> function is the primary tool for analyzing discrimination test data:</p> <pre><code>from senspy import discrim\n\n# Triangle test: 80 correct out of 100 trials\nresult = discrim(correct=80, total=100, method=\"triangle\")\n\nprint(f\"d-prime: {result.d_prime:.3f}\")\nprint(f\"Proportion correct: {result.pc:.3f}\")\nprint(f\"Proportion discriminating: {result.pd:.3f}\")\nprint(f\"P-value: {result.p_value:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#supported-protocols","title":"Supported Protocols","text":"<p>sensPy supports all major discrimination protocols:</p> Protocol Function Guessing Probability Triangle <code>method=\"triangle\"</code> 1/3 Duo-trio <code>method=\"duotrio\"</code> 1/2 2-AFC <code>method=\"twoAFC\"</code> 1/2 3-AFC <code>method=\"threeAFC\"</code> 1/3 Tetrad <code>method=\"tetrad\"</code> 1/3 Hexad <code>method=\"hexad\"</code> 1/10"},{"location":"getting-started/quickstart/#power-analysis","title":"Power Analysis","text":"<p>Calculate the power of a discrimination test:</p> <pre><code>from senspy import discrim_power\n\npower = discrim_power(\n    d_prime=1.0,\n    n=100,\n    method=\"triangle\",\n    alpha=0.05\n)\nprint(f\"Power: {power:.3f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#sample-size-calculation","title":"Sample Size Calculation","text":"<p>Determine the required sample size:</p> <pre><code>from senspy import discrim_sample_size\n\nn = discrim_sample_size(\n    d_prime=1.0,\n    method=\"triangle\",\n    power=0.80,\n    alpha=0.05\n)\nprint(f\"Required N: {n}\")\n</code></pre>"},{"location":"getting-started/quickstart/#interactive-plots","title":"Interactive Plots","text":"<p>sensPy uses Plotly for interactive visualizations:</p> <pre><code>from senspy import plot_psychometric, plot_roc\n\n# Compare psychometric functions\nfig = plot_psychometric_comparison()\nfig.show()\n\n# ROC curve with confidence band\nfig = plot_roc(d_prime=1.5, se_d=0.2)\nfig.show()\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Discrimination Protocols - Detailed protocol guide</li> <li>Power Analysis - Power and sample size planning</li> <li>Plotting - Visualization options</li> <li>API Reference - Complete function reference</li> </ul>"},{"location":"guide/plotting/","title":"Plotting","text":"<p>sensPy uses Plotly for interactive visualizations that work seamlessly in Jupyter notebooks and can be exported to various formats.</p>"},{"location":"guide/plotting/#roc-curves","title":"ROC Curves","text":"<p>Plot Receiver Operating Characteristic curves for signal detection analysis:</p> <pre><code>from senspy import plot_roc\n\n# Basic ROC curve\nfig = plot_roc(d_prime=1.5)\nfig.show()\n\n# With confidence bands\nfig = plot_roc(d_prime=1.5, se_d=0.2)\nfig.show()\n</code></pre>"},{"location":"guide/plotting/#customization","title":"Customization","text":"<pre><code>fig = plot_roc(\n    d_prime=1.5,\n    se_d=0.2,\n    ci_alpha=0.05,        # 95% CI\n    title=\"My ROC Curve\",\n    show_diagonal=True,   # Show chance line\n    show_auc=True,        # Display AUC value\n    width=700,\n    height=600\n)\n</code></pre>"},{"location":"guide/plotting/#psychometric-functions","title":"Psychometric Functions","text":"<p>Visualize the relationship between d-prime and proportion correct:</p> <pre><code>from senspy import plot_psychometric\n\n# Single protocol\nfig = plot_psychometric(method=\"triangle\")\nfig.show()\n</code></pre>"},{"location":"guide/plotting/#compare-multiple-protocols","title":"Compare Multiple Protocols","text":"<pre><code>from senspy import plot_psychometric_comparison\n\nfig = plot_psychometric_comparison(\n    methods=[\"triangle\", \"duotrio\", \"twoAFC\", \"threeAFC\", \"tetrad\"]\n)\nfig.show()\n</code></pre>"},{"location":"guide/plotting/#signal-detection-distributions","title":"Signal Detection Distributions","text":"<p>Visualize the underlying signal detection theory:</p> <pre><code>from senspy import plot_sdt_distributions\n\nfig = plot_sdt_distributions(\n    d_prime=1.5,\n    show_criterion=True,\n    criterion=0.5  # Optional: custom criterion location\n)\nfig.show()\n</code></pre> <p>This shows: - Noise distribution (green) - Signal distribution (red) - Decision criterion - Hit rate and false alarm rate</p>"},{"location":"guide/plotting/#profile-likelihood","title":"Profile Likelihood","text":"<p>Plot profile likelihood for confidence interval visualization:</p> <pre><code>import numpy as np\nfrom senspy import plot_profile_likelihood\n\n# Example: simulate profile likelihood data\nd_values = np.linspace(0.5, 2.5, 100)\nll_values = -50 * (d_values - 1.5)**2  # Parabolic approximation\n\nfig = plot_profile_likelihood(\n    d_values,\n    ll_values,\n    levels=(0.95, 0.99)  # Show 95% and 99% CI\n)\nfig.show()\n</code></pre>"},{"location":"guide/plotting/#power-curves","title":"Power Curves","text":"<p>Visualize statistical power:</p> <pre><code>import numpy as np\nfrom senspy import discrim_power, plot_power_curve\n\nd_values = np.linspace(0, 3, 50)\npowers = [discrim_power(d_prime=d, n=100, method=\"triangle\")\n          for d in d_values]\n\nfig = plot_power_curve(d_values, powers, target_power=0.8)\nfig.show()\n</code></pre>"},{"location":"guide/plotting/#sample-size-curves","title":"Sample Size Curves","text":"<pre><code>from senspy import plot_sample_size_curve\n\npowers = np.linspace(0.5, 0.95, 30)\nn_values = [50 / (1 - p) for p in powers]  # Example relationship\n\nfig = plot_sample_size_curve(powers, n_values, target_power=0.8)\nfig.show()\n</code></pre>"},{"location":"guide/plotting/#exporting-plots","title":"Exporting Plots","text":""},{"location":"guide/plotting/#to-html","title":"To HTML","text":"<pre><code>fig.write_html(\"my_plot.html\")\n</code></pre>"},{"location":"guide/plotting/#to-static-image","title":"To Static Image","text":"<p>Requires kaleido:</p> <pre><code>pip install kaleido\n</code></pre> <pre><code>fig.write_image(\"my_plot.png\", scale=2)  # High resolution\nfig.write_image(\"my_plot.pdf\")           # Vector format\nfig.write_image(\"my_plot.svg\")           # SVG format\n</code></pre>"},{"location":"guide/plotting/#inline-in-jupyter","title":"Inline in Jupyter","text":"<p>Plots display automatically in Jupyter. For other environments:</p> <pre><code>fig.show()\n</code></pre>"},{"location":"guide/plotting/#theming","title":"Theming","text":"<p>Plotly figures can be customized with templates:</p> <pre><code>import plotly.io as pio\n\n# Use a different template\npio.templates.default = \"plotly_white\"\n\n# Or per-figure\nfig.update_layout(template=\"plotly_dark\")\n</code></pre> <p>Available templates: <code>plotly</code>, <code>plotly_white</code>, <code>plotly_dark</code>, <code>ggplot2</code>, <code>seaborn</code>, <code>simple_white</code></p>"},{"location":"guide/power/","title":"Power Analysis","text":"<p>Statistical power is the probability of correctly detecting a true difference when one exists. Proper power analysis is essential for designing effective sensory studies.</p>"},{"location":"guide/power/#power-calculation","title":"Power Calculation","text":"<p>Calculate the power of a planned study:</p> <pre><code>from senspy import discrim_power, dprime_power\n\n# Power for triangle test\npower = discrim_power(\n    d_prime=1.0,      # Expected effect size\n    n=100,            # Sample size\n    method=\"triangle\",\n    alpha=0.05        # Significance level\n)\nprint(f\"Power: {power:.1%}\")\n</code></pre>"},{"location":"guide/power/#using-proportion-discriminating","title":"Using Proportion Discriminating","text":"<p>You can also specify the effect size as proportion discriminating (Pd):</p> <pre><code>power = discrim_power(\n    pd=0.25,          # 25% truly discriminating\n    n=100,\n    method=\"triangle\",\n    alpha=0.05\n)\n</code></pre>"},{"location":"guide/power/#sample-size-calculation","title":"Sample Size Calculation","text":"<p>Determine the required sample size to achieve a target power:</p> <pre><code>from senspy import discrim_sample_size, dprime_sample_size\n\n# Required N for 80% power\nn = discrim_sample_size(\n    d_prime=1.0,\n    method=\"triangle\",\n    power=0.80,\n    alpha=0.05\n)\nprint(f\"Required N: {n}\")\n</code></pre>"},{"location":"guide/power/#visualizing-power","title":"Visualizing Power","text":"<pre><code>import numpy as np\nfrom senspy import discrim_power, plot_power_curve\n\n# Calculate power across effect sizes\nd_values = np.linspace(0, 3, 50)\npowers = [discrim_power(d_prime=d, n=100, method=\"triangle\")\n          for d in d_values]\n\nfig = plot_power_curve(d_values, powers, target_power=0.8)\nfig.show()\n</code></pre>"},{"location":"guide/power/#sample-size-vs-power-curve","title":"Sample Size vs Power Curve","text":"<pre><code>from senspy import discrim_sample_size, plot_sample_size_curve\n\n# Calculate sample sizes for various power levels\npowers = np.linspace(0.5, 0.95, 20)\nsample_sizes = [discrim_sample_size(d_prime=1.0, method=\"triangle\", power=p)\n                for p in powers]\n\nfig = plot_sample_size_curve(powers, sample_sizes)\nfig.show()\n</code></pre>"},{"location":"guide/power/#protocol-comparison","title":"Protocol Comparison","text":"<p>Different protocols require different sample sizes for the same power:</p> <pre><code>from senspy import discrim_sample_size\n\nmethods = [\"triangle\", \"duotrio\", \"twoAFC\", \"tetrad\"]\nd_prime = 1.0\n\nfor method in methods:\n    n = discrim_sample_size(d_prime=d_prime, method=method, power=0.8)\n    print(f\"{method:12s}: N = {n}\")\n</code></pre> <p>Typical results: <pre><code>triangle    : N = 92\nduotrio     : N = 136\ntwoAFC      : N = 64\ntetrad      : N = 78\n</code></pre></p> <p>2-AFC is the most efficient, while duo-trio requires the largest samples.</p>"},{"location":"guide/power/#advanced-dod-power-analysis","title":"Advanced: DOD Power Analysis","text":"<p>For Degree-of-Difference (DOD) studies, use Monte Carlo simulation:</p> <pre><code>from senspy import dod_power\n\nresult = dod_power(\n    d_prime=1.5,\n    n=50,\n    tau=[1, 2, 3, 4],\n    n_sim=1000,\n    alpha=0.05\n)\nprint(f\"DOD Power: {result.power:.1%}\")\n</code></pre>"},{"location":"guide/power/#guidelines","title":"Guidelines","text":"Target Power Typical Use 0.80 Standard research 0.90 Confirmatory studies 0.95 Critical decisions <p>Tips: - Always conduct power analysis before collecting data - Consider the smallest meaningful effect size - Account for potential dropouts (add 10-20% to calculated N) - Use pilot data to estimate expected effect size</p>"},{"location":"guide/protocols/","title":"Discrimination Protocols","text":"<p>sensPy implements all major sensory discrimination protocols used in the food, beverage, and consumer products industries.</p>"},{"location":"guide/protocols/#overview","title":"Overview","text":"<p>Discrimination tests measure an assessor's ability to detect sensory differences between products. The result is quantified as d-prime (d'), a measure of discriminability that is independent of the specific protocol used.</p>"},{"location":"guide/protocols/#forced-choice-protocols","title":"Forced-Choice Protocols","text":""},{"location":"guide/protocols/#triangle-test","title":"Triangle Test","text":"<p>The triangle test presents three samples (two identical, one different). The assessor must identify the odd sample.</p> <pre><code>from senspy import discrim\n\n# 25 correct out of 50 triangle tests\nresult = discrim(correct=25, total=50, method=\"triangle\")\nprint(f\"d-prime: {result.d_prime:.3f}\")\n</code></pre> <p>Properties: - Guessing probability: 1/3 - Cognitively demanding - Widely used in industry</p>"},{"location":"guide/protocols/#duo-trio-test","title":"Duo-Trio Test","text":"<p>Two samples presented with a reference. The assessor identifies which sample matches the reference.</p> <pre><code>result = discrim(correct=35, total=50, method=\"duotrio\")\n</code></pre> <p>Properties: - Guessing probability: 1/2 - Less demanding than triangle - Reference-based comparison</p>"},{"location":"guide/protocols/#2-afc-two-alternative-forced-choice","title":"2-AFC (Two-Alternative Forced Choice)","text":"<p>Two samples presented; assessor chooses which is stronger on a specified attribute.</p> <pre><code>result = discrim(correct=60, total=100, method=\"twoAFC\")\n</code></pre> <p>Properties: - Guessing probability: 1/2 - Attribute-specific - Most statistically efficient</p>"},{"location":"guide/protocols/#3-afc-three-alternative-forced-choice","title":"3-AFC (Three-Alternative Forced Choice)","text":"<p>Three samples (two identical controls, one target); assessor identifies the different sample based on a specified attribute.</p> <pre><code>result = discrim(correct=45, total=100, method=\"threeAFC\")\n</code></pre> <p>Properties: - Guessing probability: 1/3 - Attribute-specific - Combines benefits of AFC and triangle</p>"},{"location":"guide/protocols/#tetrad-test","title":"Tetrad Test","text":"<p>Four samples (two pairs); assessor groups into matching pairs.</p> <pre><code>result = discrim(correct=40, total=100, method=\"tetrad\")\n</code></pre> <p>Properties: - Guessing probability: 1/3 - More powerful than triangle - Good for small differences</p>"},{"location":"guide/protocols/#hexad-test","title":"Hexad Test","text":"<p>Six samples (two groups of three); assessor groups into matching triads.</p> <pre><code>result = discrim(correct=20, total=100, method=\"hexad\")\n</code></pre> <p>Properties: - Guessing probability: 1/10 - Very sensitive - Cognitively demanding</p>"},{"location":"guide/protocols/#comparing-protocols","title":"Comparing Protocols","text":"<p>Different protocols have different psychometric functions:</p> <pre><code>from senspy import plot_psychometric_comparison\n\nfig = plot_psychometric_comparison(\n    methods=[\"triangle\", \"duotrio\", \"twoAFC\", \"threeAFC\", \"tetrad\"]\n)\nfig.show()\n</code></pre> <p>At the same d-prime, protocols with lower guessing probability require higher discriminability to achieve the same proportion correct.</p>"},{"location":"guide/protocols/#protocol-selection-guidelines","title":"Protocol Selection Guidelines","text":"Situation Recommended Protocol General difference testing Triangle, Tetrad Specific attribute 2-AFC, 3-AFC Less experienced panel Duo-trio Detecting small differences Tetrad, Hexad Large sample sizes available 2-AFC"},{"location":"guide/protocols/#converting-between-scales","title":"Converting Between Scales","text":"<p>Convert between proportion correct (Pc), proportion discriminating (Pd), and d-prime:</p> <pre><code>from senspy import psy_fun, psy_inv, pc_to_pd, pd_to_pc\n\n# d-prime to proportion correct\npc = psy_fun(d_prime=1.5, method=\"triangle\")\n\n# proportion correct to d-prime\nd_prime = psy_inv(pc=0.6, method=\"triangle\")\n\n# proportion correct to proportion discriminating\npd = pc_to_pd(pc=0.6, method=\"triangle\")\n</code></pre>"}]}